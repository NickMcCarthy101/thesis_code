{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-12 08:38:09.123992\n",
      "trading_gym 0.7.6\n",
      "ray 0.7.2\n"
     ]
    }
   ],
   "source": [
    "import trading_gym\n",
    "from trading_gym.registry.gaia.v7.env import GAIAPredictorsContinuousV7\n",
    "from datetime import datetime\n",
    "from collections import namedtuple\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import ray\n",
    "print(datetime.now())\n",
    "print(trading_gym.__name__, trading_gym.__version__)\n",
    "print(ray.__name__, ray.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-12 08:38:09,138\tINFO node.py:498 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-07-12_08-38-09_136918_105476/logs.\n",
      "2019-07-12 08:38:09,266\tINFO services.py:409 -- Waiting for redis server at 127.0.0.1:27935 to respond...\n",
      "2019-07-12 08:38:09,396\tINFO services.py:409 -- Waiting for redis server at 127.0.0.1:23426 to respond...\n",
      "2019-07-12 08:38:09,401\tINFO services.py:806 -- Starting Redis shard with 10.0 GB max memory.\n",
      "2019-07-12 08:38:09,466\tINFO node.py:512 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-07-12_08-38-09_136918_105476/logs.\n",
      "2019-07-12 08:38:09,469\tWARNING services.py:1298 -- Warning: Capping object memory store to 20.0GB. To increase this further, specify `object_store_memory` when calling ray.init() or ray start.\n",
      "2019-07-12 08:38:09,470\tINFO services.py:1446 -- Starting the Plasma object store with 20.0 GB memory using /dev/shm.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0.7.2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray\n",
    "from ray import rllib, tune\n",
    "from trading_gym.ray.logger import calculate_tearsheet, CustomLogger\n",
    "from copy import deepcopy\n",
    "# ray.init(num_cpus=8,ignore_reinit_error=True,object_store_memory= 10*100 )\n",
    "ray.init(ignore_reinit_error=True)\n",
    "#          object_store_memory = 50000000)\n",
    "\n",
    "ray.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<trading_gym.registry.gaia.v7.env.GAIAPredictorsContinuousV7 at 0x7f6c5d46c828>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_config = dict()\n",
    "env_config['folds'] =  {\n",
    "    'training-set': [datetime.min, datetime(2008, 3, 18)],\n",
    "    'test-set': [datetime(2008, 3, 19), datetime.max],\n",
    "}\n",
    "env = GAIAPredictorsContinuousV7(env_config)\n",
    "env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is a 'common config' that sets ray's params\n",
    "# and then default_config, which sets the PPO config \n",
    "config = rllib.agents.ppo.DEFAULT_CONFIG.copy()\n",
    "#  The env is self.explanatory \n",
    "config['env'] = GAIAPredictorsContinuousV7\n",
    "config['callbacks']['on_train_result'] = tune.function(calculate_tearsheet)\n",
    "config['num_workers'] = 6\n",
    "\n",
    "config['gamma'] = 0 # tune.grid_search([0])\n",
    "config['vf_clip_param'] = 0 # tune.grid_search([0.])\n",
    "config['vf_loss_coeff'] = 0 # tune.grid_search([0.])\n",
    "config['lambda'] = 0 # tune.grid_search([0])\n",
    "\n",
    "config['use_gae'] = False #tune.grid_search([False])\n",
    "config['vf_share_layers'] = False #tune.grid_search([False])\n",
    "\n",
    "# If you do use this, have vf_share_layers as True (loss function then combines ) \n",
    "config['use_lstm']: False\n",
    "# Whether to roll out complete epsiodes or truncate them \n",
    "config['batch_mode'] = 'complete_episodes'\n",
    "\n",
    "\n",
    "# Literature suggests having different LR for actor and critic and -3 and -2 \n",
    "config['lr'] = tune.grid_search([1e-5])\n",
    "\n",
    "# Size of batches collected from each worker (number of experiences used for one iteration of SGD)\n",
    "#  Don't think I actually want to use the following. \n",
    "config['sample_batch_size'] = 200 # tune.grid_search([256])\n",
    "\n",
    "# Increase this to maximize the amount of info(no. of experiences(think transition tuples)) we gather before making an update to policy\n",
    "config['train_batch_size'] = tune.grid_search([4000])\n",
    "# Total SGD batch size across all devices\n",
    "config['sgd_minibatch_size'] = 128\n",
    "# Number of SGD iterations in each outer loop \n",
    "config['num_sgd_iter'] = tune.grid_search([8])\n",
    "\n",
    "\n",
    "# Coefficient of entropy regularizer (i.e how much we encourage explorsation)\n",
    "config['entropy_coeff'] = tune.grid_search([1e-5])\n",
    "\n",
    "# Initial coefficient for KL divergence \n",
    "config['kl_coeff'] = tune.grid_search([0.2])\n",
    "# Target value for the KL divergence \n",
    "config['kl_target'] = tune.grid_search([0.01])\n",
    "\n",
    "# PPO clip parameter\n",
    "config['clip_param'] = tune.grid_search([0.7])\n",
    "# config['ignore_worker_failures'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['env_config'] = env_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.models.model import Model\n",
    "from ray.rllib.models.misc import normc_initializer, get_activation_fn\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "\n",
    "\n",
    "class MLP(Model):\n",
    "    def _build_layers_v2(self, input_dict: dict, num_outputs: int, config: dict):\n",
    "        import tensorflow.contrib.slim as slim\n",
    "\n",
    "        with tf.name_scope(\"fc_net\"):\n",
    "            last_layer = input_dict['obs']\n",
    "            activation = get_activation_fn(config.get(\"fcnet_activation\"))\n",
    "            for i, size in enumerate(config.get(\"fcnet_hiddens\"), 1):\n",
    "                last_layer = slim.fully_connected(\n",
    "                    inputs=last_layer,\n",
    "                    num_outputs=size,\n",
    "                    weights_initializer=normc_initializer(1.0),\n",
    "                    activation_fn=activation,\n",
    "                    scope=\"fc{}\".format(i),\n",
    "                )\n",
    "#                 We don't need any dropout at this stage\n",
    "#                 last_layer = tf.layers.dropout(\n",
    "#                     inputs=last_layer,\n",
    "#                     rate=config['custom_options'][\"fcnet_dropout_rate\"],\n",
    "#                     training=input_dict['is_training'],\n",
    "#                     name=\"dropout{}\".format(i),\n",
    "#                 )\n",
    "            output = slim.fully_connected(\n",
    "                inputs=last_layer,\n",
    "                num_outputs=num_outputs,\n",
    "                weights_initializer=normc_initializer(0.01),\n",
    "                activation_fn=None,\n",
    "                scope=\"fc_out\",\n",
    "            )\n",
    "            return output, last_layer\n",
    "\n",
    "ModelCatalog.register_custom_model(MLP.__name__, MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config['model']['custom_options'] = {'fcnet_dropout_rate': 0.5}\n",
    "config['model']['custom_model'] = MLP.__name__\n",
    "# config['model']['custom_model'] = CNN.__name__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-12 08:39:08,570\tINFO tune.py:65 -- Did not find checkpoint file in logs/PPOclip_0.7-WalkForward-750k2013.\n",
      "2019-07-12 08:39:08,571\tINFO tune.py:233 -- Starting a new experiment.\n",
      "2019-07-12 08:39:08,623\tWARNING signature.py:108 -- The function with_updates has a **kwargs argument, which is currently not supported.\n",
      "2019-07-12 08:39:08,711\tWARNING util.py:64 -- The `start_trial` operation took 0.10505199432373047 seconds to complete, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_______________________________________2013____________________________________________\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:39:14,079\tWARNING ppo.py:151 -- FYI: By default, the value function will not share layers with the policy model ('vf_share_layers': False).\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:39:16,196\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 0 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:39:16.197090: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:39:27,108\tINFO dynamic_tf_policy.py:313 -- Initializing loss function with dummy input:\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m { 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m   'actions': <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m   'behaviour_logits': <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 3) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 3) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m   'prev_actions': <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:39:28,858\tINFO rollout_worker.py:719 -- Built policy map: {'default_policy': <ray.rllib.policy.tf_policy_template.PPOTFPolicy object at 0x7efff5dcff60>}\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:39:28,858\tINFO rollout_worker.py:720 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7efff5dcfb38>}\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:39:28,858\tINFO rollout_worker.py:333 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7efff5dcf978>}\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:39:29,008\tINFO multi_gpu_optimizer.py:79 -- LocalMultiGPUOptimizer devices ['/cpu:0']\n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m 2019-07-12 08:39:38,744\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 1 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m 2019-07-12 08:39:38.768815: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=105652)\u001b[0m 2019-07-12 08:39:38,960\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 5 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=105652)\u001b[0m 2019-07-12 08:39:39.034312: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=105656)\u001b[0m 2019-07-12 08:39:39,759\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 4 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=105651)\u001b[0m 2019-07-12 08:39:39,815\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 6 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=105656)\u001b[0m 2019-07-12 08:39:39.815418: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=105654)\u001b[0m 2019-07-12 08:39:39,869\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 2 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=105651)\u001b[0m 2019-07-12 08:39:39.932866: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=105654)\u001b[0m 2019-07-12 08:39:39.899787: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=105655)\u001b[0m 2019-07-12 08:39:40,603\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 3 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=105655)\u001b[0m 2019-07-12 08:39:40.670544: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m 2019-07-12 08:40:01,594\tINFO dynamic_tf_policy.py:313 -- Initializing loss function with dummy input:\n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m { 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m   'actions': <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m   'behaviour_logits': <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,\n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 3) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 3) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m   'prev_actions': <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}\n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=105652)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=105652)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=105652)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=105652)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=105654)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=105654)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=105654)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=105654)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=105655)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=105655)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=105655)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=105655)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=105656)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=105656)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=105656)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=105656)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=105651)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=105651)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=105651)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=105651)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m 2019-07-12 08:40:04,707\tINFO rollout_worker.py:428 -- Generating sample batch of size 200\n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m 2019-07-12 08:40:04,741\tINFO sampler.py:308 -- Raw obs from env: { 0: { 'agent0': np.ndarray((3,), dtype=float64, min=-1.508, max=-0.715, mean=-1.074)}}\n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m 2019-07-12 08:40:04,743\tINFO sampler.py:309 -- Info return from env: {0: {'agent0': None}}\n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m 2019-07-12 08:40:04,744\tINFO sampler.py:407 -- Preprocessed obs: np.ndarray((3,), dtype=float64, min=-1.508, max=-0.715, mean=-1.074)\n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m 2019-07-12 08:40:04,744\tINFO sampler.py:411 -- Filtered obs: np.ndarray((3,), dtype=float64, min=-1.508, max=-0.715, mean=-1.074)\n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m 2019-07-12 08:40:04,746\tINFO sampler.py:525 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m                                   'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m                                   'obs': np.ndarray((3,), dtype=float64, min=-1.508, max=-0.715, mean=-1.074),\n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m 2019-07-12 08:40:04,746\tINFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m 2019-07-12 08:40:04,955\tINFO sampler.py:552 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m { 'default_policy': ( np.ndarray((1, 2), dtype=float32, min=0.008, max=0.992, mean=0.5),\n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m                       [],\n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m                       { 'action_prob': np.ndarray((1,), dtype=float32, min=0.989, max=0.989, mean=0.989),\n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m                         'behaviour_logits': np.ndarray((1, 2), dtype=float32, min=-0.001, max=0.003, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=0.0, max=0.0, mean=0.0)})}\n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m 2019-07-12 08:40:05,139\tINFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m { 'agent0': { 'data': { 'action_prob': np.ndarray((20,), dtype=float32, min=0.989, max=1.012, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m                         'actions': np.ndarray((20, 2), dtype=float32, min=0.008, max=0.992, mean=0.5),\n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m                         'advantages': np.ndarray((20,), dtype=float32, min=-0.008, max=0.006, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m                         'agent_index': np.ndarray((20,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m                         'behaviour_logits': np.ndarray((20, 2), dtype=float32, min=-0.01, max=0.005, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m                         'dones': np.ndarray((20,), dtype=bool, min=0.0, max=1.0, mean=0.05),\n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m                         'eps_id': np.ndarray((20,), dtype=int64, min=563758613.0, max=563758613.0, mean=563758613.0),\n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m                         'infos': np.ndarray((20,), dtype=object, head={'time': Timestamp('2013-10-18 00:00:00'), 'nlv': 100.0, 'nr_contracts': np.ndarray((3,), dtype=float64, min=0.0, max=0.02, mean=0.007), 'bid_prices': np.ndarray((3,), dtype=float64, min=1.0, max=5003.371, mean=1840.962), 'ask_prices': np.ndarray((3,), dtype=float64, min=1.0, max=5003.371, mean=1840.962), 'cost_of_spread': np.ndarray((3,), dtype=float64, min=0.0, max=0.0, mean=0.0), 'cost_of_commissions': np.ndarray((3,), dtype=float64, min=0.0, max=0.0, mean=0.0), 'profit_on_idle_cash': 0.0, 'margin': 0, 'weights_target': np.ndarray((3,), dtype=float32, min=0.0, max=0.992, mean=0.333), 'weights_drift': np.ndarray((3,), dtype=float64, min=0.0, max=1.0, mean=0.333), 'weights_diff': np.ndarray((3,), dtype=float64, min=-1.0, max=0.992, mean=-0.0), 'positions_value_diff': np.ndarray((3,), dtype=float64, min=-100.0, max=99.165, mean=-0.0), 'acq_prices': np.ndarray((3,), dtype=float64, min=1.0, max=5003.371, mean=1840.962), 'nr_contracts_diff': np.ndarray((3,), dtype=float64, min=-100.0, max=0.02, mean=-33.326), 'nlv_pre': 100.0}),\n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m                         'new_obs': np.ndarray((20, 3), dtype=float32, min=-2.085, max=2.015, mean=-0.03),\n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m                         'obs': np.ndarray((20, 3), dtype=float32, min=-2.085, max=2.015, mean=-0.076),\n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m                         'prev_actions': np.ndarray((20, 2), dtype=float32, min=0.0, max=0.992, mean=0.475),\n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m                         'prev_rewards': np.ndarray((20,), dtype=float32, min=-0.008, max=0.006, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m                         'rewards': np.ndarray((20,), dtype=float32, min=-0.008, max=0.006, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m                         't': np.ndarray((20,), dtype=int64, min=0.0, max=19.0, mean=9.5),\n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m                         'unroll_id': np.ndarray((20,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m                         'value_targets': np.ndarray((20,), dtype=float64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m                         'vf_preds': np.ndarray((20,), dtype=float32, min=0.0, max=0.0, mean=0.0)},\n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m               'type': 'SampleBatch'}}\n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m 2019-07-12 08:40:07,574\tINFO rollout_worker.py:462 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m { 'data': { 'action_prob': np.ndarray((200,), dtype=float32, min=0.989, max=1.032, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m             'actions': np.ndarray((200, 2), dtype=float32, min=0.0, max=1.0, mean=0.5),\n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m             'advantages': np.ndarray((200,), dtype=float32, min=-0.02, max=0.013, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m             'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m             'behaviour_logits': np.ndarray((200, 2), dtype=float32, min=-0.01, max=0.01, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m             'dones': np.ndarray((200,), dtype=bool, min=0.0, max=1.0, mean=0.05),\n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m             'eps_id': np.ndarray((200,), dtype=int64, min=563758613.0, max=1775900971.0, mean=1133377231.3),\n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m             'infos': np.ndarray((200,), dtype=object, head={'time': Timestamp('2013-10-18 00:00:00'), 'nlv': 100.0, 'nr_contracts': np.ndarray((3,), dtype=float64, min=0.0, max=0.02, mean=0.007), 'bid_prices': np.ndarray((3,), dtype=float64, min=1.0, max=5003.371, mean=1840.962), 'ask_prices': np.ndarray((3,), dtype=float64, min=1.0, max=5003.371, mean=1840.962), 'cost_of_spread': np.ndarray((3,), dtype=float64, min=0.0, max=0.0, mean=0.0), 'cost_of_commissions': np.ndarray((3,), dtype=float64, min=0.0, max=0.0, mean=0.0), 'profit_on_idle_cash': 0.0, 'margin': 0, 'weights_target': np.ndarray((3,), dtype=float32, min=0.0, max=0.992, mean=0.333), 'weights_drift': np.ndarray((3,), dtype=float64, min=0.0, max=1.0, mean=0.333), 'weights_diff': np.ndarray((3,), dtype=float64, min=-1.0, max=0.992, mean=-0.0), 'positions_value_diff': np.ndarray((3,), dtype=float64, min=-100.0, max=99.165, mean=-0.0), 'acq_prices': np.ndarray((3,), dtype=float64, min=1.0, max=5003.371, mean=1840.962), 'nr_contracts_diff': np.ndarray((3,), dtype=float64, min=-100.0, max=0.02, mean=-33.326), 'nlv_pre': 100.0}),\n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m             'new_obs': np.ndarray((200, 3), dtype=float32, min=-7.28, max=6.349, mean=0.09),\n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m             'obs': np.ndarray((200, 3), dtype=float32, min=-7.28, max=9.798, mean=0.12),\n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m             'prev_actions': np.ndarray((200, 2), dtype=float32, min=0.0, max=1.0, mean=0.475),\n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m             'prev_rewards': np.ndarray((200,), dtype=float32, min=-0.02, max=0.013, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m             'rewards': np.ndarray((200,), dtype=float32, min=-0.02, max=0.013, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m             't': np.ndarray((200,), dtype=int64, min=0.0, max=19.0, mean=9.5),\n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m             'unroll_id': np.ndarray((200,), dtype=int64, min=0.0, max=9.0, mean=4.5),\n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m             'value_targets': np.ndarray((200,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m             'vf_preds': np.ndarray((200,), dtype=float32, min=0.0, max=0.0, mean=0.0)},\n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m   'type': 'SampleBatch'}\n",
      "\u001b[2m\u001b[36m(pid=105653)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:40:13,489\tINFO multi_gpu_impl.py:146 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m { 'inputs': [ np.ndarray((4000, 2), dtype=float32, min=0.0, max=1.0, mean=0.475),\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m               np.ndarray((4000,), dtype=float32, min=-0.036, max=0.033, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m               np.ndarray((4000, 3), dtype=float32, min=-14.063, max=10.641, mean=0.055),\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m               np.ndarray((4000, 2), dtype=float32, min=0.0, max=1.0, mean=0.5),\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m               np.ndarray((4000,), dtype=float32, min=-7.595, max=6.921, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m               np.ndarray((4000, 2), dtype=float32, min=-0.011, max=0.011, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m               np.ndarray((4000,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m               np.ndarray((4000,), dtype=float32, min=0.0, max=0.0, mean=0.0)],\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m   'placeholders': [ <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m                     <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m                     <tf.Tensor 'default_policy/observation:0' shape=(?, 3) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m                     <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m                     <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m                     <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m                     <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m                     <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>],\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m   'state_inputs': []}\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:40:13,489\tINFO multi_gpu_impl.py:191 -- Divided 4000 rollout sequences, each of length 1, among 1 devices.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:40:15,979\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:40:17,228\tINFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:40:30,151\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:40:42,285\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:40:56,083\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:41:08,764\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:41:22,128\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:41:34,128\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:41:47,317\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:42:00,269\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:42:12,015\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:42:25,527\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:42:37,633\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:42:50,323\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:43:02,425\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:43:14,921\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:43:27,638\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:43:39,451\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:43:53,064\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:44:04,434\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:44:16,179\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:44:29,118\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:44:41,795\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:44:54,793\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:45:06,717\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:45:18,122\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:45:30,696\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:45:42,927\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:45:56,515\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:46:08,743\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:46:21,345\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:46:32,744\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:46:45,484\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:46:58,032\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:47:10,254\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:47:23,024\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:47:36,153\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:47:48,522\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:48:01,293\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:48:14,017\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:48:26,453\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:48:38,502\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:48:51,124\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:49:03,505\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:49:16,161\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:49:28,712\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:49:41,268\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:49:54,527\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:50:06,605\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:50:19,573\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:50:31,560\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:50:44,330\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:50:57,109\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:51:08,924\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:51:22,732\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:51:34,191\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:51:46,889\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:51:59,423\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:52:11,852\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:52:25,043\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:52:37,448\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:52:49,992\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:53:02,166\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:53:14,862\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:53:28,125\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:53:40,050\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:53:53,105\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:54:04,358\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:54:17,345\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:54:30,070\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:54:42,790\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:54:56,433\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:55:08,286\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:55:21,550\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:55:33,242\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:55:48,602\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:56:01,256\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:56:14,519\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:56:29,586\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:56:42,041\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:56:55,764\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:57:08,846\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:57:23,025\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:57:35,988\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:57:50,277\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:58:03,549\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:58:17,334\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:58:30,150\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:58:42,383\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:58:56,160\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:59:08,026\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:59:20,703\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:59:33,368\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:59:45,721\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 08:59:58,599\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 09:00:10,117\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 09:00:23,578\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 09:00:35,479\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 09:00:47,413\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 09:01:00,759\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 09:01:12,273\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 09:01:25,312\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 09:01:37,076\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 09:01:48,959\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 09:02:01,603\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 09:02:13,735\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 09:02:27,488\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 09:02:38,810\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 09:02:51,072\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 09:03:03,471\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 09:03:15,603\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 09:03:29,035\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 09:03:40,610\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 09:03:53,022\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 09:04:05,061\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 09:04:16,818\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 09:04:30,320\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 09:04:41,699\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 09:04:54,720\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 09:05:07,405\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 09:05:19,841\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 09:05:33,074\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 09:05:45,875\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 09:05:58,723\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 09:06:11,254\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 09:06:23,728\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 09:06:36,590\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 09:06:48,610\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 09:07:01,137\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 09:07:13,576\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 09:07:26,312\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 09:07:38,062\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 09:07:50,731\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 09:08:02,692\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 09:08:14,982\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 09:08:27,754\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 09:08:40,398\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 09:08:53,629\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 09:09:04,672\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 09:09:17,749\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 09:09:30,652\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 09:09:40,609\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=105657)\u001b[0m 2019-07-12 09:09:55,686\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-12 09:10:04,332\tERROR trial_runner.py:487 -- Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/Nicholas/.venv/lib/python3.6/site-packages/ray/tune/trial_runner.py\", line 436, in _process_trial\n",
      "    result = self.trial_executor.fetch_result(trial)\n",
      "  File \"/home/Nicholas/.venv/lib/python3.6/site-packages/ray/tune/ray_trial_executor.py\", line 323, in fetch_result\n",
      "    result = ray.get(trial_future[0])\n",
      "  File \"/home/Nicholas/.venv/lib/python3.6/site-packages/ray/worker.py\", line 2195, in get\n",
      "    raise value\n",
      "ray.exceptions.RayTaskError: \u001b[36mray_PPO:train()\u001b[39m (pid=105657, host=Nicholas)\n",
      "  File \"/home/Nicholas/.venv/lib/python3.6/site-packages/ray/memory_monitor.py\", line 77, in raise_if_low_memory\n",
      "    self.error_threshold))\n",
      "ray.memory_monitor.RayOutOfMemoryError: More than 95% of the memory on node Nicholas is used (64.17 / 67.53 GB). The top 5 memory consumers are:\n",
      "\n",
      "PID\tMEM\tCOMMAND\n",
      "67034\t15.03GB\t/home/Nicholas/anaconda3/bin/python /home/Nicholas/anaconda3/bin/tensorboard --logdir /home/Nicholas\n",
      "58853\t11.56GB\t/home/Nicholas/Desktop/trading-gym/venv/bin/python /home/Nicholas/Desktop/trading-gym/venv/bin/tenso\n",
      "102842\t8.69GB\t/home/Nicholas/Desktop/trading-gym/venv/bin/python /home/Nicholas/Desktop/trading-gym/venv/bin/tenso\n",
      "42895\t2.13GB\t/home/Nicholas/.venv/bin/python3 -m ipykernel_launcher -f /home/Nicholas/.local/share/jupyter/runtim\n",
      "72912\t1.75GB\tray_RolloutWorker\n",
      "\n",
      "In addition, ~2.25 GB of shared memory is currently being used by the Ray object store. You can set the object store size with the `object_store_memory` parameter when starting Ray, and the max Redis size with `redis_max_memory`.\n",
      "\n",
      "2019-07-12 09:10:04,339\tINFO ray_trial_executor.py:187 -- Destroying actor for trial PPO_GAIAPredictorsContinuousV7_0_clip_param=0.7,entropy_coeff=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n"
     ]
    },
    {
     "ename": "TuneError",
     "evalue": "('Trials did not complete', [PPO_GAIAPredictorsContinuousV7_0_clip_param=0.7,entropy_coeff=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTuneError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-23a39367093f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mreuse_actors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mresume\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     )\n",
      "\u001b[0;32m~/.venv/lib/python3.6/site-packages/ray/tune/tune.py\u001b[0m in \u001b[0;36mrun_experiments\u001b[0;34m(experiments, search_alg, scheduler, with_server, server_port, verbose, resume, queue_trials, reuse_actors, trial_executor, raise_on_failed_trial)\u001b[0m\n\u001b[1;32m    331\u001b[0m             \u001b[0mreuse_actors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreuse_actors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m             \u001b[0mtrial_executor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial_executor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m             raise_on_failed_trial=raise_on_failed_trial)\n\u001b[0m\u001b[1;32m    334\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venv/lib/python3.6/site-packages/ray/tune/tune.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, stop, config, resources_per_trial, num_samples, local_dir, upload_dir, trial_name_creator, loggers, sync_function, checkpoint_freq, checkpoint_at_end, export_formats, max_failures, restore, search_alg, scheduler, with_server, server_port, verbose, resume, queue_trials, reuse_actors, trial_executor, raise_on_failed_trial, return_trials, ray_auto_init)\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merrored_trials\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mraise_on_failed_trial\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTuneError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrored_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrored_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTuneError\u001b[0m: ('Trials did not complete', [PPO_GAIAPredictorsContinuousV7_0_clip_param=0.7,entropy_coeff=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000])"
     ]
    }
   ],
   "source": [
    "for year in range(2013, 2018):\n",
    "    print('_______________________________________{}____________________________________________'.format(year))\n",
    "    config['env_config'] = {\n",
    "        'folds': {\n",
    "            'training-set': [datetime.min, datetime(year, 12, 31)],\n",
    "            'test-set': [datetime(year + 1, 1, 1), datetime(year + 1, 12, 31)],\n",
    "        }\n",
    "    }\n",
    "    experiment = tune.Experiment(\n",
    "        name='PPOclip_0.7-WalkForward-750k{}'.format(year),\n",
    "        run=rllib.agents.ppo.PPOTrainer,\n",
    "        stop={\"timesteps_total\": 750000},\n",
    "        config=deepcopy(config),\n",
    "        num_samples=1,\n",
    "        local_dir='logs',\n",
    "        #checkpoint_freq=int(1e4 / config['train_batch_size']),  # checkpoint every 100k iters\n",
    "        checkpoint_at_end=True,\n",
    "        max_failures=0,\n",
    "        loggers=[CustomLogger],\n",
    "    )\n",
    "    trials = tune.run_experiments(\n",
    "        experiments=experiment,\n",
    "        search_alg=tune.suggest.BasicVariantGenerator(),\n",
    "        scheduler=tune.schedulers.FIFOScheduler(),\n",
    "        verbose=0,\n",
    "        reuse_actors=False,\n",
    "        resume=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import cloudpickle\n",
    "from ray.utils import binary_to_hex, hex_to_binary\n",
    "\n",
    "\n",
    "def cloudpickleloads(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        try:\n",
    "            return cloudpickle.loads(hex_to_binary(obj[\"value\"]))\n",
    "        except:\n",
    "            for key, value in obj.items():\n",
    "                if isinstance(value, dict):\n",
    "                    if sorted(value) == ['_type', 'value']:\n",
    "                        obj[key] = cloudpickle.loads(hex_to_binary(value[\"value\"]))\n",
    "                    else:\n",
    "                        obj[key] = cloudpickleloads(value)\n",
    "                elif isinstance(value, list):\n",
    "                    for i, item in enumerate(value):\n",
    "                        obj[key][i] = cloudpickleloads(item)\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# raise ValueError('TODO: update paths with latest runs')\n",
    "#(1.0 clip)\n",
    "paths = {2007: '/home/Nicholas/Desktop/trading-gym/notebooks/registry/gaia/v7/PPO Tuning/logs/PPOclip_0.8-WalkForward-750k2007/experiment_state-2019-07-01_10-37-58.json',\n",
    "        2008: '/home/Nicholas/Desktop/trading-gym/notebooks/registry/gaia/v7/PPO Tuning/logs/PPOclip_0.8-WalkForward-750k2008/experiment_state-2019-07-01_12-18-57.json',\n",
    "        2009: '/home/Nicholas/Desktop/trading-gym/notebooks/registry/gaia/v7/PPO Tuning/logs/PPOclip_0.8-WalkForward-750k2009/experiment_state-2019-07-01_13-41-34.json',\n",
    "        2010: '/home/Nicholas/Desktop/trading-gym/notebooks/registry/gaia/v7/PPO Tuning/logs/PPOclip_0.8-WalkForward-750k2010/experiment_state-2019-07-01_14-50-33.json',\n",
    "        2011: '/home/Nicholas/Desktop/trading-gym/notebooks/registry/gaia/v7/PPO Tuning/logs/PPOclip_0.8-WalkForward-750k2011/experiment_state-2019-07-01_15-59-02.json',\n",
    "        2012: '/home/Nicholas/Desktop/trading-gym/notebooks/registry/gaia/v7/PPO Tuning/logs/PPOclip_0.8-WalkForward-750k2012/experiment_state-2019-07-01_17-19-21.json',\n",
    "        2013: '/home/Nicholas/Desktop/trading-gym/notebooks/registry/gaia/v7/PPO Tuning/logs/PPOclip_0.8-WalkForward-750k2013/experiment_state-2019-07-02_11-49-44.json',\n",
    "        2014: '/home/Nicholas/Desktop/trading-gym/notebooks/registry/gaia/v7/PPO Tuning/logs/PPOclip_0.8-WalkForward-750k2014/experiment_state-2019-07-02_12-12-03.json',\n",
    "        2015: '/home/Nicholas/Desktop/trading-gym/notebooks/registry/gaia/v7/PPO Tuning/logs/PPOclip_0.8-WalkForward-750k2015/experiment_state-2019-07-02_12-42-28.json',\n",
    "        2016: '/home/Nicholas/Desktop/trading-gym/notebooks/registry/gaia/v7/PPO Tuning/logs/PPOclip_0.8-WalkForward-750k2016/experiment_state-2019-07-02_13-14-05.json',\n",
    "        2017: '/home/Nicholas/Desktop/trading-gym/notebooks/registry/gaia/v7/PPO Tuning/logs/PPOclip_0.8-WalkForward-750k2017/experiment_state-2019-07-02_13-46-32.json'\n",
    "        }\n",
    "\n",
    "# These are for the 750k runs (0.8 clip) note they're saved in the normal logs folder (have to run it in copy1)\n",
    "# These are the best results\n",
    "# paths = {2007: '/home/Nicholas/Desktop/trading-gym/notebooks/registry/gaia/v7/logs/PPOclip_0.8-WalkForward-750k2007/experiment_state-2019-06-24_23-35-32.json',\n",
    "#         2008: '/home/Nicholas/Desktop/trading-gym/notebooks/registry/gaia/v7/logs/PPOclip_0.8-WalkForward-750k2008/experiment_state-2019-06-25_00-28-20.json',\n",
    "#         2009: '/home/Nicholas/Desktop/trading-gym/notebooks/registry/gaia/v7/logs/PPOclip_0.8-WalkForward-750k2009/experiment_state-2019-06-25_01-21-47.json',\n",
    "#         2010: '/home/Nicholas/Desktop/trading-gym/notebooks/registry/gaia/v7/logs/PPOclip_0.8-WalkForward-750k2010/experiment_state-2019-06-25_02-14-52.json',\n",
    "#         2011: '/home/Nicholas/Desktop/trading-gym/notebooks/registry/gaia/v7/logs/PPOclip_0.8-WalkForward-750k2011/experiment_state-2019-06-25_03-08-23.json',\n",
    "#         2012: '/home/Nicholas/Desktop/trading-gym/notebooks/registry/gaia/v7/logs/PPOclip_0.8-WalkForward-750k2012/experiment_state-2019-06-25_04-00-54.json',\n",
    "#         2013: '/home/Nicholas/Desktop/trading-gym/notebooks/registry/gaia/v7/logs/PPOclip_0.8-WalkForward-750k2013/experiment_state-2019-06-25_04-54-29.json',\n",
    "#         2014: '/home/Nicholas/Desktop/trading-gym/notebooks/registry/gaia/v7/logs/PPOclip_0.8-WalkForward-750k2014/experiment_state-2019-06-25_05-47-18.json',\n",
    "#         2015: '/home/Nicholas/Desktop/trading-gym/notebooks/registry/gaia/v7/logs/PPOclip_0.8-WalkForward-750k2015/experiment_state-2019-06-25_06-40-12.json',\n",
    "#         2016: '/home/Nicholas/Desktop/trading-gym/notebooks/registry/gaia/v7/logs/PPOclip_0.8-WalkForward-750k2016/experiment_state-2019-06-25_07-33-17.json',\n",
    "#         2017: '/home/Nicholas/Desktop/trading-gym/notebooks/registry/gaia/v7/logs/PPOclip_0.8-WalkForward-750k2017/experiment_state-2019-06-25_08-26-33.json'\n",
    "#         }\n",
    "\n",
    "#  (0.9 clip) \n",
    "# paths = {2007: '/home/Nicholas/Desktop/trading-gym/notebooks/registry/gaia/v7/PPO Tuning/logs/PPOclip_0.9-WalkForward-750k2007/experiment_state-2019-07-02_22-13-01.json',\n",
    "#         2008: '/home/Nicholas/Desktop/trading-gym/notebooks/registry/gaia/v7/PPO Tuning/logs/PPOclip_0.9-WalkForward-750k2008/experiment_state-2019-07-02_22-45-20.json',\n",
    "#         2009: '/home/Nicholas/Desktop/trading-gym/notebooks/registry/gaia/v7/PPO Tuning/logs/PPOclip_0.9-WalkForward-750k2009/experiment_state-2019-07-02_23-09-36.json',\n",
    "#         2010: '/home/Nicholas/Desktop/trading-gym/notebooks/registry/gaia/v7/PPO Tuning/logs/PPOclip_0.9-WalkForward-750k2010/experiment_state-2019-07-02_23-33-28.json',\n",
    "#         2011: '/home/Nicholas/Desktop/trading-gym/notebooks/registry/gaia/v7/PPO Tuning/logs/PPOclip_0.9-WalkForward-750k2011/experiment_state-2019-07-03_00-03-19.json',\n",
    "#         2012: '/home/Nicholas/Desktop/trading-gym/notebooks/registry/gaia/v7/PPO Tuning/logs/PPOclip_0.9-WalkForward-750k2012/experiment_state-2019-07-03_00-50-22.json',\n",
    "#         2013: '/home/Nicholas/Desktop/trading-gym/notebooks/registry/gaia/v7/PPO Tuning/logs/PPOclip_0.9-WalkForward-750k2013/experiment_state-2019-07-02_17-15-12.json',\n",
    "#         2014: '/home/Nicholas/Desktop/trading-gym/notebooks/registry/gaia/v7/PPO Tuning/logs/PPOclip_0.9-WalkForward-750k2014/experiment_state-2019-07-02_17-47-30.json',\n",
    "#         2015: '/home/Nicholas/Desktop/trading-gym/notebooks/registry/gaia/v7/PPO Tuning/logs/PPOclip_0.9-WalkForward-750k2015/experiment_state-2019-07-02_18-22-00.json',\n",
    "#         2016: '/home/Nicholas/Desktop/trading-gym/notebooks/registry/gaia/v7/PPO Tuning/logs/PPOclip_0.9-WalkForward-750k2016/experiment_state-2019-07-02_18-56-40.json',\n",
    "#         2017: '/home/Nicholas/Desktop/trading-gym/notebooks/registry/gaia/v7/PPO Tuning/logs/PPOclip_0.9-WalkForward-750k2017/experiment_state-2019-07-02_19-30-56.json'\n",
    "#         }\n",
    "\n",
    "#  Insert 0.7 clip paths (need to add the rest)\n",
    "# paths = {2007: '/home/Nicholas/Desktop/trading-gym/notebooks/registry/gaia/v7/PPO Tuning/logs/PPOclip_0.7-WalkForward-750k2007/experiment_state-2019-07-03_09-32-28.json',\n",
    "#         2008: '/home/Nicholas/Desktop/trading-gym/notebooks/registry/gaia/v7/PPO Tuning/logs/PPOclip_0.7-WalkForward-750k2008/experiment_state-2019-07-03_10-13-04.json',\n",
    "#         2009: '/home/Nicholas/Desktop/trading-gym/notebooks/registry/gaia/v7/PPO Tuning/logs/PPOclip_0.7-WalkForward-750k2009/experiment_state-2019-07-03_10-56-47.json',\n",
    "#         2010: '/home/Nicholas/Desktop/trading-gym/notebooks/registry/gaia/v7/PPO Tuning/logs/PPOclip_0.7-WalkForward-750k2010/experiment_state-2019-07-03_11-38-06.json',\n",
    "#         2011: '/home/Nicholas/Desktop/trading-gym/notebooks/registry/gaia/v7/PPO Tuning/logs/PPOclip_0.7-WalkForward-750k2011/experiment_state-2019-07-03_12-15-59.json',\n",
    "#         2012: '/home/Nicholas/Desktop/trading-gym/notebooks/registry/gaia/v7/PPO Tuning/logs/PPOclip_0.7-WalkForward-750k2012/experiment_state-2019-07-03_12-51-25.json',\n",
    "#         2013: '',\n",
    "#         2014: '',\n",
    "#         2015: '',\n",
    "#         2016: '',\n",
    "#         2017: ''\n",
    "#         }\n",
    "\n",
    "#  These are actually 0.9 clip \n",
    "# paths = {2007: '/home/Nicholas/Desktop/trading-gym/notebooks/registry/gaia/v7/PPO Tuning/logs/PPOclip_1.1-WalkForward-750k2007/experiment_state-2019-07-03_14-23-27.json',\n",
    "#         2008: '/home/Nicholas/Desktop/trading-gym/notebooks/registry/gaia/v7/PPO Tuning/logs/PPOclip_1.1-WalkForward-750k2008/experiment_state-2019-07-03_15-09-40.json',\n",
    "#         2009: '/home/Nicholas/Desktop/trading-gym/notebooks/registry/gaia/v7/PPO Tuning/logs/PPOclip_1.1-WalkForward-750k2009/experiment_state-2019-07-03_15-58-09.json',\n",
    "#         2010: '/home/Nicholas/Desktop/trading-gym/notebooks/registry/gaia/v7/PPO Tuning/logs/PPOclip_1.1-WalkForward-750k2010/experiment_state-2019-07-03_16-43-36.json',\n",
    "#         2011: '/home/Nicholas/Desktop/trading-gym/notebooks/registry/gaia/v7/PPO Tuning/logs/PPOclip_1.1-WalkForward-750k2011/experiment_state-2019-07-03_17-31-51.json',\n",
    "#         2012: '/home/Nicholas/Desktop/trading-gym/notebooks/registry/gaia/v7/PPO Tuning/logs/PPOclip_1.1-WalkForward-750k2012/experiment_state-2019-07-03_18-19-23.json',\n",
    "#         2013: '/home/Nicholas/Desktop/trading-gym/notebooks/registry/gaia/v7/PPO Tuning/logs/PPOclip_1.1-WalkForward-750k2013/experiment_state-2019-07-03_18-56-29.json',\n",
    "#         2014: '/home/Nicholas/Desktop/trading-gym/notebooks/registry/gaia/v7/PPO Tuning/logs/PPOclip_1.1-WalkForward-750k2014/experiment_state-2019-07-03_19-29-31.json',\n",
    "#         2015: '/home/Nicholas/Desktop/trading-gym/notebooks/registry/gaia/v7/PPO Tuning/logs/PPOclip_1.1-WalkForward-750k2015/experiment_state-2019-07-03_20-04-11.json',\n",
    "#         2016: '/home/Nicholas/Desktop/trading-gym/notebooks/registry/gaia/v7/PPO Tuning/logs/PPOclip_1.1-WalkForward-750k2016/experiment_state-2019-07-03_20-37-50.json',\n",
    "#         2017: '/home/Nicholas/Desktop/trading-gym/notebooks/registry/gaia/v7/PPO Tuning/logs/PPOclip_1.1-WalkForward-750k2017/experiment_state-2019-07-03_21-11-29.json'\n",
    "#         }\n",
    "\n",
    "# These will now be the real 1.1 clip param  (note that they end up being found in the same folder, just have different subfolders)\n",
    "paths = {2007: '',\n",
    "        2008: '',\n",
    "        2009: '/home/Nicholas/Desktop/trading-gym/notebooks/registry/gaia/v7/PPO Tuning/logs/PPOclip_1.1-WalkForward-750k2009/experiment_state-2019-07-03_15-58-09.json',\n",
    "        2010: '/home/Nicholas/Desktop/trading-gym/notebooks/registry/gaia/v7/PPO Tuning/logs/PPOclip_1.1-WalkForward-750k2010/experiment_state-2019-07-03_16-43-36.json',\n",
    "        2011: '/home/Nicholas/Desktop/trading-gym/notebooks/registry/gaia/v7/PPO Tuning/logs/PPOclip_1.1-WalkForward-750k2011/experiment_state-2019-07-03_17-31-51.json',\n",
    "        2012: '/home/Nicholas/Desktop/trading-gym/notebooks/registry/gaia/v7/PPO Tuning/logs/PPOclip_1.1-WalkForward-750k2012/experiment_state-2019-07-03_18-19-23.json',\n",
    "        2013: '/home/Nicholas/Desktop/trading-gym/notebooks/registry/gaia/v7/PPO Tuning/logs/PPOclip_1.1-WalkForward-750k2013/experiment_state-2019-07-03_18-56-29.json',\n",
    "        2014: '/home/Nicholas/Desktop/trading-gym/notebooks/registry/gaia/v7/PPO Tuning/logs/PPOclip_1.1-WalkForward-750k2014/experiment_state-2019-07-03_19-29-31.json',\n",
    "        2015: '/home/Nicholas/Desktop/trading-gym/notebooks/registry/gaia/v7/PPO Tuning/logs/PPOclip_1.1-WalkForward-750k2015/experiment_state-2019-07-03_20-04-11.json',\n",
    "        2016: '/home/Nicholas/Desktop/trading-gym/notebooks/registry/gaia/v7/PPO Tuning/logs/PPOclip_1.1-WalkForward-750k2016/experiment_state-2019-07-03_20-37-50.json',\n",
    "        2017: '/home/Nicholas/Desktop/trading-gym/notebooks/registry/gaia/v7/PPO Tuning/logs/PPOclip_1.1-WalkForward-750k2017/experiment_state-2019-07-03_21-11-29.json'\n",
    "        }\n",
    "\n",
    "# These will be the 0.6 clip - they'll be the third set of experiments in the 1.1 folderssss\n",
    "\n",
    "# These will be the 0.5 clip - they'll be the 4th set of experiments in the 1.1 folder \n",
    "\n",
    "\n",
    "\n",
    "episodes = dict()\n",
    "agents = dict()\n",
    "for year, path in paths.items():\n",
    "    # RESTORE part (a)\n",
    "    with open(path) as f:\n",
    "        metadata = json.load(f)\n",
    "\n",
    "    runner_data = metadata['runner_data']\n",
    "    stats = metadata['stats']\n",
    "\n",
    "    checkpoint = metadata['checkpoints'][-1]\n",
    "    checkpoint = cloudpickleloads(checkpoint)\n",
    "    checkpoint_path = cloudpickle.loads(hex_to_binary(checkpoint['_checkpoint'])).value\n",
    "\n",
    "    config = checkpoint['config']\n",
    "#     Don't actually need to redefine the env_cls as it's always the same \n",
    "    env_cls = config['env']\n",
    "    env_config = config['env_config']\n",
    "    path_restore = os.path.join(checkpoint['logdir'], checkpoint_path)\n",
    "    \n",
    "    # RESTORE part (b)\n",
    "    agent = rllib.agents.ppo.PPOTrainer(config, env_cls)\n",
    "    agent.restore(path_restore)\n",
    "# THIS IS A BUG: \n",
    "#     agent._restore(path_restore)\n",
    "    \n",
    "    env = env_cls(env_config)\n",
    "    episode = env.sample_episode(\n",
    "        fold='test-set',\n",
    "        policy=agent,\n",
    "        episode_length=None,\n",
    "        benchmark=env._load_benchmark().squeeze(),\n",
    "        risk_free=env._load_risk_free().squeeze(),\n",
    "        burn=1,\n",
    "    )\n",
    "    \n",
    "    episodes[year] = episode\n",
    "    agents[year] = agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year,path in paths.items():\n",
    "    with open(path) as f:\n",
    "        metadata = json.load(f)\n",
    "\n",
    "    runner_data = metadata['runner_data']\n",
    "    stats = metadata['stats']\n",
    "\n",
    "    checkpoint = metadata['checkpoints'][-1]\n",
    "    checkpoint = cloudpickleloads(checkpoint)\n",
    "    checkpoint_path = cloudpickle.loads(hex_to_binary(checkpoint['_checkpoint'])).value\n",
    "\n",
    "    config = checkpoint['config']\n",
    "    print(config)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "levels = list()\n",
    "mappings = pd.DataFrame()\n",
    "mapping_functions = dict()\n",
    "for year in paths:\n",
    "    episode = episodes[year]\n",
    "    agent = agents[year]\n",
    "\n",
    "    # Load.\n",
    "    actions = episode.actions_as_frame()\n",
    "    states = episode.states_as_frame()\n",
    "    \n",
    "    # Parse.\n",
    "    gaia_predictor = states[0].to_frame('GAIA Predictor')\n",
    "    \n",
    "#     The following line was here before\n",
    "#     target_weight_russell_1000 = actions[ETF('Russell 1000')]\n",
    "    target_weight_russell_1000 = actions[actions.columns[0]]\n",
    "    target_weight_russell_1000.name = 'Target weight: ' + str(target_weight_russell_1000.name)\n",
    "    mapping = gaia_predictor.join(target_weight_russell_1000)\n",
    "    mapping_function = mapping.set_index('GAIA Predictor')\n",
    "\n",
    "    levels.append(episode.renderer.level.to_frame().pct_change())\n",
    "    mappings = mappings.append(mapping)\n",
    "    mapping_functions[year] = mapping_function\n",
    "\n",
    "    # Visualize.\n",
    "    mapping.iplot(\n",
    "        title=\"Hisorical GAIA predictor for Russell 1000 vs agent's target weights\",\n",
    "        secondary_y='GAIA Predictor',\n",
    "        yTitle=target_weight_russell_1000.name,\n",
    "        secondary_y_title='GAIA Predictor',\n",
    "        legend={'orientation': 'h'},\n",
    "    )\n",
    "    mapping_function.iplot(\n",
    "        title='Policy: mapping from GAIA predictor (state) to target weight for Russell 1000 (action)',\n",
    "        xTitle='GAIA predictor for Russell 1000 (standardized)',\n",
    "        yTitle='Target weight for Russell 1000',\n",
    "        kind='scatter',\n",
    "        mode='markers',\n",
    "        size=4,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "daily_ret = pd.concat(levels).sort_index().fillna(0)\n",
    "cumulative_performance = (1 + daily_ret).cumprod() - 1\n",
    "cumulative_performance *= 100\n",
    "\n",
    "aric = cumulative_performance.columns[1]\n",
    "cumulative_performance['Strategy relative to Aric-Benchmark'] = cumulative_performance['Strategy'] - cumulative_performance[aric]\n",
    "\n",
    "\n",
    "# Visualizations.\n",
    "cumulative_performance.iplot(\n",
    "    legend={'orientation': 'h'},\n",
    "    yTitle='Total returns',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "levels = (1 + cumulative_performance / 100)\n",
    "annual_rets = (levels.resample('Y').last() / levels.resample('Y').first() - 1)\n",
    "\n",
    "    \n",
    "annual_rets['Strategy relative to Aric-Benchmark'] = annual_rets['Strategy'] - annual_rets[aric]\n",
    "annual_rets.index = annual_rets.index.year\n",
    "annual_rets *= 100\n",
    "annual_rets.iplot(kind='bar', legend={'orientation': 'h'}, yTitle='%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "levels.drop('Strategy relative to Aric-Benchmark', axis='columns').tearsheet(\n",
    "    benchmark=env._load_benchmark().loc['2008':].squeeze(),\n",
    "    risk_free=env._load_risk_free().loc['2008':].squeeze(),\n",
    "    weights=env.broker.track_record.to_frame('weights_target').iloc[1:]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "import cufflinks\n",
    "cufflinks.go_offline()\n",
    "init_notebook_mode(connected=False)\n",
    "\n",
    "\n",
    "traces = list()\n",
    "for year, series in mapping_functions.items():\n",
    "    trace = go.Scatter(\n",
    "        x = list(series.squeeze().index[:-1]),\n",
    "        y = list(series.squeeze().values[:-1]),\n",
    "        mode = 'markers',\n",
    "        name = year\n",
    "    )\n",
    "    traces.append(trace)\n",
    "    \n",
    "layout = go.Layout(\n",
    "    title='GAIA vs RL mapping functions',\n",
    "    xaxis=dict(\n",
    "        title='GAIA Mapping'\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='PPO Mapping'\n",
    "        )\n",
    "        \n",
    "    )\n",
    "fig = go.Figure(data=traces,layout=layout)\n",
    "iplot(fig,filename='scatter=mode')\n",
    "\n",
    "# iplot(traces, filename='scatter-mode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
