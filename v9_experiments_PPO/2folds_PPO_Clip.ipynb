{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trading-gym 0.8.1\n",
      "ray 0.7.2\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from ray import rllib, tune\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import trading_gym\n",
    "from trading_gym.registry.gaia.v9.env import GAIAPredictorsContinuousV9\n",
    "from trading_gym.registry.gaia.v10.env import GAIAPredictorsContinuousV10\n",
    "from trading_gym.ray.walkforward import WalkForwardRunner, WalkForwardResults\n",
    "%matplotlib inline\n",
    "print(trading_gym.__package__, trading_gym.__version__)\n",
    "print(ray.__package__, ray.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-31 16:45:22,820\tINFO node.py:498 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-07-31_16-45-22_816547_86284/logs.\n",
      "2019-07-31 16:45:23,009\tINFO services.py:409 -- Waiting for redis server at 127.0.0.1:16985 to respond...\n",
      "2019-07-31 16:45:23,154\tINFO services.py:409 -- Waiting for redis server at 127.0.0.1:60375 to respond...\n",
      "2019-07-31 16:45:23,161\tINFO services.py:806 -- Starting Redis shard with 10.0 GB max memory.\n",
      "2019-07-31 16:45:23,355\tINFO node.py:512 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-07-31_16-45-22_816547_86284/logs.\n",
      "2019-07-31 16:45:23,358\tWARNING services.py:1298 -- Warning: Capping object memory store to 20.0GB. To increase this further, specify `object_store_memory` when calling ray.init() or ray start.\n",
      "2019-07-31 16:45:23,359\tINFO services.py:1446 -- Starting the Plasma object store with 20.0 GB memory using /dev/shm.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0.7.2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray\n",
    "from ray import rllib, tune\n",
    "from trading_gym.ray.logger import calculate_tearsheet, CustomLogger\n",
    "from copy import deepcopy\n",
    "# ray.init(num_cpus=8,ignore_reinit_error=True,object_store_memory= 10*100 )\n",
    "ray.init(ignore_reinit_error=True)\n",
    "#          object_store_memory = 50000000)\n",
    "\n",
    "ray.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<trading_gym.registry.gaia.v9.env.GAIAPredictorsContinuousV9 at 0x7f0fd42919e8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_config = dict()\n",
    "env_config['folds'] =  {\n",
    "    'training-set': [datetime.min, datetime(2008, 3, 18)],\n",
    "    'test-set': [datetime(2008, 3, 19), datetime.max],\n",
    "}\n",
    "env = GAIAPredictorsContinuousV9(env_config)\n",
    "env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the config dict\n",
    "config = ray.rllib.agents.ppo.DEFAULT_CONFIG.copy()\n",
    "config['env'] = GAIAPredictorsContinuousV9\n",
    "config['env_config'] = {\n",
    "    'cost_of_commissions': tune.grid_search([0.00005]),\n",
    "    'cost_of_spread': 0.0001,\n",
    "}\n",
    "config['gamma'] = 0.82  # 2 weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['vf_clip_param'] = 0\n",
    "config['vf_loss_coeff'] = 0\n",
    "config['lambda'] = 0\n",
    "config['use_gae'] = False\n",
    "\n",
    "# need to have vf share layers if lstm is used\n",
    "config['vf_share_layers'] = False \n",
    "config['use_lstm']: False\n",
    "config['batch_mode'] = 'complete_episodes'\n",
    "config['train_batch_size'] = tune.grid_search([4000])\n",
    "config['sgd_minibatch_size'] = 128\n",
    "config['num_sgd_iter'] = tune.grid_search([8])\n",
    "config['entropy_coeff'] = tune.grid_search([1e-5])\n",
    "config['kl_coeff'] = tune.grid_search([0.2])\n",
    "config['kl_target'] = tune.grid_search([0.01])\n",
    "config['clip_param'] = tune.grid_search([0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0,1.1])\n",
    "\n",
    "config['lr'] = tune.grid_search([1e-5])\n",
    "\n",
    "# config['model']['custom_model'] = CNN.__name__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = tune.Experiment(\n",
    "    name='clip_exps',\n",
    "    run=rllib.agents.ppo.PPOTrainer,\n",
    "    stop={\"timesteps_total\": 100000},\n",
    "    config=deepcopy(config),\n",
    "#     This determines the number of times the grid search is run. s\n",
    "    num_samples=1,\n",
    "    local_dir='logs/clip_exps_aric',\n",
    "    #checkpoint_freq=int(1e4 / config['train_batch_size']),  # checkpoint every 100k iters\n",
    "    checkpoint_at_end=True,\n",
    "    max_failures=0,\n",
    "    loggers=[CustomLogger],\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-31 16:59:24,371\tINFO tune.py:65 -- Did not find checkpoint file in logs/clip_exps_aric/clip_exps.\n",
      "2019-07-31 16:59:24,379\tINFO tune.py:233 -- Starting a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 38.5/67.5 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-31 16:59:24,792\tWARNING signature.py:108 -- The function with_updates has a **kwargs argument, which is currently not supported.\n",
      "2019-07-31 16:59:24,919\tWARNING util.py:64 -- The `start_trial` operation took 0.14847111701965332 seconds to complete, which may be a performance bottleneck.\n",
      "2019-07-31 16:59:24,990\tWARNING signature.py:108 -- The function with_updates has a **kwargs argument, which is currently not supported.\n",
      "2019-07-31 16:59:25,080\tWARNING util.py:64 -- The `start_trial` operation took 0.10506844520568848 seconds to complete, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 38.5/67.5 GB\n",
      "Result logdir: logs/clip_exps_aric/clip_exps\n",
      "Number of trials: 11 ({'RUNNING': 1, 'PENDING': 10})\n",
      "PENDING trials:\n",
      " - PPO_GAIAPredictorsContinuousV9_1_clip_param=0.2,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tPENDING\n",
      " - PPO_GAIAPredictorsContinuousV9_2_clip_param=0.3,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tPENDING\n",
      " - PPO_GAIAPredictorsContinuousV9_3_clip_param=0.4,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tPENDING\n",
      " - PPO_GAIAPredictorsContinuousV9_4_clip_param=0.5,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tPENDING\n",
      " - PPO_GAIAPredictorsContinuousV9_5_clip_param=0.6,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tPENDING\n",
      " - PPO_GAIAPredictorsContinuousV9_6_clip_param=0.7,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tPENDING\n",
      " - PPO_GAIAPredictorsContinuousV9_7_clip_param=0.8,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tPENDING\n",
      " - PPO_GAIAPredictorsContinuousV9_8_clip_param=0.9,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tPENDING\n",
      " - PPO_GAIAPredictorsContinuousV9_9_clip_param=1.0,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tPENDING\n",
      " - PPO_GAIAPredictorsContinuousV9_10_clip_param=1.1,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tPENDING\n",
      "RUNNING trials:\n",
      " - PPO_GAIAPredictorsContinuousV9_0_clip_param=0.1,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tRUNNING\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=89162)\u001b[0m 2019-07-31 16:59:39,637\tWARNING ppo.py:151 -- FYI: By default, the value function will not share layers with the policy model ('vf_share_layers': False).\n",
      "\u001b[2m\u001b[36m(pid=89159)\u001b[0m 2019-07-31 16:59:39,724\tWARNING ppo.py:151 -- FYI: By default, the value function will not share layers with the policy model ('vf_share_layers': False).\n",
      "\u001b[2m\u001b[36m(pid=89159)\u001b[0m 2019-07-31 16:59:44,305\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 0 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=89159)\u001b[0m 2019-07-31 16:59:44.306481: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=89162)\u001b[0m 2019-07-31 16:59:44,331\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 0 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=89162)\u001b[0m 2019-07-31 16:59:44.332833: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=89159)\u001b[0m 2019-07-31 16:59:44,867\tINFO dynamic_tf_policy.py:313 -- Initializing loss function with dummy input:\n",
      "\u001b[2m\u001b[36m(pid=89159)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=89159)\u001b[0m { 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89159)\u001b[0m   'actions': <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89159)\u001b[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89159)\u001b[0m   'behaviour_logits': <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 4) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89159)\u001b[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,\n",
      "\u001b[2m\u001b[36m(pid=89159)\u001b[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89159)\u001b[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89159)\u001b[0m   'prev_actions': <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89159)\u001b[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89159)\u001b[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89159)\u001b[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89159)\u001b[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}\n",
      "\u001b[2m\u001b[36m(pid=89159)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=89162)\u001b[0m 2019-07-31 16:59:44,945\tINFO dynamic_tf_policy.py:313 -- Initializing loss function with dummy input:\n",
      "\u001b[2m\u001b[36m(pid=89162)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=89162)\u001b[0m { 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89162)\u001b[0m   'actions': <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89162)\u001b[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89162)\u001b[0m   'behaviour_logits': <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 4) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89162)\u001b[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,\n",
      "\u001b[2m\u001b[36m(pid=89162)\u001b[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89162)\u001b[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89162)\u001b[0m   'prev_actions': <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89162)\u001b[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89162)\u001b[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89162)\u001b[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89162)\u001b[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}\n",
      "\u001b[2m\u001b[36m(pid=89162)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=89159)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=89159)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=89159)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=89159)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=89162)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=89162)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=89162)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=89162)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=89162)\u001b[0m 2019-07-31 16:59:48,446\tINFO rollout_worker.py:719 -- Built policy map: {'default_policy': <ray.rllib.policy.tf_policy_template.PPOTFPolicy object at 0x7f3b89b2ea58>}\n",
      "\u001b[2m\u001b[36m(pid=89162)\u001b[0m 2019-07-31 16:59:48,447\tINFO rollout_worker.py:720 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7f3b89b2e710>}\n",
      "\u001b[2m\u001b[36m(pid=89162)\u001b[0m 2019-07-31 16:59:48,447\tINFO rollout_worker.py:333 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7f3b89b2e5c0>}\n",
      "\u001b[2m\u001b[36m(pid=89162)\u001b[0m 2019-07-31 16:59:48,632\tINFO multi_gpu_optimizer.py:79 -- LocalMultiGPUOptimizer devices ['/cpu:0']\n",
      "\u001b[2m\u001b[36m(pid=89159)\u001b[0m 2019-07-31 16:59:48,836\tINFO rollout_worker.py:719 -- Built policy map: {'default_policy': <ray.rllib.policy.tf_policy_template.PPOTFPolicy object at 0x7fd176965a20>}\n",
      "\u001b[2m\u001b[36m(pid=89159)\u001b[0m 2019-07-31 16:59:48,837\tINFO rollout_worker.py:720 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7fd1769656d8>}\n",
      "\u001b[2m\u001b[36m(pid=89159)\u001b[0m 2019-07-31 16:59:48,838\tINFO rollout_worker.py:333 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7fd176965588>}\n",
      "\u001b[2m\u001b[36m(pid=89159)\u001b[0m 2019-07-31 16:59:48,975\tINFO multi_gpu_optimizer.py:79 -- LocalMultiGPUOptimizer devices ['/cpu:0']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m 2019-07-31 17:00:09,022\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 1 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m 2019-07-31 17:00:09.123210: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m 2019-07-31 17:00:09,339\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 1 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m 2019-07-31 17:00:09.399745: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m 2019-07-31 17:00:09,601\tINFO dynamic_tf_policy.py:313 -- Initializing loss function with dummy input:\n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m { 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m   'actions': <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m   'behaviour_logits': <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 4) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,\n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m   'prev_actions': <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}\n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m 2019-07-31 17:00:09,805\tINFO dynamic_tf_policy.py:313 -- Initializing loss function with dummy input:\n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m { 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m   'actions': <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m   'behaviour_logits': <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 4) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,\n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m   'prev_actions': <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}\n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=89163)\u001b[0m 2019-07-31 17:00:09,867\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 2 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=89163)\u001b[0m 2019-07-31 17:00:10.000779: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=89157)\u001b[0m 2019-07-31 17:00:10,252\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 2 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=89157)\u001b[0m 2019-07-31 17:00:10.283685: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=89163)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=89163)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=89163)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=89163)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=89157)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=89157)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=89157)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=89157)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m 2019-07-31 17:00:14,023\tINFO rollout_worker.py:428 -- Generating sample batch of size 200\n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m 2019-07-31 17:00:14,096\tINFO sampler.py:308 -- Raw obs from env: { 0: { 'agent0': np.ndarray((5,), dtype=float64, min=-1.0, max=3.002, mean=0.685)}}\n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m 2019-07-31 17:00:14,096\tINFO sampler.py:309 -- Info return from env: {0: {'agent0': None}}\n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m 2019-07-31 17:00:14,097\tINFO sampler.py:407 -- Preprocessed obs: np.ndarray((5,), dtype=float64, min=-1.0, max=3.002, mean=0.685)\n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m 2019-07-31 17:00:14,103\tINFO sampler.py:411 -- Filtered obs: np.ndarray((5,), dtype=float64, min=-1.0, max=3.002, mean=0.685)\n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m 2019-07-31 17:00:14,105\tINFO sampler.py:525 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m                                   'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m                                   'obs': np.ndarray((5,), dtype=float64, min=-1.0, max=3.002, mean=0.685),\n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m 2019-07-31 17:00:14,105\tINFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m 2019-07-31 17:00:14,263\tINFO sampler.py:552 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m { 'default_policy': ( np.ndarray((1, 2), dtype=float32, min=-1.373, max=-0.048, mean=-0.711),\n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m                       [],\n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m                       { 'action_prob': np.ndarray((1,), dtype=float32, min=0.062, max=0.062, mean=0.062),\n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m                         'behaviour_logits': np.ndarray((1, 4), dtype=float32, min=-0.003, max=0.006, mean=0.003),\n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=0.0, max=0.0, mean=0.0)})}\n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m 2019-07-31 17:00:14,458\tINFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m { 'agent0': { 'data': { 'action_prob': np.ndarray((41,), dtype=float32, min=0.001, max=0.158, mean=0.069),\n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m                         'actions': np.ndarray((41, 2), dtype=float32, min=-2.883, max=2.703, mean=-0.033),\n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m                         'advantages': np.ndarray((41,), dtype=float32, min=-0.004, max=0.006, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m                         'agent_index': np.ndarray((41,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m                         'behaviour_logits': np.ndarray((41, 4), dtype=float32, min=-0.005, max=0.009, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m                         'dones': np.ndarray((41,), dtype=bool, min=0.0, max=1.0, mean=0.024),\n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m                         'eps_id': np.ndarray((41,), dtype=int64, min=49174152.0, max=49174152.0, mean=49174152.0),\n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m                         'infos': np.ndarray((41,), dtype=object, head={'time': Timestamp('2005-02-14 00:00:00'), 'nlv': 100.0, 'nr_contracts': np.ndarray((3,), dtype=float64, min=0.0, max=100.0, mean=33.333), 'bid_prices': np.ndarray((3,), dtype=float64, min=1.0, max=2790.12, mean=1041.89), 'ask_prices': np.ndarray((3,), dtype=float64, min=1.0, max=2790.4, mean=1041.995), 'cost_of_spread': np.ndarray((3,), dtype=float64, min=0.0, max=0.0, mean=0.0), 'cost_of_commissions': np.ndarray((3,), dtype=float64, min=0.0, max=0.0, mean=0.0), 'profit_on_idle_cash': 0.0, 'margin': 0, 'weights_target': np.ndarray((3,), dtype=float64, min=0.0, max=1.0, mean=0.333), 'weights_drift': np.ndarray((3,), dtype=float64, min=0.0, max=1.0, mean=0.333), 'weights_diff': np.ndarray((3,), dtype=float64, min=0.0, max=0.0, mean=0.0), 'positions_value_diff': np.ndarray((3,), dtype=float64, min=0.0, max=0.0, mean=0.0), 'acq_prices': np.ndarray((3,), dtype=float64, min=1.0, max=2790.26, mean=1041.943), 'nr_contracts_diff': np.ndarray((3,), dtype=float64, min=0.0, max=0.0, mean=0.0), 'nlv_pre': 100.0}),\n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m                         'new_obs': np.ndarray((41, 5), dtype=float32, min=-1.235, max=3.313, mean=0.121),\n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m                         'obs': np.ndarray((41, 5), dtype=float32, min=-1.235, max=3.313, mean=0.112),\n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m                         'prev_actions': np.ndarray((41, 2), dtype=float32, min=-2.883, max=2.703, mean=-0.048),\n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m                         'prev_rewards': np.ndarray((41,), dtype=float32, min=-0.006, max=0.003, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m                         'rewards': np.ndarray((41,), dtype=float32, min=-0.006, max=0.004, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m                         't': np.ndarray((41,), dtype=int64, min=0.0, max=40.0, mean=20.0),\n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m                         'unroll_id': np.ndarray((41,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m                         'value_targets': np.ndarray((41,), dtype=float64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m                         'vf_preds': np.ndarray((41,), dtype=float32, min=0.0, max=0.0, mean=0.0)},\n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m               'type': 'SampleBatch'}}\n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m 2019-07-31 17:00:14,582\tINFO rollout_worker.py:428 -- Generating sample batch of size 200\n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m 2019-07-31 17:00:14,659\tINFO sampler.py:308 -- Raw obs from env: { 0: { 'agent0': np.ndarray((5,), dtype=float64, min=-1.0, max=3.778, mean=0.914)}}\n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m 2019-07-31 17:00:14,659\tINFO sampler.py:309 -- Info return from env: {0: {'agent0': None}}\n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m 2019-07-31 17:00:14,660\tINFO sampler.py:407 -- Preprocessed obs: np.ndarray((5,), dtype=float64, min=-1.0, max=3.778, mean=0.914)\n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m 2019-07-31 17:00:14,660\tINFO sampler.py:411 -- Filtered obs: np.ndarray((5,), dtype=float64, min=-1.0, max=3.778, mean=0.914)\n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m 2019-07-31 17:00:14,662\tINFO sampler.py:525 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m                                   'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m                                   'obs': np.ndarray((5,), dtype=float64, min=-1.0, max=3.778, mean=0.914),\n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m 2019-07-31 17:00:14,662\tINFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m 2019-07-31 17:00:14,933\tINFO sampler.py:552 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m { 'default_policy': ( np.ndarray((1, 2), dtype=float32, min=-0.198, max=1.361, mean=0.582),\n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m                       [],\n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m                       { 'action_prob': np.ndarray((1,), dtype=float32, min=0.062, max=0.062, mean=0.062),\n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m                         'behaviour_logits': np.ndarray((1, 4), dtype=float32, min=-0.012, max=0.001, mean=-0.003),\n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=0.0, max=0.0, mean=0.0)})}\n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m 2019-07-31 17:00:15,457\tINFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m { 'agent0': { 'data': { 'action_prob': np.ndarray((41,), dtype=float32, min=0.002, max=0.153, mean=0.082),\n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m                         'actions': np.ndarray((41, 2), dtype=float32, min=-2.077, max=2.786, mean=0.152),\n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m                         'advantages': np.ndarray((41,), dtype=float32, min=-0.003, max=0.001, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m                         'agent_index': np.ndarray((41,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m                         'behaviour_logits': np.ndarray((41, 4), dtype=float32, min=-0.012, max=0.01, mean=-0.001),\n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m                         'dones': np.ndarray((41,), dtype=bool, min=0.0, max=1.0, mean=0.024),\n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m                         'eps_id': np.ndarray((41,), dtype=int64, min=702207021.0, max=702207021.0, mean=702207021.0),\n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m                         'infos': np.ndarray((41,), dtype=object, head={'time': Timestamp('2018-07-03 00:00:00'), 'nlv': 99.99900004545226, 'nr_contracts': np.ndarray((3,), dtype=float64, min=0.0, max=90.909, mean=30.303), 'bid_prices': np.ndarray((3,), dtype=float64, min=1.0, max=8501.364, mean=3022.488), 'ask_prices': np.ndarray((3,), dtype=float64, min=1.0, max=8502.214, mean=3022.79), 'cost_of_spread': np.ndarray((3,), dtype=float64, min=0.0, max=0.0, mean=0.0), 'cost_of_commissions': np.ndarray((3,), dtype=float64, min=0.0, max=0.0, mean=0.0), 'profit_on_idle_cash': 0.0, 'margin': 0, 'weights_target': np.ndarray((3,), dtype=float64, min=0.0, max=0.909, mean=0.333), 'weights_drift': np.ndarray((3,), dtype=float64, min=0.0, max=1.0, mean=0.333), 'weights_diff': np.ndarray((3,), dtype=float64, min=-0.091, max=0.091, mean=-0.0), 'positions_value_diff': np.ndarray((3,), dtype=float64, min=-9.091, max=9.091, mean=-0.0), 'acq_prices': np.ndarray((3,), dtype=float64, min=1.0, max=8502.214, mean=3022.781), 'nr_contracts_diff': np.ndarray((3,), dtype=float64, min=-9.091, max=0.001, mean=-3.03), 'nlv_pre': 100.0}),\n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m                         'new_obs': np.ndarray((41, 5), dtype=float32, min=-3.887, max=1.843, mean=0.255),\n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m                         'obs': np.ndarray((41, 5), dtype=float32, min=-3.887, max=3.778, mean=0.268),\n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m                         'prev_actions': np.ndarray((41, 2), dtype=float32, min=-2.077, max=2.786, mean=0.177),\n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m                         'prev_rewards': np.ndarray((41,), dtype=float32, min=-0.002, max=0.001, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m                         'rewards': np.ndarray((41,), dtype=float32, min=-0.002, max=0.001, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m                         't': np.ndarray((41,), dtype=int64, min=0.0, max=40.0, mean=20.0),\n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m                         'unroll_id': np.ndarray((41,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m                         'value_targets': np.ndarray((41,), dtype=float64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m                         'vf_preds': np.ndarray((41,), dtype=float32, min=0.0, max=0.0, mean=0.0)},\n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m               'type': 'SampleBatch'}}\n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m 2019-07-31 17:00:17,256\tINFO rollout_worker.py:462 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m { 'data': { 'action_prob': np.ndarray((205,), dtype=float32, min=0.001, max=0.158, mean=0.086),\n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m             'actions': np.ndarray((205, 2), dtype=float32, min=-2.883, max=2.77, mean=0.027),\n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m             'advantages': np.ndarray((205,), dtype=float32, min=-0.01, max=0.006, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m             'agent_index': np.ndarray((205,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m             'behaviour_logits': np.ndarray((205, 4), dtype=float32, min=-0.01, max=0.009, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m             'dones': np.ndarray((205,), dtype=bool, min=0.0, max=1.0, mean=0.024),\n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m             'eps_id': np.ndarray((205,), dtype=int64, min=49174152.0, max=1478943400.0, mean=692353354.6),\n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m             'infos': np.ndarray((205,), dtype=object, head={'time': Timestamp('2005-02-14 00:00:00'), 'nlv': 100.0, 'nr_contracts': np.ndarray((3,), dtype=float64, min=0.0, max=100.0, mean=33.333), 'bid_prices': np.ndarray((3,), dtype=float64, min=1.0, max=2790.12, mean=1041.89), 'ask_prices': np.ndarray((3,), dtype=float64, min=1.0, max=2790.4, mean=1041.995), 'cost_of_spread': np.ndarray((3,), dtype=float64, min=0.0, max=0.0, mean=0.0), 'cost_of_commissions': np.ndarray((3,), dtype=float64, min=0.0, max=0.0, mean=0.0), 'profit_on_idle_cash': 0.0, 'margin': 0, 'weights_target': np.ndarray((3,), dtype=float64, min=0.0, max=1.0, mean=0.333), 'weights_drift': np.ndarray((3,), dtype=float64, min=0.0, max=1.0, mean=0.333), 'weights_diff': np.ndarray((3,), dtype=float64, min=0.0, max=0.0, mean=0.0), 'positions_value_diff': np.ndarray((3,), dtype=float64, min=0.0, max=0.0, mean=0.0), 'acq_prices': np.ndarray((3,), dtype=float64, min=1.0, max=2790.26, mean=1041.943), 'nr_contracts_diff': np.ndarray((3,), dtype=float64, min=0.0, max=0.0, mean=0.0), 'nlv_pre': 100.0}),\n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m             'new_obs': np.ndarray((205, 5), dtype=float32, min=-6.577, max=4.461, mean=0.192),\n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m             'obs': np.ndarray((205, 5), dtype=float32, min=-6.907, max=4.461, mean=0.184),\n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m             'prev_actions': np.ndarray((205, 2), dtype=float32, min=-2.883, max=2.77, mean=0.027),\n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m             'prev_rewards': np.ndarray((205,), dtype=float32, min=-0.012, max=0.006, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m             'rewards': np.ndarray((205,), dtype=float32, min=-0.012, max=0.006, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m             't': np.ndarray((205,), dtype=int64, min=0.0, max=40.0, mean=20.0),\n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m             'unroll_id': np.ndarray((205,), dtype=int64, min=0.0, max=4.0, mean=2.0),\n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m             'value_targets': np.ndarray((205,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m             'vf_preds': np.ndarray((205,), dtype=float32, min=0.0, max=0.0, mean=0.0)},\n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m   'type': 'SampleBatch'}\n",
      "\u001b[2m\u001b[36m(pid=89161)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m 2019-07-31 17:00:17,773\tINFO rollout_worker.py:462 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m { 'data': { 'action_prob': np.ndarray((205,), dtype=float32, min=0.001, max=0.16, mean=0.083),\n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m             'actions': np.ndarray((205, 2), dtype=float32, min=-2.216, max=2.801, mean=0.075),\n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m             'advantages': np.ndarray((205,), dtype=float32, min=-0.007, max=0.011, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m             'agent_index': np.ndarray((205,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m             'behaviour_logits': np.ndarray((205, 4), dtype=float32, min=-0.015, max=0.012, mean=-0.001),\n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m             'dones': np.ndarray((205,), dtype=bool, min=0.0, max=1.0, mean=0.024),\n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m             'eps_id': np.ndarray((205,), dtype=int64, min=496548347.0, max=1749530910.0, mean=1095179745.2),\n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m             'infos': np.ndarray((205,), dtype=object, head={'time': Timestamp('2018-07-03 00:00:00'), 'nlv': 99.99900004545226, 'nr_contracts': np.ndarray((3,), dtype=float64, min=0.0, max=90.909, mean=30.303), 'bid_prices': np.ndarray((3,), dtype=float64, min=1.0, max=8501.364, mean=3022.488), 'ask_prices': np.ndarray((3,), dtype=float64, min=1.0, max=8502.214, mean=3022.79), 'cost_of_spread': np.ndarray((3,), dtype=float64, min=0.0, max=0.0, mean=0.0), 'cost_of_commissions': np.ndarray((3,), dtype=float64, min=0.0, max=0.0, mean=0.0), 'profit_on_idle_cash': 0.0, 'margin': 0, 'weights_target': np.ndarray((3,), dtype=float64, min=0.0, max=0.909, mean=0.333), 'weights_drift': np.ndarray((3,), dtype=float64, min=0.0, max=1.0, mean=0.333), 'weights_diff': np.ndarray((3,), dtype=float64, min=-0.091, max=0.091, mean=-0.0), 'positions_value_diff': np.ndarray((3,), dtype=float64, min=-9.091, max=9.091, mean=-0.0), 'acq_prices': np.ndarray((3,), dtype=float64, min=1.0, max=8502.214, mean=3022.781), 'nr_contracts_diff': np.ndarray((3,), dtype=float64, min=-9.091, max=0.001, mean=-3.03), 'nlv_pre': 100.0}),\n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m             'new_obs': np.ndarray((205, 5), dtype=float32, min=-14.063, max=8.218, mean=0.229),\n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m             'obs': np.ndarray((205, 5), dtype=float32, min=-14.063, max=8.218, mean=0.236),\n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m             'prev_actions': np.ndarray((205, 2), dtype=float32, min=-2.216, max=2.801, mean=0.076),\n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m             'prev_rewards': np.ndarray((205,), dtype=float32, min=-0.006, max=0.01, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m             'rewards': np.ndarray((205,), dtype=float32, min=-0.006, max=0.01, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m             't': np.ndarray((205,), dtype=int64, min=0.0, max=40.0, mean=20.0),\n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m             'unroll_id': np.ndarray((205,), dtype=int64, min=0.0, max=4.0, mean=2.0),\n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m             'value_targets': np.ndarray((205,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m             'vf_preds': np.ndarray((205,), dtype=float32, min=0.0, max=0.0, mean=0.0)},\n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m   'type': 'SampleBatch'}\n",
      "\u001b[2m\u001b[36m(pid=89158)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=89159)\u001b[0m 2019-07-31 17:00:51,302\tINFO multi_gpu_impl.py:146 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(pid=89159)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=89159)\u001b[0m { 'inputs': [ np.ndarray((4100, 2), dtype=float32, min=-3.849, max=4.059, mean=0.009),\n",
      "\u001b[2m\u001b[36m(pid=89159)\u001b[0m               np.ndarray((4100,), dtype=float32, min=-0.015, max=0.014, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=89159)\u001b[0m               np.ndarray((4100, 5), dtype=float32, min=-14.063, max=15.356, mean=0.135),\n",
      "\u001b[2m\u001b[36m(pid=89159)\u001b[0m               np.ndarray((4100, 2), dtype=float32, min=-3.849, max=4.059, mean=0.008),\n",
      "\u001b[2m\u001b[36m(pid=89159)\u001b[0m               np.ndarray((4100,), dtype=float32, min=-6.699, max=6.128, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=89159)\u001b[0m               np.ndarray((4100, 4), dtype=float32, min=-0.01, max=0.013, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=89159)\u001b[0m               np.ndarray((4100,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=89159)\u001b[0m               np.ndarray((4100,), dtype=float32, min=0.0, max=0.0, mean=0.0)],\n",
      "\u001b[2m\u001b[36m(pid=89159)\u001b[0m   'placeholders': [ <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89159)\u001b[0m                     <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89159)\u001b[0m                     <tf.Tensor 'default_policy/observation:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89159)\u001b[0m                     <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89159)\u001b[0m                     <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89159)\u001b[0m                     <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 4) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89159)\u001b[0m                     <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89159)\u001b[0m                     <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>],\n",
      "\u001b[2m\u001b[36m(pid=89159)\u001b[0m   'state_inputs': []}\n",
      "\u001b[2m\u001b[36m(pid=89159)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=89159)\u001b[0m 2019-07-31 17:00:51,303\tINFO multi_gpu_impl.py:191 -- Divided 4100 rollout sequences, each of length 1, among 1 devices.\n",
      "\u001b[2m\u001b[36m(pid=89162)\u001b[0m 2019-07-31 17:00:55,507\tINFO multi_gpu_impl.py:146 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(pid=89162)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=89162)\u001b[0m { 'inputs': [ np.ndarray((4100, 2), dtype=float32, min=-3.488, max=3.374, mean=-0.005),\n",
      "\u001b[2m\u001b[36m(pid=89162)\u001b[0m               np.ndarray((4100,), dtype=float32, min=-0.018, max=0.013, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=89162)\u001b[0m               np.ndarray((4100, 5), dtype=float32, min=-14.063, max=15.356, mean=0.143),\n",
      "\u001b[2m\u001b[36m(pid=89162)\u001b[0m               np.ndarray((4100, 2), dtype=float32, min=-3.488, max=3.374, mean=-0.005),\n",
      "\u001b[2m\u001b[36m(pid=89162)\u001b[0m               np.ndarray((4100,), dtype=float32, min=-9.104, max=7.436, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=89162)\u001b[0m               np.ndarray((4100, 4), dtype=float32, min=-0.015, max=0.013, mean=-0.001),\n",
      "\u001b[2m\u001b[36m(pid=89162)\u001b[0m               np.ndarray((4100,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=89162)\u001b[0m               np.ndarray((4100,), dtype=float32, min=0.0, max=0.0, mean=0.0)],\n",
      "\u001b[2m\u001b[36m(pid=89162)\u001b[0m   'placeholders': [ <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89162)\u001b[0m                     <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89162)\u001b[0m                     <tf.Tensor 'default_policy/observation:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89162)\u001b[0m                     <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89162)\u001b[0m                     <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89162)\u001b[0m                     <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 4) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89162)\u001b[0m                     <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89162)\u001b[0m                     <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>],\n",
      "\u001b[2m\u001b[36m(pid=89162)\u001b[0m   'state_inputs': []}\n",
      "\u001b[2m\u001b[36m(pid=89162)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=89162)\u001b[0m 2019-07-31 17:00:55,507\tINFO multi_gpu_impl.py:191 -- Divided 4100 rollout sequences, each of length 1, among 1 devices.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-31 17:00:56,058\tERROR trial_runner.py:487 -- Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/Nicholas/.venv/lib/python3.6/site-packages/ray/tune/trial_runner.py\", line 468, in _process_trial\n",
      "    result, terminate=(decision == TrialScheduler.STOP))\n",
      "  File \"/home/Nicholas/.venv/lib/python3.6/site-packages/ray/tune/trial.py\", line 533, in update_last_result\n",
      "    self.result_logger.on_result(self.last_result)\n",
      "  File \"/home/Nicholas/.venv/lib/python3.6/site-packages/ray/tune/logger.py\", line 239, in on_result\n",
      "    _logger.on_result(result)\n",
      "  File \"/home/Nicholas/.venv/lib/python3.6/site-packages/trading_gym/ray/logger.py\", line 69, in on_result\n",
      "    for fold in result['trading-gym']:\n",
      "KeyError: 'trading-gym'\n",
      "2019-07-31 17:00:56,081\tINFO ray_trial_executor.py:187 -- Destroying actor for trial PPO_GAIAPredictorsContinuousV9_1_clip_param=0.2,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2019-07-31 17:00:56,157\tWARNING signature.py:108 -- The function with_updates has a **kwargs argument, which is currently not supported.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=89159)\u001b[0m 2019-07-31 17:00:56,049\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 39.8/67.5 GB\n",
      "Result logdir: logs/clip_exps_aric/clip_exps\n",
      "Number of trials: 11 ({'RUNNING': 1, 'ERROR': 1, 'PENDING': 9})\n",
      "ERROR trials:\n",
      " - PPO_GAIAPredictorsContinuousV9_1_clip_param=0.2,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tERROR, 1 failures: logs/clip_exps_aric/clip_exps/PPO_GAIAPredictorsContinuousV9_1_clip_param=0.2,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05_2019-07-31_16-59-259aw8hwth/error_2019-07-31_17-00-56.txt, [3 CPUs, 0 GPUs], [pid=89159], 49 s, 1 iter, 4100 ts, 0.00465 rew\n",
      "PENDING trials:\n",
      " - PPO_GAIAPredictorsContinuousV9_2_clip_param=0.3,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tPENDING\n",
      " - PPO_GAIAPredictorsContinuousV9_3_clip_param=0.4,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tPENDING\n",
      " - PPO_GAIAPredictorsContinuousV9_4_clip_param=0.5,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tPENDING\n",
      " - PPO_GAIAPredictorsContinuousV9_5_clip_param=0.6,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tPENDING\n",
      " - PPO_GAIAPredictorsContinuousV9_6_clip_param=0.7,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tPENDING\n",
      " - PPO_GAIAPredictorsContinuousV9_7_clip_param=0.8,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tPENDING\n",
      " - PPO_GAIAPredictorsContinuousV9_8_clip_param=0.9,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tPENDING\n",
      " - PPO_GAIAPredictorsContinuousV9_9_clip_param=1.0,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tPENDING\n",
      " - PPO_GAIAPredictorsContinuousV9_10_clip_param=1.1,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tPENDING\n",
      "RUNNING trials:\n",
      " - PPO_GAIAPredictorsContinuousV9_0_clip_param=0.1,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tRUNNING\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-31 17:00:56,320\tWARNING util.py:64 -- The `start_trial` operation took 0.1772935390472412 seconds to complete, which may be a performance bottleneck.\n",
      "2019-07-31 17:00:56,469\tWARNING util.py:64 -- The `experiment_checkpoint` operation took 0.13764548301696777 seconds to complete, which may be a performance bottleneck.\n",
      "2019-07-31 17:00:59,796\tERROR trial_runner.py:487 -- Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/Nicholas/.venv/lib/python3.6/site-packages/ray/tune/trial_runner.py\", line 468, in _process_trial\n",
      "    result, terminate=(decision == TrialScheduler.STOP))\n",
      "  File \"/home/Nicholas/.venv/lib/python3.6/site-packages/ray/tune/trial.py\", line 533, in update_last_result\n",
      "    self.result_logger.on_result(self.last_result)\n",
      "  File \"/home/Nicholas/.venv/lib/python3.6/site-packages/ray/tune/logger.py\", line 239, in on_result\n",
      "    _logger.on_result(result)\n",
      "  File \"/home/Nicholas/.venv/lib/python3.6/site-packages/trading_gym/ray/logger.py\", line 69, in on_result\n",
      "    for fold in result['trading-gym']:\n",
      "KeyError: 'trading-gym'\n",
      "2019-07-31 17:00:59,804\tINFO ray_trial_executor.py:187 -- Destroying actor for trial PPO_GAIAPredictorsContinuousV9_0_clip_param=0.1,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2019-07-31 17:00:59,876\tWARNING signature.py:108 -- The function with_updates has a **kwargs argument, which is currently not supported.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=89162)\u001b[0m 2019-07-31 17:00:59,790\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-31 17:01:00,034\tWARNING util.py:64 -- The `start_trial` operation took 0.1658031940460205 seconds to complete, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=89156)\u001b[0m 2019-07-31 17:01:00,862\tWARNING ppo.py:151 -- FYI: By default, the value function will not share layers with the policy model ('vf_share_layers': False).\n",
      "\u001b[2m\u001b[36m(pid=89160)\u001b[0m 2019-07-31 17:01:05,449\tWARNING ppo.py:151 -- FYI: By default, the value function will not share layers with the policy model ('vf_share_layers': False).\n",
      "\u001b[2m\u001b[36m(pid=89156)\u001b[0m 2019-07-31 17:01:06,374\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 0 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=89156)\u001b[0m 2019-07-31 17:01:06.389900: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=89156)\u001b[0m 2019-07-31 17:01:06,963\tINFO dynamic_tf_policy.py:313 -- Initializing loss function with dummy input:\n",
      "\u001b[2m\u001b[36m(pid=89156)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=89156)\u001b[0m { 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89156)\u001b[0m   'actions': <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89156)\u001b[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89156)\u001b[0m   'behaviour_logits': <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 4) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89156)\u001b[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,\n",
      "\u001b[2m\u001b[36m(pid=89156)\u001b[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89156)\u001b[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89156)\u001b[0m   'prev_actions': <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89156)\u001b[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89156)\u001b[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89156)\u001b[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89156)\u001b[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}\n",
      "\u001b[2m\u001b[36m(pid=89156)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=89156)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=89156)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=89156)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=89156)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=89156)\u001b[0m 2019-07-31 17:01:09,931\tINFO rollout_worker.py:719 -- Built policy map: {'default_policy': <ray.rllib.policy.tf_policy_template.PPOTFPolicy object at 0x7eff68ea25f8>}\n",
      "\u001b[2m\u001b[36m(pid=89156)\u001b[0m 2019-07-31 17:01:09,931\tINFO rollout_worker.py:720 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7eff68ea22b0>}\n",
      "\u001b[2m\u001b[36m(pid=89156)\u001b[0m 2019-07-31 17:01:09,931\tINFO rollout_worker.py:333 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7eff68ea2160>}\n",
      "\u001b[2m\u001b[36m(pid=89156)\u001b[0m 2019-07-31 17:01:10,143\tINFO multi_gpu_optimizer.py:79 -- LocalMultiGPUOptimizer devices ['/cpu:0']\n",
      "\u001b[2m\u001b[36m(pid=89160)\u001b[0m 2019-07-31 17:01:10,583\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 0 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=89160)\u001b[0m 2019-07-31 17:01:10.584813: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=89160)\u001b[0m 2019-07-31 17:01:11,651\tINFO dynamic_tf_policy.py:313 -- Initializing loss function with dummy input:\n",
      "\u001b[2m\u001b[36m(pid=89160)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=89160)\u001b[0m { 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89160)\u001b[0m   'actions': <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89160)\u001b[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89160)\u001b[0m   'behaviour_logits': <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 4) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89160)\u001b[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,\n",
      "\u001b[2m\u001b[36m(pid=89160)\u001b[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89160)\u001b[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89160)\u001b[0m   'prev_actions': <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89160)\u001b[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89160)\u001b[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89160)\u001b[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89160)\u001b[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}\n",
      "\u001b[2m\u001b[36m(pid=89160)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=89160)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=89160)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=89160)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=89160)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=89160)\u001b[0m 2019-07-31 17:01:17,039\tINFO rollout_worker.py:719 -- Built policy map: {'default_policy': <ray.rllib.policy.tf_policy_template.PPOTFPolicy object at 0x7fa8109145f8>}\n",
      "\u001b[2m\u001b[36m(pid=89160)\u001b[0m 2019-07-31 17:01:17,039\tINFO rollout_worker.py:720 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7fa8109142b0>}\n",
      "\u001b[2m\u001b[36m(pid=89160)\u001b[0m 2019-07-31 17:01:17,040\tINFO rollout_worker.py:333 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7fa810914160>}\n",
      "\u001b[2m\u001b[36m(pid=89160)\u001b[0m 2019-07-31 17:01:17,370\tINFO multi_gpu_optimizer.py:79 -- LocalMultiGPUOptimizer devices ['/cpu:0']\n",
      "\u001b[2m\u001b[36m(pid=98090)\u001b[0m 2019-07-31 17:01:31,417\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 2 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=98090)\u001b[0m 2019-07-31 17:01:31.501154: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=98090)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=98090)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=98090)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=98090)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m 2019-07-31 17:01:36,085\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 1 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m 2019-07-31 17:01:36.144874: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m 2019-07-31 17:01:36,261\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 1 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m 2019-07-31 17:01:36.327529: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m 2019-07-31 17:01:37,115\tINFO dynamic_tf_policy.py:313 -- Initializing loss function with dummy input:\n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m { 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m   'actions': <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m   'behaviour_logits': <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 4) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,\n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m   'prev_actions': <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}\n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m 2019-07-31 17:01:37,168\tINFO dynamic_tf_policy.py:313 -- Initializing loss function with dummy input:\n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m { 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m   'actions': <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m   'behaviour_logits': <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 4) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,\n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m   'prev_actions': <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}\n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=98156)\u001b[0m 2019-07-31 17:01:37,908\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 2 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=98156)\u001b[0m 2019-07-31 17:01:37.999700: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=98156)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=98156)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=98156)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=98156)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m 2019-07-31 17:01:40,512\tINFO rollout_worker.py:428 -- Generating sample batch of size 200\n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m 2019-07-31 17:01:40,544\tINFO sampler.py:308 -- Raw obs from env: { 0: { 'agent0': np.ndarray((5,), dtype=float64, min=-1.0, max=2.035, mean=0.4)}}\n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m 2019-07-31 17:01:40,545\tINFO sampler.py:309 -- Info return from env: {0: {'agent0': None}}\n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m 2019-07-31 17:01:40,545\tINFO sampler.py:407 -- Preprocessed obs: np.ndarray((5,), dtype=float64, min=-1.0, max=2.035, mean=0.4)\n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m 2019-07-31 17:01:40,545\tINFO sampler.py:411 -- Filtered obs: np.ndarray((5,), dtype=float64, min=-1.0, max=2.035, mean=0.4)\n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m 2019-07-31 17:01:40,547\tINFO sampler.py:525 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m                                   'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m                                   'obs': np.ndarray((5,), dtype=float64, min=-1.0, max=2.035, mean=0.4),\n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m 2019-07-31 17:01:40,547\tINFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m 2019-07-31 17:01:40,630\tINFO sampler.py:552 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m { 'default_policy': ( np.ndarray((1, 2), dtype=float32, min=0.251, max=0.615, mean=0.433),\n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m                       [],\n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m                       { 'action_prob': np.ndarray((1,), dtype=float32, min=0.126, max=0.126, mean=0.126),\n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m                         'behaviour_logits': np.ndarray((1, 4), dtype=float32, min=-0.002, max=0.008, mean=0.005),\n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=0.0, max=0.0, mean=0.0)})}\n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m 2019-07-31 17:01:40,746\tINFO rollout_worker.py:428 -- Generating sample batch of size 200\n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m 2019-07-31 17:01:40,774\tINFO sampler.py:308 -- Raw obs from env: { 0: { 'agent0': np.ndarray((5,), dtype=float64, min=-1.0, max=0.54, mean=-0.041)}}\n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m 2019-07-31 17:01:40,774\tINFO sampler.py:309 -- Info return from env: {0: {'agent0': None}}\n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m 2019-07-31 17:01:40,775\tINFO sampler.py:407 -- Preprocessed obs: np.ndarray((5,), dtype=float64, min=-1.0, max=0.54, mean=-0.041)\n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m 2019-07-31 17:01:40,775\tINFO sampler.py:411 -- Filtered obs: np.ndarray((5,), dtype=float64, min=-1.0, max=0.54, mean=-0.041)\n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m 2019-07-31 17:01:40,776\tINFO sampler.py:525 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m                                   'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m                                   'obs': np.ndarray((5,), dtype=float64, min=-1.0, max=0.54, mean=-0.041),\n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m 2019-07-31 17:01:40,777\tINFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m 2019-07-31 17:01:40,901\tINFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m { 'agent0': { 'data': { 'action_prob': np.ndarray((41,), dtype=float32, min=0.003, max=0.155, mean=0.073),\n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m                         'actions': np.ndarray((41, 2), dtype=float32, min=-2.06, max=2.696, mean=-0.06),\n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m                         'advantages': np.ndarray((41,), dtype=float32, min=-0.005, max=0.002, mean=-0.001),\n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m                         'agent_index': np.ndarray((41,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m                         'behaviour_logits': np.ndarray((41, 4), dtype=float32, min=-0.005, max=0.008, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m                         'dones': np.ndarray((41,), dtype=bool, min=0.0, max=1.0, mean=0.024),\n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m                         'eps_id': np.ndarray((41,), dtype=int64, min=728301217.0, max=728301217.0, mean=728301217.0),\n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m                         'infos': np.ndarray((41,), dtype=object, head={'time': Timestamp('2014-03-14 00:00:00'), 'nlv': 99.99816674999585, 'nr_contracts': np.ndarray((3,), dtype=float64, min=0.002, max=83.333, mean=27.784), 'bid_prices': np.ndarray((3,), dtype=float64, min=1.0, max=5337.432, mean=1954.049), 'ask_prices': np.ndarray((3,), dtype=float64, min=1.0, max=5337.966, mean=1954.245), 'cost_of_spread': np.ndarray((3,), dtype=float64, min=0.0, max=0.0, mean=0.0), 'cost_of_commissions': np.ndarray((3,), dtype=float64, min=0.0, max=0.0, mean=0.0), 'profit_on_idle_cash': 0.0, 'margin': 0, 'weights_target': np.ndarray((3,), dtype=float64, min=0.083, max=0.833, mean=0.333), 'weights_drift': np.ndarray((3,), dtype=float64, min=0.0, max=1.0, mean=0.333), 'weights_diff': np.ndarray((3,), dtype=float64, min=-0.167, max=0.083, mean=-0.0), 'positions_value_diff': np.ndarray((3,), dtype=float64, min=-16.667, max=8.333, mean=-0.0), 'acq_prices': np.ndarray((3,), dtype=float64, min=1.0, max=5337.966, mean=1954.245), 'nr_contracts_diff': np.ndarray((3,), dtype=float64, min=-16.667, max=0.016, mean=-5.55), 'nlv_pre': 100.0}),\n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m                         'new_obs': np.ndarray((41, 5), dtype=float32, min=-1.407, max=1.649, mean=0.037),\n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m                         'obs': np.ndarray((41, 5), dtype=float32, min=-1.407, max=2.035, mean=0.044),\n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m                         'prev_actions': np.ndarray((41, 2), dtype=float32, min=-2.06, max=2.696, mean=-0.053),\n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m                         'prev_rewards': np.ndarray((41,), dtype=float32, min=-0.004, max=0.002, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m                         'rewards': np.ndarray((41,), dtype=float32, min=-0.004, max=0.002, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m                         't': np.ndarray((41,), dtype=int64, min=0.0, max=40.0, mean=20.0),\n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m                         'unroll_id': np.ndarray((41,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m                         'value_targets': np.ndarray((41,), dtype=float64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m                         'vf_preds': np.ndarray((41,), dtype=float32, min=0.0, max=0.0, mean=0.0)},\n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m               'type': 'SampleBatch'}}\n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m 2019-07-31 17:01:40,957\tINFO sampler.py:552 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m { 'default_policy': ( np.ndarray((1, 2), dtype=float32, min=-1.939, max=-0.2, mean=-1.069),\n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m                       [],\n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m                       { 'action_prob': np.ndarray((1,), dtype=float32, min=0.024, max=0.024, mean=0.024),\n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m                         'behaviour_logits': np.ndarray((1, 4), dtype=float32, min=-0.0, max=0.004, mean=0.002),\n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=0.0, max=0.0, mean=0.0)})}\n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m 2019-07-31 17:01:41,368\tINFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m { 'agent0': { 'data': { 'action_prob': np.ndarray((41,), dtype=float32, min=0.004, max=0.157, mean=0.074),\n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m                         'actions': np.ndarray((41, 2), dtype=float32, min=-2.118, max=2.585, mean=0.096),\n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m                         'advantages': np.ndarray((41,), dtype=float32, min=-0.008, max=0.015, mean=0.002),\n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m                         'agent_index': np.ndarray((41,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m                         'behaviour_logits': np.ndarray((41, 4), dtype=float32, min=-0.002, max=0.008, mean=0.003),\n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m                         'dones': np.ndarray((41,), dtype=bool, min=0.0, max=1.0, mean=0.024),\n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m                         'eps_id': np.ndarray((41,), dtype=int64, min=964500389.0, max=964500389.0, mean=964500389.0),\n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m                         'infos': np.ndarray((41,), dtype=object, head={'time': Timestamp('2009-06-19 00:00:00'), 'nlv': 100.0, 'nr_contracts': np.ndarray((3,), dtype=float64, min=0.0, max=100.0, mean=33.333), 'bid_prices': np.ndarray((3,), dtype=float64, min=1.0, max=2363.505, mean=923.391), 'ask_prices': np.ndarray((3,), dtype=float64, min=1.0, max=2363.741, mean=923.484), 'cost_of_spread': np.ndarray((3,), dtype=float64, min=0.0, max=0.0, mean=0.0), 'cost_of_commissions': np.ndarray((3,), dtype=float64, min=0.0, max=0.0, mean=0.0), 'profit_on_idle_cash': 0.0, 'margin': 0, 'weights_target': np.ndarray((3,), dtype=float64, min=0.0, max=1.0, mean=0.333), 'weights_drift': np.ndarray((3,), dtype=float64, min=0.0, max=1.0, mean=0.333), 'weights_diff': np.ndarray((3,), dtype=float64, min=0.0, max=0.0, mean=0.0), 'positions_value_diff': np.ndarray((3,), dtype=float64, min=0.0, max=0.0, mean=0.0), 'acq_prices': np.ndarray((3,), dtype=float64, min=1.0, max=2363.623, mean=923.438), 'nr_contracts_diff': np.ndarray((3,), dtype=float64, min=0.0, max=0.0, mean=0.0), 'nlv_pre': 100.0}),\n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m                         'new_obs': np.ndarray((41, 5), dtype=float32, min=-2.297, max=1.217, mean=-0.022),\n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m                         'obs': np.ndarray((41, 5), dtype=float32, min=-2.297, max=1.217, mean=-0.014),\n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m                         'prev_actions': np.ndarray((41, 2), dtype=float32, min=-2.118, max=2.585, mean=0.086),\n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m                         'prev_rewards': np.ndarray((41,), dtype=float32, min=-0.008, max=0.009, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m                         'rewards': np.ndarray((41,), dtype=float32, min=-0.008, max=0.009, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m                         't': np.ndarray((41,), dtype=int64, min=0.0, max=40.0, mean=20.0),\n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m                         'unroll_id': np.ndarray((41,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m                         'value_targets': np.ndarray((41,), dtype=float64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m                         'vf_preds': np.ndarray((41,), dtype=float32, min=0.0, max=0.0, mean=0.0)},\n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m               'type': 'SampleBatch'}}\n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m 2019-07-31 17:01:42,722\tINFO rollout_worker.py:462 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m { 'data': { 'action_prob': np.ndarray((205,), dtype=float32, min=0.001, max=0.158, mean=0.085),\n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m             'actions': np.ndarray((205, 2), dtype=float32, min=-3.161, max=2.696, mean=-0.072),\n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m             'advantages': np.ndarray((205,), dtype=float32, min=-0.006, max=0.006, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m             'agent_index': np.ndarray((205,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m             'behaviour_logits': np.ndarray((205, 4), dtype=float32, min=-0.007, max=0.008, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m             'dones': np.ndarray((205,), dtype=bool, min=0.0, max=1.0, mean=0.024),\n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m             'eps_id': np.ndarray((205,), dtype=int64, min=156945015.0, max=1592815187.0, mean=941256865.2),\n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m             'infos': np.ndarray((205,), dtype=object, head={'time': Timestamp('2014-03-14 00:00:00'), 'nlv': 99.99816674999585, 'nr_contracts': np.ndarray((3,), dtype=float64, min=0.002, max=83.333, mean=27.784), 'bid_prices': np.ndarray((3,), dtype=float64, min=1.0, max=5337.432, mean=1954.049), 'ask_prices': np.ndarray((3,), dtype=float64, min=1.0, max=5337.966, mean=1954.245), 'cost_of_spread': np.ndarray((3,), dtype=float64, min=0.0, max=0.0, mean=0.0), 'cost_of_commissions': np.ndarray((3,), dtype=float64, min=0.0, max=0.0, mean=0.0), 'profit_on_idle_cash': 0.0, 'margin': 0, 'weights_target': np.ndarray((3,), dtype=float64, min=0.083, max=0.833, mean=0.333), 'weights_drift': np.ndarray((3,), dtype=float64, min=0.0, max=1.0, mean=0.333), 'weights_diff': np.ndarray((3,), dtype=float64, min=-0.167, max=0.083, mean=-0.0), 'positions_value_diff': np.ndarray((3,), dtype=float64, min=-16.667, max=8.333, mean=-0.0), 'acq_prices': np.ndarray((3,), dtype=float64, min=1.0, max=5337.966, mean=1954.245), 'nr_contracts_diff': np.ndarray((3,), dtype=float64, min=-16.667, max=0.016, mean=-5.55), 'nlv_pre': 100.0}),\n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m             'new_obs': np.ndarray((205, 5), dtype=float32, min=-1.407, max=1.649, mean=0.053),\n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m             'obs': np.ndarray((205, 5), dtype=float32, min=-3.051, max=2.035, mean=0.047),\n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m             'prev_actions': np.ndarray((205, 2), dtype=float32, min=-3.161, max=2.696, mean=-0.058),\n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m             'prev_rewards': np.ndarray((205,), dtype=float32, min=-0.005, max=0.005, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m             'rewards': np.ndarray((205,), dtype=float32, min=-0.005, max=0.005, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m             't': np.ndarray((205,), dtype=int64, min=0.0, max=40.0, mean=20.0),\n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m             'unroll_id': np.ndarray((205,), dtype=int64, min=0.0, max=4.0, mean=2.0),\n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m             'value_targets': np.ndarray((205,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m             'vf_preds': np.ndarray((205,), dtype=float32, min=0.0, max=0.0, mean=0.0)},\n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m   'type': 'SampleBatch'}\n",
      "\u001b[2m\u001b[36m(pid=98089)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m 2019-07-31 17:01:43,360\tINFO rollout_worker.py:462 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m { 'data': { 'action_prob': np.ndarray((205,), dtype=float32, min=0.001, max=0.157, mean=0.075),\n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m             'actions': np.ndarray((205, 2), dtype=float32, min=-3.126, max=2.664, mean=-0.048),\n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m             'advantages': np.ndarray((205,), dtype=float32, min=-0.008, max=0.015, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m             'agent_index': np.ndarray((205,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m             'behaviour_logits': np.ndarray((205, 4), dtype=float32, min=-0.007, max=0.011, mean=0.002),\n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m             'dones': np.ndarray((205,), dtype=bool, min=0.0, max=1.0, mean=0.024),\n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m             'eps_id': np.ndarray((205,), dtype=int64, min=227830524.0, max=1575382277.0, mean=874994986.4),\n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m             'infos': np.ndarray((205,), dtype=object, head={'time': Timestamp('2009-06-19 00:00:00'), 'nlv': 100.0, 'nr_contracts': np.ndarray((3,), dtype=float64, min=0.0, max=100.0, mean=33.333), 'bid_prices': np.ndarray((3,), dtype=float64, min=1.0, max=2363.505, mean=923.391), 'ask_prices': np.ndarray((3,), dtype=float64, min=1.0, max=2363.741, mean=923.484), 'cost_of_spread': np.ndarray((3,), dtype=float64, min=0.0, max=0.0, mean=0.0), 'cost_of_commissions': np.ndarray((3,), dtype=float64, min=0.0, max=0.0, mean=0.0), 'profit_on_idle_cash': 0.0, 'margin': 0, 'weights_target': np.ndarray((3,), dtype=float64, min=0.0, max=1.0, mean=0.333), 'weights_drift': np.ndarray((3,), dtype=float64, min=0.0, max=1.0, mean=0.333), 'weights_diff': np.ndarray((3,), dtype=float64, min=0.0, max=0.0, mean=0.0), 'positions_value_diff': np.ndarray((3,), dtype=float64, min=0.0, max=0.0, mean=0.0), 'acq_prices': np.ndarray((3,), dtype=float64, min=1.0, max=2363.623, mean=923.438), 'nr_contracts_diff': np.ndarray((3,), dtype=float64, min=0.0, max=0.0, mean=0.0), 'nlv_pre': 100.0}),\n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m             'new_obs': np.ndarray((205, 5), dtype=float32, min=-14.063, max=6.289, mean=0.146),\n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m             'obs': np.ndarray((205, 5), dtype=float32, min=-14.063, max=9.592, mean=0.163),\n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m             'prev_actions': np.ndarray((205, 2), dtype=float32, min=-3.126, max=2.664, mean=-0.042),\n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m             'prev_rewards': np.ndarray((205,), dtype=float32, min=-0.008, max=0.009, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m             'rewards': np.ndarray((205,), dtype=float32, min=-0.008, max=0.009, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m             't': np.ndarray((205,), dtype=int64, min=0.0, max=40.0, mean=20.0),\n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m             'unroll_id': np.ndarray((205,), dtype=int64, min=0.0, max=4.0, mean=2.0),\n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m             'value_targets': np.ndarray((205,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m             'vf_preds': np.ndarray((205,), dtype=float32, min=0.0, max=0.0, mean=0.0)},\n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m   'type': 'SampleBatch'}\n",
      "\u001b[2m\u001b[36m(pid=98188)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=89156)\u001b[0m 2019-07-31 17:02:07,908\tINFO multi_gpu_impl.py:146 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(pid=89156)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=89156)\u001b[0m { 'inputs': [ np.ndarray((4100, 2), dtype=float32, min=-4.536, max=3.786, mean=0.006),\n",
      "\u001b[2m\u001b[36m(pid=89156)\u001b[0m               np.ndarray((4100,), dtype=float32, min=-0.018, max=0.013, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=89156)\u001b[0m               np.ndarray((4100, 5), dtype=float32, min=-14.063, max=15.356, mean=0.127),\n",
      "\u001b[2m\u001b[36m(pid=89156)\u001b[0m               np.ndarray((4100, 2), dtype=float32, min=-4.536, max=3.786, mean=0.006),\n",
      "\u001b[2m\u001b[36m(pid=89156)\u001b[0m               np.ndarray((4100,), dtype=float32, min=-7.063, max=7.699, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=89156)\u001b[0m               np.ndarray((4100, 4), dtype=float32, min=-0.011, max=0.011, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=89156)\u001b[0m               np.ndarray((4100,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=89156)\u001b[0m               np.ndarray((4100,), dtype=float32, min=0.0, max=0.0, mean=0.0)],\n",
      "\u001b[2m\u001b[36m(pid=89156)\u001b[0m   'placeholders': [ <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89156)\u001b[0m                     <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89156)\u001b[0m                     <tf.Tensor 'default_policy/observation:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89156)\u001b[0m                     <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89156)\u001b[0m                     <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89156)\u001b[0m                     <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 4) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89156)\u001b[0m                     <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89156)\u001b[0m                     <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>],\n",
      "\u001b[2m\u001b[36m(pid=89156)\u001b[0m   'state_inputs': []}\n",
      "\u001b[2m\u001b[36m(pid=89156)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=89156)\u001b[0m 2019-07-31 17:02:07,908\tINFO multi_gpu_impl.py:191 -- Divided 4100 rollout sequences, each of length 1, among 1 devices.\n",
      "\u001b[2m\u001b[36m(pid=89160)\u001b[0m 2019-07-31 17:02:10,921\tINFO multi_gpu_impl.py:146 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(pid=89160)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=89160)\u001b[0m { 'inputs': [ np.ndarray((4100, 2), dtype=float32, min=-3.508, max=3.642, mean=0.006),\n",
      "\u001b[2m\u001b[36m(pid=89160)\u001b[0m               np.ndarray((4100,), dtype=float32, min=-0.019, max=0.014, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=89160)\u001b[0m               np.ndarray((4100, 5), dtype=float32, min=-14.063, max=10.831, mean=0.152),\n",
      "\u001b[2m\u001b[36m(pid=89160)\u001b[0m               np.ndarray((4100, 2), dtype=float32, min=-3.508, max=3.642, mean=0.007),\n",
      "\u001b[2m\u001b[36m(pid=89160)\u001b[0m               np.ndarray((4100,), dtype=float32, min=-7.499, max=7.084, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=89160)\u001b[0m               np.ndarray((4100, 4), dtype=float32, min=-0.007, max=0.012, mean=0.002),\n",
      "\u001b[2m\u001b[36m(pid=89160)\u001b[0m               np.ndarray((4100,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=89160)\u001b[0m               np.ndarray((4100,), dtype=float32, min=0.0, max=0.0, mean=0.0)],\n",
      "\u001b[2m\u001b[36m(pid=89160)\u001b[0m   'placeholders': [ <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89160)\u001b[0m                     <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89160)\u001b[0m                     <tf.Tensor 'default_policy/observation:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89160)\u001b[0m                     <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89160)\u001b[0m                     <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89160)\u001b[0m                     <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 4) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89160)\u001b[0m                     <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=89160)\u001b[0m                     <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>],\n",
      "\u001b[2m\u001b[36m(pid=89160)\u001b[0m   'state_inputs': []}\n",
      "\u001b[2m\u001b[36m(pid=89160)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=89160)\u001b[0m 2019-07-31 17:02:10,921\tINFO multi_gpu_impl.py:191 -- Divided 4100 rollout sequences, each of length 1, among 1 devices.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-31 17:02:11,538\tERROR trial_runner.py:487 -- Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/Nicholas/.venv/lib/python3.6/site-packages/ray/tune/trial_runner.py\", line 468, in _process_trial\n",
      "    result, terminate=(decision == TrialScheduler.STOP))\n",
      "  File \"/home/Nicholas/.venv/lib/python3.6/site-packages/ray/tune/trial.py\", line 533, in update_last_result\n",
      "    self.result_logger.on_result(self.last_result)\n",
      "  File \"/home/Nicholas/.venv/lib/python3.6/site-packages/ray/tune/logger.py\", line 239, in on_result\n",
      "    _logger.on_result(result)\n",
      "  File \"/home/Nicholas/.venv/lib/python3.6/site-packages/trading_gym/ray/logger.py\", line 69, in on_result\n",
      "    for fold in result['trading-gym']:\n",
      "KeyError: 'trading-gym'\n",
      "2019-07-31 17:02:11,551\tINFO ray_trial_executor.py:187 -- Destroying actor for trial PPO_GAIAPredictorsContinuousV9_2_clip_param=0.3,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2019-07-31 17:02:11,658\tWARNING signature.py:108 -- The function with_updates has a **kwargs argument, which is currently not supported.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=89156)\u001b[0m 2019-07-31 17:02:11,534\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 39.5/67.5 GB\n",
      "Result logdir: logs/clip_exps_aric/clip_exps\n",
      "Number of trials: 11 ({'ERROR': 3, 'RUNNING': 1, 'PENDING': 7})\n",
      "ERROR trials:\n",
      " - PPO_GAIAPredictorsContinuousV9_0_clip_param=0.1,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tERROR, 1 failures: logs/clip_exps_aric/clip_exps/PPO_GAIAPredictorsContinuousV9_0_clip_param=0.1,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05_2019-07-31_16-59-24r06fnmw4/error_2019-07-31_17-00-59.txt, [3 CPUs, 0 GPUs], [pid=89162], 55 s, 1 iter, 4100 ts, 0.00508 rew\n",
      " - PPO_GAIAPredictorsContinuousV9_1_clip_param=0.2,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tERROR, 1 failures: logs/clip_exps_aric/clip_exps/PPO_GAIAPredictorsContinuousV9_1_clip_param=0.2,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05_2019-07-31_16-59-259aw8hwth/error_2019-07-31_17-00-56.txt, [3 CPUs, 0 GPUs], [pid=89159], 49 s, 1 iter, 4100 ts, 0.00465 rew\n",
      " - PPO_GAIAPredictorsContinuousV9_2_clip_param=0.3,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tERROR, 1 failures: logs/clip_exps_aric/clip_exps/PPO_GAIAPredictorsContinuousV9_2_clip_param=0.3,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05_2019-07-31_17-00-56qkigs8s9/error_2019-07-31_17-02-11.txt, [3 CPUs, 0 GPUs], [pid=89156], 45 s, 1 iter, 4100 ts, 0.00346 rew\n",
      "PENDING trials:\n",
      " - PPO_GAIAPredictorsContinuousV9_4_clip_param=0.5,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tPENDING\n",
      " - PPO_GAIAPredictorsContinuousV9_5_clip_param=0.6,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tPENDING\n",
      " - PPO_GAIAPredictorsContinuousV9_6_clip_param=0.7,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tPENDING\n",
      " - PPO_GAIAPredictorsContinuousV9_7_clip_param=0.8,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tPENDING\n",
      " - PPO_GAIAPredictorsContinuousV9_8_clip_param=0.9,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tPENDING\n",
      " - PPO_GAIAPredictorsContinuousV9_9_clip_param=1.0,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tPENDING\n",
      " - PPO_GAIAPredictorsContinuousV9_10_clip_param=1.1,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tPENDING\n",
      "RUNNING trials:\n",
      " - PPO_GAIAPredictorsContinuousV9_3_clip_param=0.4,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tRUNNING\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-31 17:02:11,870\tWARNING util.py:64 -- The `start_trial` operation took 0.23322725296020508 seconds to complete, which may be a performance bottleneck.\n",
      "2019-07-31 17:02:15,399\tERROR trial_runner.py:487 -- Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/Nicholas/.venv/lib/python3.6/site-packages/ray/tune/trial_runner.py\", line 468, in _process_trial\n",
      "    result, terminate=(decision == TrialScheduler.STOP))\n",
      "  File \"/home/Nicholas/.venv/lib/python3.6/site-packages/ray/tune/trial.py\", line 533, in update_last_result\n",
      "    self.result_logger.on_result(self.last_result)\n",
      "  File \"/home/Nicholas/.venv/lib/python3.6/site-packages/ray/tune/logger.py\", line 239, in on_result\n",
      "    _logger.on_result(result)\n",
      "  File \"/home/Nicholas/.venv/lib/python3.6/site-packages/trading_gym/ray/logger.py\", line 69, in on_result\n",
      "    for fold in result['trading-gym']:\n",
      "KeyError: 'trading-gym'\n",
      "2019-07-31 17:02:15,403\tINFO ray_trial_executor.py:187 -- Destroying actor for trial PPO_GAIAPredictorsContinuousV9_3_clip_param=0.4,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2019-07-31 17:02:15,508\tWARNING signature.py:108 -- The function with_updates has a **kwargs argument, which is currently not supported.\n",
      "2019-07-31 17:02:15,606\tWARNING util.py:64 -- The `start_trial` operation took 0.11599564552307129 seconds to complete, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=89160)\u001b[0m 2019-07-31 17:02:15,393\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=98159)\u001b[0m 2019-07-31 17:02:16,591\tWARNING ppo.py:151 -- FYI: By default, the value function will not share layers with the policy model ('vf_share_layers': False).\n",
      "\u001b[2m\u001b[36m(pid=98157)\u001b[0m 2019-07-31 17:02:19,762\tWARNING ppo.py:151 -- FYI: By default, the value function will not share layers with the policy model ('vf_share_layers': False).\n",
      "\u001b[2m\u001b[36m(pid=98159)\u001b[0m 2019-07-31 17:02:21,114\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 0 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=98159)\u001b[0m 2019-07-31 17:02:21.120407: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=98159)\u001b[0m 2019-07-31 17:02:21,637\tINFO dynamic_tf_policy.py:313 -- Initializing loss function with dummy input:\n",
      "\u001b[2m\u001b[36m(pid=98159)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=98159)\u001b[0m { 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=98159)\u001b[0m   'actions': <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=98159)\u001b[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=98159)\u001b[0m   'behaviour_logits': <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 4) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=98159)\u001b[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,\n",
      "\u001b[2m\u001b[36m(pid=98159)\u001b[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=98159)\u001b[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=98159)\u001b[0m   'prev_actions': <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=98159)\u001b[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=98159)\u001b[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=98159)\u001b[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=98159)\u001b[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}\n",
      "\u001b[2m\u001b[36m(pid=98159)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=98159)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=98159)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=98159)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=98159)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=98157)\u001b[0m 2019-07-31 17:02:23,593\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 0 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=98157)\u001b[0m 2019-07-31 17:02:23.595265: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=98157)\u001b[0m 2019-07-31 17:02:24,090\tINFO dynamic_tf_policy.py:313 -- Initializing loss function with dummy input:\n",
      "\u001b[2m\u001b[36m(pid=98157)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=98157)\u001b[0m { 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=98157)\u001b[0m   'actions': <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=98157)\u001b[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=98157)\u001b[0m   'behaviour_logits': <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 4) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=98157)\u001b[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,\n",
      "\u001b[2m\u001b[36m(pid=98157)\u001b[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=98157)\u001b[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=98157)\u001b[0m   'prev_actions': <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=98157)\u001b[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=98157)\u001b[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=98157)\u001b[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=98157)\u001b[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}\n",
      "\u001b[2m\u001b[36m(pid=98157)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=98157)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=98157)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=98157)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=98157)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=98159)\u001b[0m 2019-07-31 17:02:25,557\tINFO rollout_worker.py:719 -- Built policy map: {'default_policy': <ray.rllib.policy.tf_policy_template.PPOTFPolicy object at 0x7fcda9bb15c0>}\n",
      "\u001b[2m\u001b[36m(pid=98159)\u001b[0m 2019-07-31 17:02:25,557\tINFO rollout_worker.py:720 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7fcda9bb1278>}\n",
      "\u001b[2m\u001b[36m(pid=98159)\u001b[0m 2019-07-31 17:02:25,557\tINFO rollout_worker.py:333 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7fcda9bb1128>}\n",
      "\u001b[2m\u001b[36m(pid=98159)\u001b[0m 2019-07-31 17:02:25,700\tINFO multi_gpu_optimizer.py:79 -- LocalMultiGPUOptimizer devices ['/cpu:0']\n",
      "\u001b[2m\u001b[36m(pid=98157)\u001b[0m 2019-07-31 17:02:28,203\tINFO rollout_worker.py:719 -- Built policy map: {'default_policy': <ray.rllib.policy.tf_policy_template.PPOTFPolicy object at 0x7fad4875e630>}\n",
      "\u001b[2m\u001b[36m(pid=98157)\u001b[0m 2019-07-31 17:02:28,204\tINFO rollout_worker.py:720 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7fad4875e2e8>}\n",
      "\u001b[2m\u001b[36m(pid=98157)\u001b[0m 2019-07-31 17:02:28,204\tINFO rollout_worker.py:333 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7fad4875e198>}\n",
      "\u001b[2m\u001b[36m(pid=98157)\u001b[0m 2019-07-31 17:02:28,382\tINFO multi_gpu_optimizer.py:79 -- LocalMultiGPUOptimizer devices ['/cpu:0']\n",
      "\u001b[2m\u001b[36m(pid=99181)\u001b[0m 2019-07-31 17:02:53,874\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 2 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=99181)\u001b[0m 2019-07-31 17:02:53.974217: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m 2019-07-31 17:02:54,157\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 1 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m 2019-07-31 17:02:54.204981: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=99235)\u001b[0m 2019-07-31 17:02:54,304\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 2 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=99235)\u001b[0m 2019-07-31 17:02:54.331913: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m 2019-07-31 17:02:54,402\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 1 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m 2019-07-31 17:02:54.436732: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m 2019-07-31 17:02:54,818\tINFO dynamic_tf_policy.py:313 -- Initializing loss function with dummy input:\n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m { 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m   'actions': <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m   'behaviour_logits': <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 4) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,\n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m   'prev_actions': <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}\n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m 2019-07-31 17:02:54,962\tINFO dynamic_tf_policy.py:313 -- Initializing loss function with dummy input:\n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m { 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m   'actions': <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m   'behaviour_logits': <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 4) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,\n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m   'prev_actions': <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}\n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=99181)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=99181)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99181)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=99181)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99235)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=99235)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99235)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=99235)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m 2019-07-31 17:02:58,123\tINFO rollout_worker.py:428 -- Generating sample batch of size 200\n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m 2019-07-31 17:02:58,224\tINFO sampler.py:308 -- Raw obs from env: { 0: { 'agent0': np.ndarray((5,), dtype=float64, min=-1.0, max=1.495, mean=0.241)}}\n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m 2019-07-31 17:02:58,225\tINFO sampler.py:309 -- Info return from env: {0: {'agent0': None}}\n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m 2019-07-31 17:02:58,225\tINFO sampler.py:407 -- Preprocessed obs: np.ndarray((5,), dtype=float64, min=-1.0, max=1.495, mean=0.241)\n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m 2019-07-31 17:02:58,226\tINFO sampler.py:411 -- Filtered obs: np.ndarray((5,), dtype=float64, min=-1.0, max=1.495, mean=0.241)\n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m 2019-07-31 17:02:58,227\tINFO sampler.py:525 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m                                   'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m                                   'obs': np.ndarray((5,), dtype=float64, min=-1.0, max=1.495, mean=0.241),\n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m 2019-07-31 17:02:58,227\tINFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m 2019-07-31 17:02:58,351\tINFO sampler.py:552 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m { 'default_policy': ( np.ndarray((1, 2), dtype=float32, min=-0.101, max=0.24, mean=0.069),\n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m                       [],\n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m                       { 'action_prob': np.ndarray((1,), dtype=float32, min=0.153, max=0.153, mean=0.153),\n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m                         'behaviour_logits': np.ndarray((1, 4), dtype=float32, min=-0.003, max=0.004, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=0.0, max=0.0, mean=0.0)})}\n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m 2019-07-31 17:02:58,533\tINFO rollout_worker.py:428 -- Generating sample batch of size 200\n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m 2019-07-31 17:02:58,567\tINFO sampler.py:308 -- Raw obs from env: { 0: { 'agent0': np.ndarray((5,), dtype=float64, min=-1.0, max=6.034, mean=1.579)}}\n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m 2019-07-31 17:02:58,578\tINFO sampler.py:309 -- Info return from env: {0: {'agent0': None}}\n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m 2019-07-31 17:02:58,584\tINFO sampler.py:407 -- Preprocessed obs: np.ndarray((5,), dtype=float64, min=-1.0, max=6.034, mean=1.579)\n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m 2019-07-31 17:02:58,584\tINFO sampler.py:411 -- Filtered obs: np.ndarray((5,), dtype=float64, min=-1.0, max=6.034, mean=1.579)\n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m 2019-07-31 17:02:58,586\tINFO sampler.py:525 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m                                   'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m                                   'obs': np.ndarray((5,), dtype=float64, min=-1.0, max=6.034, mean=1.579),\n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m 2019-07-31 17:02:58,587\tINFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m 2019-07-31 17:02:58,659\tINFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m { 'agent0': { 'data': { 'action_prob': np.ndarray((41,), dtype=float32, min=0.007, max=0.158, mean=0.094),\n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m                         'actions': np.ndarray((41, 2), dtype=float32, min=-2.371, max=2.285, mean=0.021),\n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m                         'advantages': np.ndarray((41,), dtype=float32, min=-0.003, max=0.004, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m                         'agent_index': np.ndarray((41,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m                         'behaviour_logits': np.ndarray((41, 4), dtype=float32, min=-0.004, max=0.004, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m                         'dones': np.ndarray((41,), dtype=bool, min=0.0, max=1.0, mean=0.024),\n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m                         'eps_id': np.ndarray((41,), dtype=int64, min=1647209613.0, max=1647209613.0, mean=1647209613.0),\n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m                         'infos': np.ndarray((41,), dtype=object, head={'time': Timestamp('2017-09-26 00:00:00'), 'nlv': 99.99900004545226, 'nr_contracts': np.ndarray((3,), dtype=float64, min=0.0, max=90.909, mean=30.308), 'bid_prices': np.ndarray((3,), dtype=float64, min=1.0, max=7694.502, mean=2760.908), 'ask_prices': np.ndarray((3,), dtype=float64, min=1.0, max=7695.272, mean=2761.184), 'cost_of_spread': np.ndarray((3,), dtype=float64, min=0.0, max=0.0, mean=0.0), 'cost_of_commissions': np.ndarray((3,), dtype=float64, min=0.0, max=0.0, mean=0.0), 'profit_on_idle_cash': 0.0, 'margin': 0, 'weights_target': np.ndarray((3,), dtype=float64, min=0.0, max=0.909, mean=0.333), 'weights_drift': np.ndarray((3,), dtype=float64, min=0.0, max=1.0, mean=0.333), 'weights_diff': np.ndarray((3,), dtype=float64, min=-0.091, max=0.091, mean=-0.0), 'positions_value_diff': np.ndarray((3,), dtype=float64, min=-9.091, max=9.091, mean=-0.0), 'acq_prices': np.ndarray((3,), dtype=float64, min=1.0, max=7694.887, mean=2761.056), 'nr_contracts_diff': np.ndarray((3,), dtype=float64, min=-9.091, max=0.015, mean=-3.025), 'nlv_pre': 100.0}),\n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m                         'new_obs': np.ndarray((41, 5), dtype=float32, min=-1.0, max=1.251, mean=0.028),\n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m                         'obs': np.ndarray((41, 5), dtype=float32, min=-1.0, max=1.495, mean=0.032),\n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m                         'prev_actions': np.ndarray((41, 2), dtype=float32, min=-2.371, max=2.285, mean=0.016),\n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m                         'prev_rewards': np.ndarray((41,), dtype=float32, min=-0.003, max=0.003, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m                         'rewards': np.ndarray((41,), dtype=float32, min=-0.003, max=0.003, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m                         't': np.ndarray((41,), dtype=int64, min=0.0, max=40.0, mean=20.0),\n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m                         'unroll_id': np.ndarray((41,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m                         'value_targets': np.ndarray((41,), dtype=float64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m                         'vf_preds': np.ndarray((41,), dtype=float32, min=0.0, max=0.0, mean=0.0)},\n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m               'type': 'SampleBatch'}}\n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m 2019-07-31 17:02:58,752\tINFO sampler.py:552 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m { 'default_policy': ( np.ndarray((1, 2), dtype=float32, min=-0.136, max=2.108, mean=0.986),\n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m                       [],\n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m                       { 'action_prob': np.ndarray((1,), dtype=float32, min=0.017, max=0.017, mean=0.017),\n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m                         'behaviour_logits': np.ndarray((1, 4), dtype=float32, min=-0.004, max=0.001, mean=-0.002),\n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=0.0, max=0.0, mean=0.0)})}\n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m 2019-07-31 17:02:59,263\tINFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m { 'agent0': { 'data': { 'action_prob': np.ndarray((41,), dtype=float32, min=0.003, max=0.16, mean=0.083),\n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m                         'actions': np.ndarray((41, 2), dtype=float32, min=-2.399, max=2.108, mean=-0.058),\n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m                         'advantages': np.ndarray((41,), dtype=float32, min=-0.001, max=0.007, mean=0.002),\n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m                         'agent_index': np.ndarray((41,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m                         'behaviour_logits': np.ndarray((41, 4), dtype=float32, min=-0.012, max=0.002, mean=-0.004),\n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m                         'dones': np.ndarray((41,), dtype=bool, min=0.0, max=1.0, mean=0.024),\n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m                         'eps_id': np.ndarray((41,), dtype=int64, min=467129559.0, max=467129559.0, mean=467129559.0),\n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m                         'infos': np.ndarray((41,), dtype=object, head={'time': Timestamp('2006-08-01 00:00:00'), 'nlv': 99.99900004545226, 'nr_contracts': np.ndarray((3,), dtype=float64, min=0.0, max=90.909, mean=30.312), 'bid_prices': np.ndarray((3,), dtype=float64, min=1.0, max=3045.902, mean=1125.263), 'ask_prices': np.ndarray((3,), dtype=float64, min=1.0, max=3046.206, mean=1125.375), 'cost_of_spread': np.ndarray((3,), dtype=float64, min=0.0, max=0.0, mean=0.0), 'cost_of_commissions': np.ndarray((3,), dtype=float64, min=0.0, max=0.0, mean=0.0), 'profit_on_idle_cash': 0.0, 'margin': 0, 'weights_target': np.ndarray((3,), dtype=float64, min=0.0, max=0.909, mean=0.333), 'weights_drift': np.ndarray((3,), dtype=float64, min=0.0, max=1.0, mean=0.333), 'weights_diff': np.ndarray((3,), dtype=float64, min=-0.091, max=0.091, mean=-0.0), 'positions_value_diff': np.ndarray((3,), dtype=float64, min=-9.091, max=9.091, mean=-0.0), 'acq_prices': np.ndarray((3,), dtype=float64, min=1.0, max=3046.054, mean=1125.325), 'nr_contracts_diff': np.ndarray((3,), dtype=float64, min=-9.091, max=0.028, mean=-3.021), 'nlv_pre': 100.0}),\n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m                         'new_obs': np.ndarray((41, 5), dtype=float32, min=-1.136, max=3.326, mean=0.162),\n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m                         'obs': np.ndarray((41, 5), dtype=float32, min=-1.136, max=6.034, mean=0.203),\n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m                         'prev_actions': np.ndarray((41, 2), dtype=float32, min=-2.399, max=2.108, mean=-0.042),\n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m                         'prev_rewards': np.ndarray((41,), dtype=float32, min=-0.001, max=0.003, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m                         'rewards': np.ndarray((41,), dtype=float32, min=-0.001, max=0.003, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m                         't': np.ndarray((41,), dtype=int64, min=0.0, max=40.0, mean=20.0),\n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m                         'unroll_id': np.ndarray((41,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m                         'value_targets': np.ndarray((41,), dtype=float64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m                         'vf_preds': np.ndarray((41,), dtype=float32, min=0.0, max=0.0, mean=0.0)},\n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m               'type': 'SampleBatch'}}\n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m 2019-07-31 17:03:00,802\tINFO rollout_worker.py:462 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m { 'data': { 'action_prob': np.ndarray((205,), dtype=float32, min=0.0, max=0.158, mean=0.082),\n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m             'actions': np.ndarray((205, 2), dtype=float32, min=-2.701, max=3.511, mean=0.046),\n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m             'advantages': np.ndarray((205,), dtype=float32, min=-0.01, max=0.008, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m             'agent_index': np.ndarray((205,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m             'behaviour_logits': np.ndarray((205, 4), dtype=float32, min=-0.01, max=0.009, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m             'dones': np.ndarray((205,), dtype=bool, min=0.0, max=1.0, mean=0.024),\n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m             'eps_id': np.ndarray((205,), dtype=int64, min=295417013.0, max=1647209613.0, mean=1053923775.0),\n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m             'infos': np.ndarray((205,), dtype=object, head={'time': Timestamp('2017-09-26 00:00:00'), 'nlv': 99.99900004545226, 'nr_contracts': np.ndarray((3,), dtype=float64, min=0.0, max=90.909, mean=30.308), 'bid_prices': np.ndarray((3,), dtype=float64, min=1.0, max=7694.502, mean=2760.908), 'ask_prices': np.ndarray((3,), dtype=float64, min=1.0, max=7695.272, mean=2761.184), 'cost_of_spread': np.ndarray((3,), dtype=float64, min=0.0, max=0.0, mean=0.0), 'cost_of_commissions': np.ndarray((3,), dtype=float64, min=0.0, max=0.0, mean=0.0), 'profit_on_idle_cash': 0.0, 'margin': 0, 'weights_target': np.ndarray((3,), dtype=float64, min=0.0, max=0.909, mean=0.333), 'weights_drift': np.ndarray((3,), dtype=float64, min=0.0, max=1.0, mean=0.333), 'weights_diff': np.ndarray((3,), dtype=float64, min=-0.091, max=0.091, mean=-0.0), 'positions_value_diff': np.ndarray((3,), dtype=float64, min=-9.091, max=9.091, mean=-0.0), 'acq_prices': np.ndarray((3,), dtype=float64, min=1.0, max=7694.887, mean=2761.056), 'nr_contracts_diff': np.ndarray((3,), dtype=float64, min=-9.091, max=0.015, mean=-3.025), 'nlv_pre': 100.0}),\n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m             'new_obs': np.ndarray((205, 5), dtype=float32, min=-7.803, max=6.289, mean=0.155),\n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m             'obs': np.ndarray((205, 5), dtype=float32, min=-9.077, max=6.289, mean=0.128),\n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m             'prev_actions': np.ndarray((205, 2), dtype=float32, min=-2.701, max=3.511, mean=0.035),\n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m             'prev_rewards': np.ndarray((205,), dtype=float32, min=-0.008, max=0.011, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m             'rewards': np.ndarray((205,), dtype=float32, min=-0.008, max=0.011, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m             't': np.ndarray((205,), dtype=int64, min=0.0, max=40.0, mean=20.0),\n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m             'unroll_id': np.ndarray((205,), dtype=int64, min=0.0, max=4.0, mean=2.0),\n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m             'value_targets': np.ndarray((205,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m             'vf_preds': np.ndarray((205,), dtype=float32, min=0.0, max=0.0, mean=0.0)},\n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m   'type': 'SampleBatch'}\n",
      "\u001b[2m\u001b[36m(pid=99237)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m 2019-07-31 17:03:01,583\tINFO rollout_worker.py:462 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m { 'data': { 'action_prob': np.ndarray((205,), dtype=float32, min=0.0, max=0.16, mean=0.083),\n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m             'actions': np.ndarray((205, 2), dtype=float32, min=-3.827, max=2.651, mean=-0.109),\n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m             'advantages': np.ndarray((205,), dtype=float32, min=-0.008, max=0.007, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m             'agent_index': np.ndarray((205,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m             'behaviour_logits': np.ndarray((205, 4), dtype=float32, min=-0.012, max=0.004, mean=-0.004),\n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m             'dones': np.ndarray((205,), dtype=bool, min=0.0, max=1.0, mean=0.024),\n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m             'eps_id': np.ndarray((205,), dtype=int64, min=70336009.0, max=1620749714.0, mean=756966969.6),\n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m             'infos': np.ndarray((205,), dtype=object, head={'time': Timestamp('2006-08-01 00:00:00'), 'nlv': 99.99900004545226, 'nr_contracts': np.ndarray((3,), dtype=float64, min=0.0, max=90.909, mean=30.312), 'bid_prices': np.ndarray((3,), dtype=float64, min=1.0, max=3045.902, mean=1125.263), 'ask_prices': np.ndarray((3,), dtype=float64, min=1.0, max=3046.206, mean=1125.375), 'cost_of_spread': np.ndarray((3,), dtype=float64, min=0.0, max=0.0, mean=0.0), 'cost_of_commissions': np.ndarray((3,), dtype=float64, min=0.0, max=0.0, mean=0.0), 'profit_on_idle_cash': 0.0, 'margin': 0, 'weights_target': np.ndarray((3,), dtype=float64, min=0.0, max=0.909, mean=0.333), 'weights_drift': np.ndarray((3,), dtype=float64, min=0.0, max=1.0, mean=0.333), 'weights_diff': np.ndarray((3,), dtype=float64, min=-0.091, max=0.091, mean=-0.0), 'positions_value_diff': np.ndarray((3,), dtype=float64, min=-9.091, max=9.091, mean=-0.0), 'acq_prices': np.ndarray((3,), dtype=float64, min=1.0, max=3046.054, mean=1125.325), 'nr_contracts_diff': np.ndarray((3,), dtype=float64, min=-9.091, max=0.028, mean=-3.021), 'nlv_pre': 100.0}),\n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m             'new_obs': np.ndarray((205, 5), dtype=float32, min=-2.4, max=4.962, mean=0.085),\n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m             'obs': np.ndarray((205, 5), dtype=float32, min=-2.4, max=6.034, mean=0.097),\n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m             'prev_actions': np.ndarray((205, 2), dtype=float32, min=-3.827, max=2.651, mean=-0.112),\n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m             'prev_rewards': np.ndarray((205,), dtype=float32, min=-0.006, max=0.003, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m             'rewards': np.ndarray((205,), dtype=float32, min=-0.006, max=0.003, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m             't': np.ndarray((205,), dtype=int64, min=0.0, max=40.0, mean=20.0),\n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m             'unroll_id': np.ndarray((205,), dtype=int64, min=0.0, max=4.0, mean=2.0),\n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m             'value_targets': np.ndarray((205,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m             'vf_preds': np.ndarray((205,), dtype=float32, min=0.0, max=0.0, mean=0.0)},\n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m   'type': 'SampleBatch'}\n",
      "\u001b[2m\u001b[36m(pid=99183)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=98157)\u001b[0m 2019-07-31 17:03:25,610\tINFO multi_gpu_impl.py:146 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(pid=98157)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=98157)\u001b[0m { 'inputs': [ np.ndarray((4100, 2), dtype=float32, min=-4.041, max=4.694, mean=-0.002),\n",
      "\u001b[2m\u001b[36m(pid=98157)\u001b[0m               np.ndarray((4100,), dtype=float32, min=-0.062, max=0.033, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=98157)\u001b[0m               np.ndarray((4100, 5), dtype=float32, min=-9.356, max=15.356, mean=0.138),\n",
      "\u001b[2m\u001b[36m(pid=98157)\u001b[0m               np.ndarray((4100, 2), dtype=float32, min=-4.041, max=4.694, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=98157)\u001b[0m               np.ndarray((4100,), dtype=float32, min=-9.86, max=9.175, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=98157)\u001b[0m               np.ndarray((4100, 4), dtype=float32, min=-0.01, max=0.01, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=98157)\u001b[0m               np.ndarray((4100,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=98157)\u001b[0m               np.ndarray((4100,), dtype=float32, min=0.0, max=0.0, mean=0.0)],\n",
      "\u001b[2m\u001b[36m(pid=98157)\u001b[0m   'placeholders': [ <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=98157)\u001b[0m                     <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=98157)\u001b[0m                     <tf.Tensor 'default_policy/observation:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=98157)\u001b[0m                     <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=98157)\u001b[0m                     <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=98157)\u001b[0m                     <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 4) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=98157)\u001b[0m                     <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=98157)\u001b[0m                     <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>],\n",
      "\u001b[2m\u001b[36m(pid=98157)\u001b[0m   'state_inputs': []}\n",
      "\u001b[2m\u001b[36m(pid=98157)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=98157)\u001b[0m 2019-07-31 17:03:25,611\tINFO multi_gpu_impl.py:191 -- Divided 4100 rollout sequences, each of length 1, among 1 devices.\n",
      "\u001b[2m\u001b[36m(pid=98159)\u001b[0m 2019-07-31 17:03:26,537\tINFO multi_gpu_impl.py:146 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(pid=98159)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=98159)\u001b[0m { 'inputs': [ np.ndarray((4100, 2), dtype=float32, min=-4.214, max=3.695, mean=-0.024),\n",
      "\u001b[2m\u001b[36m(pid=98159)\u001b[0m               np.ndarray((4100,), dtype=float32, min=-0.012, max=0.016, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=98159)\u001b[0m               np.ndarray((4100, 5), dtype=float32, min=-14.063, max=15.356, mean=0.128),\n",
      "\u001b[2m\u001b[36m(pid=98159)\u001b[0m               np.ndarray((4100, 2), dtype=float32, min=-4.214, max=3.695, mean=-0.023),\n",
      "\u001b[2m\u001b[36m(pid=98159)\u001b[0m               np.ndarray((4100,), dtype=float32, min=-5.401, max=8.951, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=98159)\u001b[0m               np.ndarray((4100, 4), dtype=float32, min=-0.012, max=0.01, mean=-0.003),\n",
      "\u001b[2m\u001b[36m(pid=98159)\u001b[0m               np.ndarray((4100,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=98159)\u001b[0m               np.ndarray((4100,), dtype=float32, min=0.0, max=0.0, mean=0.0)],\n",
      "\u001b[2m\u001b[36m(pid=98159)\u001b[0m   'placeholders': [ <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=98159)\u001b[0m                     <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=98159)\u001b[0m                     <tf.Tensor 'default_policy/observation:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=98159)\u001b[0m                     <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=98159)\u001b[0m                     <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=98159)\u001b[0m                     <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 4) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=98159)\u001b[0m                     <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=98159)\u001b[0m                     <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>],\n",
      "\u001b[2m\u001b[36m(pid=98159)\u001b[0m   'state_inputs': []}\n",
      "\u001b[2m\u001b[36m(pid=98159)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=98159)\u001b[0m 2019-07-31 17:03:26,537\tINFO multi_gpu_impl.py:191 -- Divided 4100 rollout sequences, each of length 1, among 1 devices.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-31 17:03:29,350\tERROR trial_runner.py:487 -- Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/Nicholas/.venv/lib/python3.6/site-packages/ray/tune/trial_runner.py\", line 468, in _process_trial\n",
      "    result, terminate=(decision == TrialScheduler.STOP))\n",
      "  File \"/home/Nicholas/.venv/lib/python3.6/site-packages/ray/tune/trial.py\", line 533, in update_last_result\n",
      "    self.result_logger.on_result(self.last_result)\n",
      "  File \"/home/Nicholas/.venv/lib/python3.6/site-packages/ray/tune/logger.py\", line 239, in on_result\n",
      "    _logger.on_result(result)\n",
      "  File \"/home/Nicholas/.venv/lib/python3.6/site-packages/trading_gym/ray/logger.py\", line 69, in on_result\n",
      "    for fold in result['trading-gym']:\n",
      "KeyError: 'trading-gym'\n",
      "2019-07-31 17:03:29,357\tINFO ray_trial_executor.py:187 -- Destroying actor for trial PPO_GAIAPredictorsContinuousV9_5_clip_param=0.6,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2019-07-31 17:03:29,488\tWARNING util.py:64 -- The `experiment_checkpoint` operation took 0.12128353118896484 seconds to complete, which may be a performance bottleneck.\n",
      "2019-07-31 17:03:29,529\tWARNING signature.py:108 -- The function with_updates has a **kwargs argument, which is currently not supported.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=98157)\u001b[0m 2019-07-31 17:03:29,343\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 40.5/67.5 GB\n",
      "Result logdir: logs/clip_exps_aric/clip_exps\n",
      "Number of trials: 11 ({'ERROR': 5, 'RUNNING': 1, 'PENDING': 5})\n",
      "ERROR trials:\n",
      " - PPO_GAIAPredictorsContinuousV9_0_clip_param=0.1,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tERROR, 1 failures: logs/clip_exps_aric/clip_exps/PPO_GAIAPredictorsContinuousV9_0_clip_param=0.1,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05_2019-07-31_16-59-24r06fnmw4/error_2019-07-31_17-00-59.txt, [3 CPUs, 0 GPUs], [pid=89162], 55 s, 1 iter, 4100 ts, 0.00508 rew\n",
      " - PPO_GAIAPredictorsContinuousV9_1_clip_param=0.2,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tERROR, 1 failures: logs/clip_exps_aric/clip_exps/PPO_GAIAPredictorsContinuousV9_1_clip_param=0.2,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05_2019-07-31_16-59-259aw8hwth/error_2019-07-31_17-00-56.txt, [3 CPUs, 0 GPUs], [pid=89159], 49 s, 1 iter, 4100 ts, 0.00465 rew\n",
      " - PPO_GAIAPredictorsContinuousV9_2_clip_param=0.3,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tERROR, 1 failures: logs/clip_exps_aric/clip_exps/PPO_GAIAPredictorsContinuousV9_2_clip_param=0.3,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05_2019-07-31_17-00-56qkigs8s9/error_2019-07-31_17-02-11.txt, [3 CPUs, 0 GPUs], [pid=89156], 45 s, 1 iter, 4100 ts, 0.00346 rew\n",
      " - PPO_GAIAPredictorsContinuousV9_3_clip_param=0.4,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tERROR, 1 failures: logs/clip_exps_aric/clip_exps/PPO_GAIAPredictorsContinuousV9_3_clip_param=0.4,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05_2019-07-31_17-00-59cwpbby9k/error_2019-07-31_17-02-15.txt, [3 CPUs, 0 GPUs], [pid=89160], 42 s, 1 iter, 4100 ts, 0.00395 rew\n",
      " - PPO_GAIAPredictorsContinuousV9_5_clip_param=0.6,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tERROR, 1 failures: logs/clip_exps_aric/clip_exps/PPO_GAIAPredictorsContinuousV9_5_clip_param=0.6,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05_2019-07-31_17-02-15dfj8taa2/error_2019-07-31_17-03-29.txt, [3 CPUs, 0 GPUs], [pid=98157], 37 s, 1 iter, 4100 ts, 0.0025 rew\n",
      "PENDING trials:\n",
      " - PPO_GAIAPredictorsContinuousV9_6_clip_param=0.7,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tPENDING\n",
      " - PPO_GAIAPredictorsContinuousV9_7_clip_param=0.8,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tPENDING\n",
      " - PPO_GAIAPredictorsContinuousV9_8_clip_param=0.9,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tPENDING\n",
      " - PPO_GAIAPredictorsContinuousV9_9_clip_param=1.0,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tPENDING\n",
      " - PPO_GAIAPredictorsContinuousV9_10_clip_param=1.1,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tPENDING\n",
      "RUNNING trials:\n",
      " - PPO_GAIAPredictorsContinuousV9_4_clip_param=0.5,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tRUNNING\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-31 17:03:29,666\tWARNING util.py:64 -- The `start_trial` operation took 0.15463829040527344 seconds to complete, which may be a performance bottleneck.\n",
      "2019-07-31 17:03:30,376\tERROR trial_runner.py:487 -- Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/Nicholas/.venv/lib/python3.6/site-packages/ray/tune/trial_runner.py\", line 468, in _process_trial\n",
      "    result, terminate=(decision == TrialScheduler.STOP))\n",
      "  File \"/home/Nicholas/.venv/lib/python3.6/site-packages/ray/tune/trial.py\", line 533, in update_last_result\n",
      "    self.result_logger.on_result(self.last_result)\n",
      "  File \"/home/Nicholas/.venv/lib/python3.6/site-packages/ray/tune/logger.py\", line 239, in on_result\n",
      "    _logger.on_result(result)\n",
      "  File \"/home/Nicholas/.venv/lib/python3.6/site-packages/trading_gym/ray/logger.py\", line 69, in on_result\n",
      "    for fold in result['trading-gym']:\n",
      "KeyError: 'trading-gym'\n",
      "2019-07-31 17:03:30,384\tINFO ray_trial_executor.py:187 -- Destroying actor for trial PPO_GAIAPredictorsContinuousV9_4_clip_param=0.5,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2019-07-31 17:03:30,530\tWARNING util.py:64 -- The `experiment_checkpoint` operation took 0.13212132453918457 seconds to complete, which may be a performance bottleneck.\n",
      "2019-07-31 17:03:30,546\tWARNING signature.py:108 -- The function with_updates has a **kwargs argument, which is currently not supported.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=98159)\u001b[0m 2019-07-31 17:03:30,370\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-31 17:03:30,756\tWARNING util.py:64 -- The `experiment_checkpoint` operation took 0.13387036323547363 seconds to complete, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=99342)\u001b[0m 2019-07-31 17:03:34,553\tWARNING ppo.py:151 -- FYI: By default, the value function will not share layers with the policy model ('vf_share_layers': False).\n",
      "\u001b[2m\u001b[36m(pid=99333)\u001b[0m 2019-07-31 17:03:35,411\tWARNING ppo.py:151 -- FYI: By default, the value function will not share layers with the policy model ('vf_share_layers': False).\n",
      "\u001b[2m\u001b[36m(pid=99342)\u001b[0m 2019-07-31 17:03:40,112\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 0 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=99342)\u001b[0m 2019-07-31 17:03:40.132938: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=99333)\u001b[0m 2019-07-31 17:03:40,165\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 0 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=99333)\u001b[0m 2019-07-31 17:03:40.166259: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=99342)\u001b[0m 2019-07-31 17:03:40,812\tINFO dynamic_tf_policy.py:313 -- Initializing loss function with dummy input:\n",
      "\u001b[2m\u001b[36m(pid=99342)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99342)\u001b[0m { 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99342)\u001b[0m   'actions': <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99342)\u001b[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99342)\u001b[0m   'behaviour_logits': <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 4) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99342)\u001b[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,\n",
      "\u001b[2m\u001b[36m(pid=99342)\u001b[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99342)\u001b[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99342)\u001b[0m   'prev_actions': <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99342)\u001b[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99342)\u001b[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99342)\u001b[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99342)\u001b[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}\n",
      "\u001b[2m\u001b[36m(pid=99342)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99333)\u001b[0m 2019-07-31 17:03:40,837\tINFO dynamic_tf_policy.py:313 -- Initializing loss function with dummy input:\n",
      "\u001b[2m\u001b[36m(pid=99333)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99333)\u001b[0m { 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99333)\u001b[0m   'actions': <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99333)\u001b[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99333)\u001b[0m   'behaviour_logits': <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 4) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99333)\u001b[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,\n",
      "\u001b[2m\u001b[36m(pid=99333)\u001b[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99333)\u001b[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99333)\u001b[0m   'prev_actions': <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99333)\u001b[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99333)\u001b[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99333)\u001b[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99333)\u001b[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}\n",
      "\u001b[2m\u001b[36m(pid=99333)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99342)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=99342)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99342)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=99342)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99333)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=99333)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99333)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=99333)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99342)\u001b[0m 2019-07-31 17:03:44,124\tINFO rollout_worker.py:719 -- Built policy map: {'default_policy': <ray.rllib.policy.tf_policy_template.PPOTFPolicy object at 0x7f93b1d3f588>}\n",
      "\u001b[2m\u001b[36m(pid=99342)\u001b[0m 2019-07-31 17:03:44,124\tINFO rollout_worker.py:720 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7f93b1d3f240>}\n",
      "\u001b[2m\u001b[36m(pid=99342)\u001b[0m 2019-07-31 17:03:44,124\tINFO rollout_worker.py:333 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7f93b1d3f0f0>}\n",
      "\u001b[2m\u001b[36m(pid=99333)\u001b[0m 2019-07-31 17:03:44,257\tINFO rollout_worker.py:719 -- Built policy map: {'default_policy': <ray.rllib.policy.tf_policy_template.PPOTFPolicy object at 0x7fe34cc275f8>}\n",
      "\u001b[2m\u001b[36m(pid=99333)\u001b[0m 2019-07-31 17:03:44,257\tINFO rollout_worker.py:720 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7fe34cc272b0>}\n",
      "\u001b[2m\u001b[36m(pid=99333)\u001b[0m 2019-07-31 17:03:44,257\tINFO rollout_worker.py:333 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7fe34cc27160>}\n",
      "\u001b[2m\u001b[36m(pid=99342)\u001b[0m 2019-07-31 17:03:44,211\tINFO multi_gpu_optimizer.py:79 -- LocalMultiGPUOptimizer devices ['/cpu:0']\n",
      "\u001b[2m\u001b[36m(pid=99333)\u001b[0m 2019-07-31 17:03:44,485\tINFO multi_gpu_optimizer.py:79 -- LocalMultiGPUOptimizer devices ['/cpu:0']\n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m 2019-07-31 17:03:52,764\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 1 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m 2019-07-31 17:03:52.876654: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=99297)\u001b[0m 2019-07-31 17:03:53,032\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 2 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=99297)\u001b[0m 2019-07-31 17:03:53.093700: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=99295)\u001b[0m 2019-07-31 17:03:53,369\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 2 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=99295)\u001b[0m 2019-07-31 17:03:53.429570: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m 2019-07-31 17:03:53,452\tINFO dynamic_tf_policy.py:313 -- Initializing loss function with dummy input:\n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m { 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m   'actions': <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m   'behaviour_logits': <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 4) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,\n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m   'prev_actions': <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}\n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m 2019-07-31 17:03:53,753\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 1 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m 2019-07-31 17:03:53.859547: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m 2019-07-31 17:03:54,481\tINFO dynamic_tf_policy.py:313 -- Initializing loss function with dummy input:\n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m { 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m   'actions': <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m   'behaviour_logits': <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 4) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,\n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m   'prev_actions': <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}\n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99295)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=99295)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99295)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=99295)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99297)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=99297)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99297)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=99297)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m 2019-07-31 17:03:58,417\tINFO rollout_worker.py:428 -- Generating sample batch of size 200\n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m 2019-07-31 17:03:58,453\tINFO sampler.py:308 -- Raw obs from env: { 0: { 'agent0': np.ndarray((5,), dtype=float64, min=0.0, max=1.0, mean=0.2)}}\n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m 2019-07-31 17:03:58,454\tINFO sampler.py:309 -- Info return from env: {0: {'agent0': None}}\n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m 2019-07-31 17:03:58,454\tINFO sampler.py:407 -- Preprocessed obs: np.ndarray((5,), dtype=float64, min=0.0, max=1.0, mean=0.2)\n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m 2019-07-31 17:03:58,454\tINFO sampler.py:411 -- Filtered obs: np.ndarray((5,), dtype=float64, min=0.0, max=1.0, mean=0.2)\n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m 2019-07-31 17:03:58,462\tINFO sampler.py:525 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m                                   'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m                                   'obs': np.ndarray((5,), dtype=float64, min=0.0, max=1.0, mean=0.2),\n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m 2019-07-31 17:03:58,462\tINFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m 2019-07-31 17:03:58,566\tINFO sampler.py:552 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m { 'default_policy': ( np.ndarray((1, 2), dtype=float32, min=-0.049, max=0.525, mean=0.238),\n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m                       [],\n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m                       { 'action_prob': np.ndarray((1,), dtype=float32, min=0.138, max=0.138, mean=0.138),\n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m                         'behaviour_logits': np.ndarray((1, 4), dtype=float32, min=-0.002, max=0.002, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=0.0, max=0.0, mean=0.0)})}\n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m 2019-07-31 17:03:58,923\tINFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m { 'agent0': { 'data': { 'action_prob': np.ndarray((41,), dtype=float32, min=0.001, max=0.153, mean=0.078),\n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m                         'actions': np.ndarray((41, 2), dtype=float32, min=-2.035, max=2.791, mean=0.085),\n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m                         'advantages': np.ndarray((41,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m                         'agent_index': np.ndarray((41,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m                         'behaviour_logits': np.ndarray((41, 4), dtype=float32, min=-0.002, max=0.004, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m                         'dones': np.ndarray((41,), dtype=bool, min=0.0, max=1.0, mean=0.024),\n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m                         'eps_id': np.ndarray((41,), dtype=int64, min=760950249.0, max=760950249.0, mean=760950249.0),\n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m                         'infos': np.ndarray((41,), dtype=object, head={'time': Timestamp('2004-09-27 00:00:00'), 'nlv': 99.99900004545226, 'nr_contracts': np.ndarray((3,), dtype=float64, min=0.0, max=90.909, mean=30.312), 'bid_prices': np.ndarray((3,), dtype=float64, min=1.0, max=2520.258, mean=950.943), 'ask_prices': np.ndarray((3,), dtype=float64, min=1.0, max=2520.51, mean=951.038), 'cost_of_spread': np.ndarray((3,), dtype=float64, min=0.0, max=0.0, mean=0.0), 'cost_of_commissions': np.ndarray((3,), dtype=float64, min=0.0, max=0.0, mean=0.0), 'profit_on_idle_cash': 0.0, 'margin': 0, 'weights_target': np.ndarray((3,), dtype=float64, min=0.0, max=0.909, mean=0.333), 'weights_drift': np.ndarray((3,), dtype=float64, min=0.0, max=1.0, mean=0.333), 'weights_diff': np.ndarray((3,), dtype=float64, min=-0.091, max=0.091, mean=-0.0), 'positions_value_diff': np.ndarray((3,), dtype=float64, min=-9.091, max=9.091, mean=-0.0), 'acq_prices': np.ndarray((3,), dtype=float64, min=1.0, max=2520.384, mean=950.996), 'nr_contracts_diff': np.ndarray((3,), dtype=float64, min=-9.091, max=0.027, mean=-3.021), 'nlv_pre': 100.0}),\n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m                         'new_obs': np.ndarray((41, 5), dtype=float32, min=0.0, max=1.0, mean=0.352),\n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m                         'obs': np.ndarray((41, 5), dtype=float32, min=0.0, max=1.0, mean=0.347),\n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m                         'prev_actions': np.ndarray((41, 2), dtype=float32, min=-2.035, max=2.791, mean=0.07),\n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m                         'prev_rewards': np.ndarray((41,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m                         'rewards': np.ndarray((41,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m                         't': np.ndarray((41,), dtype=int64, min=0.0, max=40.0, mean=20.0),\n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m                         'unroll_id': np.ndarray((41,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m                         'value_targets': np.ndarray((41,), dtype=float64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m                         'vf_preds': np.ndarray((41,), dtype=float32, min=0.0, max=0.0, mean=0.0)},\n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m               'type': 'SampleBatch'}}\n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m 2019-07-31 17:03:59,230\tINFO rollout_worker.py:428 -- Generating sample batch of size 200\n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m 2019-07-31 17:03:59,303\tINFO sampler.py:308 -- Raw obs from env: { 0: { 'agent0': np.ndarray((5,), dtype=float64, min=-1.0, max=5.812, mean=1.513)}}\n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m 2019-07-31 17:03:59,311\tINFO sampler.py:309 -- Info return from env: {0: {'agent0': None}}\n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m 2019-07-31 17:03:59,311\tINFO sampler.py:407 -- Preprocessed obs: np.ndarray((5,), dtype=float64, min=-1.0, max=5.812, mean=1.513)\n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m 2019-07-31 17:03:59,311\tINFO sampler.py:411 -- Filtered obs: np.ndarray((5,), dtype=float64, min=-1.0, max=5.812, mean=1.513)\n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m 2019-07-31 17:03:59,313\tINFO sampler.py:525 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m                                   'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m                                   'obs': np.ndarray((5,), dtype=float64, min=-1.0, max=5.812, mean=1.513),\n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m 2019-07-31 17:03:59,313\tINFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m 2019-07-31 17:03:59,499\tINFO sampler.py:552 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m { 'default_policy': ( np.ndarray((1, 2), dtype=float32, min=-1.199, max=-0.363, mean=-0.781),\n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m                       [],\n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m                       { 'action_prob': np.ndarray((1,), dtype=float32, min=0.073, max=0.073, mean=0.073),\n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m                         'behaviour_logits': np.ndarray((1, 4), dtype=float32, min=-0.003, max=0.005, mean=-0.001),\n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=0.0, max=0.0, mean=0.0)})}\n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m 2019-07-31 17:03:59,971\tINFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m { 'agent0': { 'data': { 'action_prob': np.ndarray((41,), dtype=float32, min=0.0, max=0.141, mean=0.074),\n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m                         'actions': np.ndarray((41, 2), dtype=float32, min=-1.976, max=3.319, mean=0.073),\n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m                         'advantages': np.ndarray((41,), dtype=float32, min=-0.0, max=0.001, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m                         'agent_index': np.ndarray((41,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m                         'behaviour_logits': np.ndarray((41, 4), dtype=float32, min=-0.007, max=0.008, mean=-0.002),\n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m                         'dones': np.ndarray((41,), dtype=bool, min=0.0, max=1.0, mean=0.024),\n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m                         'eps_id': np.ndarray((41,), dtype=int64, min=1786666965.0, max=1786666965.0, mean=1786666965.0),\n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m                         'infos': np.ndarray((41,), dtype=object, head={'time': Timestamp('2015-07-27 00:00:00'), 'nlv': 100.0, 'nr_contracts': np.ndarray((3,), dtype=float64, min=0.0, max=100.0, mean=33.333), 'bid_prices': np.ndarray((3,), dtype=float64, min=1.0, max=6120.967, mean=2228.182), 'ask_prices': np.ndarray((3,), dtype=float64, min=1.0, max=6121.579, mean=2228.404), 'cost_of_spread': np.ndarray((3,), dtype=float64, min=0.0, max=0.0, mean=0.0), 'cost_of_commissions': np.ndarray((3,), dtype=float64, min=0.0, max=0.0, mean=0.0), 'profit_on_idle_cash': 0.0, 'margin': 0, 'weights_target': np.ndarray((3,), dtype=float64, min=0.0, max=1.0, mean=0.333), 'weights_drift': np.ndarray((3,), dtype=float64, min=0.0, max=1.0, mean=0.333), 'weights_diff': np.ndarray((3,), dtype=float64, min=0.0, max=0.0, mean=0.0), 'positions_value_diff': np.ndarray((3,), dtype=float64, min=0.0, max=0.0, mean=0.0), 'acq_prices': np.ndarray((3,), dtype=float64, min=1.0, max=6121.273, mean=2228.293), 'nr_contracts_diff': np.ndarray((3,), dtype=float64, min=0.0, max=0.0, mean=0.0), 'nlv_pre': 100.0}),\n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m                         'new_obs': np.ndarray((41, 5), dtype=float32, min=-3.138, max=2.73, mean=0.249),\n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m                         'obs': np.ndarray((41, 5), dtype=float32, min=-3.138, max=5.812, mean=0.276),\n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m                         'prev_actions': np.ndarray((41, 2), dtype=float32, min=-1.976, max=3.319, mean=0.046),\n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m                         'prev_rewards': np.ndarray((41,), dtype=float32, min=-0.0, max=0.001, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m                         'rewards': np.ndarray((41,), dtype=float32, min=-0.0, max=0.001, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m                         't': np.ndarray((41,), dtype=int64, min=0.0, max=40.0, mean=20.0),\n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m                         'unroll_id': np.ndarray((41,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m                         'value_targets': np.ndarray((41,), dtype=float64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m                         'vf_preds': np.ndarray((41,), dtype=float32, min=0.0, max=0.0, mean=0.0)},\n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m               'type': 'SampleBatch'}}\n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m 2019-07-31 17:04:00,556\tINFO rollout_worker.py:462 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m { 'data': { 'action_prob': np.ndarray((205,), dtype=float32, min=0.0, max=0.156, mean=0.082),\n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m             'actions': np.ndarray((205, 2), dtype=float32, min=-2.791, max=3.359, mean=0.071),\n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m             'advantages': np.ndarray((205,), dtype=float32, min=-0.008, max=0.006, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m             'agent_index': np.ndarray((205,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m             'behaviour_logits': np.ndarray((205, 4), dtype=float32, min=-0.005, max=0.008, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m             'dones': np.ndarray((205,), dtype=bool, min=0.0, max=1.0, mean=0.024),\n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m             'eps_id': np.ndarray((205,), dtype=int64, min=760950249.0, max=1965745360.0, mean=1278949232.8),\n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m             'infos': np.ndarray((205,), dtype=object, head={'time': Timestamp('2004-09-27 00:00:00'), 'nlv': 99.99900004545226, 'nr_contracts': np.ndarray((3,), dtype=float64, min=0.0, max=90.909, mean=30.312), 'bid_prices': np.ndarray((3,), dtype=float64, min=1.0, max=2520.258, mean=950.943), 'ask_prices': np.ndarray((3,), dtype=float64, min=1.0, max=2520.51, mean=951.038), 'cost_of_spread': np.ndarray((3,), dtype=float64, min=0.0, max=0.0, mean=0.0), 'cost_of_commissions': np.ndarray((3,), dtype=float64, min=0.0, max=0.0, mean=0.0), 'profit_on_idle_cash': 0.0, 'margin': 0, 'weights_target': np.ndarray((3,), dtype=float64, min=0.0, max=0.909, mean=0.333), 'weights_drift': np.ndarray((3,), dtype=float64, min=0.0, max=1.0, mean=0.333), 'weights_diff': np.ndarray((3,), dtype=float64, min=-0.091, max=0.091, mean=-0.0), 'positions_value_diff': np.ndarray((3,), dtype=float64, min=-9.091, max=9.091, mean=-0.0), 'acq_prices': np.ndarray((3,), dtype=float64, min=1.0, max=2520.384, mean=950.996), 'nr_contracts_diff': np.ndarray((3,), dtype=float64, min=-9.091, max=0.027, mean=-3.021), 'nlv_pre': 100.0}),\n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m             'new_obs': np.ndarray((205, 5), dtype=float32, min=-4.434, max=2.777, mean=0.153),\n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m             'obs': np.ndarray((205, 5), dtype=float32, min=-4.434, max=2.777, mean=0.159),\n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m             'prev_actions': np.ndarray((205, 2), dtype=float32, min=-2.791, max=3.359, mean=0.075),\n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m             'prev_rewards': np.ndarray((205,), dtype=float32, min=-0.005, max=0.005, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m             'rewards': np.ndarray((205,), dtype=float32, min=-0.005, max=0.005, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m             't': np.ndarray((205,), dtype=int64, min=0.0, max=40.0, mean=20.0),\n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m             'unroll_id': np.ndarray((205,), dtype=int64, min=0.0, max=4.0, mean=2.0),\n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m             'value_targets': np.ndarray((205,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m             'vf_preds': np.ndarray((205,), dtype=float32, min=0.0, max=0.0, mean=0.0)},\n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m   'type': 'SampleBatch'}\n",
      "\u001b[2m\u001b[36m(pid=99339)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m 2019-07-31 17:04:01,509\tINFO rollout_worker.py:462 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m { 'data': { 'action_prob': np.ndarray((205,), dtype=float32, min=0.0, max=0.158, mean=0.081),\n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m             'actions': np.ndarray((205, 2), dtype=float32, min=-2.922, max=3.319, mean=-0.073),\n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m             'advantages': np.ndarray((205,), dtype=float32, min=-0.01, max=0.007, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m             'agent_index': np.ndarray((205,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m             'behaviour_logits': np.ndarray((205, 4), dtype=float32, min=-0.008, max=0.009, mean=-0.002),\n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m             'dones': np.ndarray((205,), dtype=bool, min=0.0, max=1.0, mean=0.024),\n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m             'eps_id': np.ndarray((205,), dtype=int64, min=182568176.0, max=1786666965.0, mean=1275921833.4),\n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m             'infos': np.ndarray((205,), dtype=object, head={'time': Timestamp('2015-07-27 00:00:00'), 'nlv': 100.0, 'nr_contracts': np.ndarray((3,), dtype=float64, min=0.0, max=100.0, mean=33.333), 'bid_prices': np.ndarray((3,), dtype=float64, min=1.0, max=6120.967, mean=2228.182), 'ask_prices': np.ndarray((3,), dtype=float64, min=1.0, max=6121.579, mean=2228.404), 'cost_of_spread': np.ndarray((3,), dtype=float64, min=0.0, max=0.0, mean=0.0), 'cost_of_commissions': np.ndarray((3,), dtype=float64, min=0.0, max=0.0, mean=0.0), 'profit_on_idle_cash': 0.0, 'margin': 0, 'weights_target': np.ndarray((3,), dtype=float64, min=0.0, max=1.0, mean=0.333), 'weights_drift': np.ndarray((3,), dtype=float64, min=0.0, max=1.0, mean=0.333), 'weights_diff': np.ndarray((3,), dtype=float64, min=0.0, max=0.0, mean=0.0), 'positions_value_diff': np.ndarray((3,), dtype=float64, min=0.0, max=0.0, mean=0.0), 'acq_prices': np.ndarray((3,), dtype=float64, min=1.0, max=6121.273, mean=2228.293), 'nr_contracts_diff': np.ndarray((3,), dtype=float64, min=0.0, max=0.0, mean=0.0), 'nlv_pre': 100.0}),\n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m             'new_obs': np.ndarray((205, 5), dtype=float32, min=-4.434, max=2.777, mean=0.097),\n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m             'obs': np.ndarray((205, 5), dtype=float32, min=-4.434, max=5.812, mean=0.102),\n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m             'prev_actions': np.ndarray((205, 2), dtype=float32, min=-2.922, max=3.319, mean=-0.076),\n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m             'prev_rewards': np.ndarray((205,), dtype=float32, min=-0.008, max=0.006, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m             'rewards': np.ndarray((205,), dtype=float32, min=-0.008, max=0.006, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m             't': np.ndarray((205,), dtype=int64, min=0.0, max=40.0, mean=20.0),\n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m             'unroll_id': np.ndarray((205,), dtype=int64, min=0.0, max=4.0, mean=2.0),\n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m             'value_targets': np.ndarray((205,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m             'vf_preds': np.ndarray((205,), dtype=float32, min=0.0, max=0.0, mean=0.0)},\n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m   'type': 'SampleBatch'}\n",
      "\u001b[2m\u001b[36m(pid=99239)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99342)\u001b[0m 2019-07-31 17:04:24,909\tINFO multi_gpu_impl.py:146 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(pid=99342)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99342)\u001b[0m { 'inputs': [ np.ndarray((4100, 2), dtype=float32, min=-3.907, max=3.955, mean=-0.008),\n",
      "\u001b[2m\u001b[36m(pid=99342)\u001b[0m               np.ndarray((4100,), dtype=float32, min=-0.021, max=0.016, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=99342)\u001b[0m               np.ndarray((4100, 5), dtype=float32, min=-14.063, max=15.356, mean=0.147),\n",
      "\u001b[2m\u001b[36m(pid=99342)\u001b[0m               np.ndarray((4100, 2), dtype=float32, min=-3.907, max=3.955, mean=-0.006),\n",
      "\u001b[2m\u001b[36m(pid=99342)\u001b[0m               np.ndarray((4100,), dtype=float32, min=-5.653, max=8.1, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=99342)\u001b[0m               np.ndarray((4100, 4), dtype=float32, min=-0.008, max=0.01, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=99342)\u001b[0m               np.ndarray((4100,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=99342)\u001b[0m               np.ndarray((4100,), dtype=float32, min=0.0, max=0.0, mean=0.0)],\n",
      "\u001b[2m\u001b[36m(pid=99342)\u001b[0m   'placeholders': [ <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99342)\u001b[0m                     <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99342)\u001b[0m                     <tf.Tensor 'default_policy/observation:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99342)\u001b[0m                     <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99342)\u001b[0m                     <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99342)\u001b[0m                     <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 4) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99342)\u001b[0m                     <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99342)\u001b[0m                     <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>],\n",
      "\u001b[2m\u001b[36m(pid=99342)\u001b[0m   'state_inputs': []}\n",
      "\u001b[2m\u001b[36m(pid=99342)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99342)\u001b[0m 2019-07-31 17:04:24,909\tINFO multi_gpu_impl.py:191 -- Divided 4100 rollout sequences, each of length 1, among 1 devices.\n",
      "\u001b[2m\u001b[36m(pid=99333)\u001b[0m 2019-07-31 17:04:25,093\tINFO multi_gpu_impl.py:146 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(pid=99333)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99333)\u001b[0m { 'inputs': [ np.ndarray((4100, 2), dtype=float32, min=-3.529, max=3.678, mean=0.015),\n",
      "\u001b[2m\u001b[36m(pid=99333)\u001b[0m               np.ndarray((4100,), dtype=float32, min=-0.021, max=0.017, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=99333)\u001b[0m               np.ndarray((4100, 5), dtype=float32, min=-14.063, max=15.356, mean=0.129),\n",
      "\u001b[2m\u001b[36m(pid=99333)\u001b[0m               np.ndarray((4100, 2), dtype=float32, min=-3.529, max=3.678, mean=0.016),\n",
      "\u001b[2m\u001b[36m(pid=99333)\u001b[0m               np.ndarray((4100,), dtype=float32, min=-9.277, max=7.567, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=99333)\u001b[0m               np.ndarray((4100, 4), dtype=float32, min=-0.011, max=0.01, mean=-0.002),\n",
      "\u001b[2m\u001b[36m(pid=99333)\u001b[0m               np.ndarray((4100,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=99333)\u001b[0m               np.ndarray((4100,), dtype=float32, min=0.0, max=0.0, mean=0.0)],\n",
      "\u001b[2m\u001b[36m(pid=99333)\u001b[0m   'placeholders': [ <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99333)\u001b[0m                     <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99333)\u001b[0m                     <tf.Tensor 'default_policy/observation:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99333)\u001b[0m                     <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99333)\u001b[0m                     <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99333)\u001b[0m                     <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 4) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99333)\u001b[0m                     <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99333)\u001b[0m                     <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>],\n",
      "\u001b[2m\u001b[36m(pid=99333)\u001b[0m   'state_inputs': []}\n",
      "\u001b[2m\u001b[36m(pid=99333)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99333)\u001b[0m 2019-07-31 17:04:25,093\tINFO multi_gpu_impl.py:191 -- Divided 4100 rollout sequences, each of length 1, among 1 devices.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-31 17:04:28,407\tERROR trial_runner.py:487 -- Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/Nicholas/.venv/lib/python3.6/site-packages/ray/tune/trial_runner.py\", line 468, in _process_trial\n",
      "    result, terminate=(decision == TrialScheduler.STOP))\n",
      "  File \"/home/Nicholas/.venv/lib/python3.6/site-packages/ray/tune/trial.py\", line 533, in update_last_result\n",
      "    self.result_logger.on_result(self.last_result)\n",
      "  File \"/home/Nicholas/.venv/lib/python3.6/site-packages/ray/tune/logger.py\", line 239, in on_result\n",
      "    _logger.on_result(result)\n",
      "  File \"/home/Nicholas/.venv/lib/python3.6/site-packages/trading_gym/ray/logger.py\", line 69, in on_result\n",
      "    for fold in result['trading-gym']:\n",
      "KeyError: 'trading-gym'\n",
      "2019-07-31 17:04:28,411\tINFO ray_trial_executor.py:187 -- Destroying actor for trial PPO_GAIAPredictorsContinuousV9_6_clip_param=0.7,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2019-07-31 17:04:28,534\tWARNING signature.py:108 -- The function with_updates has a **kwargs argument, which is currently not supported.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=99342)\u001b[0m 2019-07-31 17:04:28,400\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 39.7/67.5 GB\n",
      "Result logdir: logs/clip_exps_aric/clip_exps\n",
      "Number of trials: 11 ({'ERROR': 7, 'RUNNING': 1, 'PENDING': 3})\n",
      "ERROR trials:\n",
      " - PPO_GAIAPredictorsContinuousV9_0_clip_param=0.1,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tERROR, 1 failures: logs/clip_exps_aric/clip_exps/PPO_GAIAPredictorsContinuousV9_0_clip_param=0.1,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05_2019-07-31_16-59-24r06fnmw4/error_2019-07-31_17-00-59.txt, [3 CPUs, 0 GPUs], [pid=89162], 55 s, 1 iter, 4100 ts, 0.00508 rew\n",
      " - PPO_GAIAPredictorsContinuousV9_1_clip_param=0.2,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tERROR, 1 failures: logs/clip_exps_aric/clip_exps/PPO_GAIAPredictorsContinuousV9_1_clip_param=0.2,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05_2019-07-31_16-59-259aw8hwth/error_2019-07-31_17-00-56.txt, [3 CPUs, 0 GPUs], [pid=89159], 49 s, 1 iter, 4100 ts, 0.00465 rew\n",
      " - PPO_GAIAPredictorsContinuousV9_2_clip_param=0.3,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tERROR, 1 failures: logs/clip_exps_aric/clip_exps/PPO_GAIAPredictorsContinuousV9_2_clip_param=0.3,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05_2019-07-31_17-00-56qkigs8s9/error_2019-07-31_17-02-11.txt, [3 CPUs, 0 GPUs], [pid=89156], 45 s, 1 iter, 4100 ts, 0.00346 rew\n",
      " - PPO_GAIAPredictorsContinuousV9_3_clip_param=0.4,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tERROR, 1 failures: logs/clip_exps_aric/clip_exps/PPO_GAIAPredictorsContinuousV9_3_clip_param=0.4,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05_2019-07-31_17-00-59cwpbby9k/error_2019-07-31_17-02-15.txt, [3 CPUs, 0 GPUs], [pid=89160], 42 s, 1 iter, 4100 ts, 0.00395 rew\n",
      " - PPO_GAIAPredictorsContinuousV9_4_clip_param=0.5,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tERROR, 1 failures: logs/clip_exps_aric/clip_exps/PPO_GAIAPredictorsContinuousV9_4_clip_param=0.5,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05_2019-07-31_17-02-11yrg8eih7/error_2019-07-31_17-03-30.txt, [3 CPUs, 0 GPUs], [pid=98159], 45 s, 1 iter, 4100 ts, 0.00312 rew\n",
      " - PPO_GAIAPredictorsContinuousV9_5_clip_param=0.6,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tERROR, 1 failures: logs/clip_exps_aric/clip_exps/PPO_GAIAPredictorsContinuousV9_5_clip_param=0.6,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05_2019-07-31_17-02-15dfj8taa2/error_2019-07-31_17-03-29.txt, [3 CPUs, 0 GPUs], [pid=98157], 37 s, 1 iter, 4100 ts, 0.0025 rew\n",
      " - PPO_GAIAPredictorsContinuousV9_6_clip_param=0.7,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tERROR, 1 failures: logs/clip_exps_aric/clip_exps/PPO_GAIAPredictorsContinuousV9_6_clip_param=0.7,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05_2019-07-31_17-03-2999481rgf/error_2019-07-31_17-04-28.txt, [3 CPUs, 0 GPUs], [pid=99342], 31 s, 1 iter, 4100 ts, 0.0051 rew\n",
      "PENDING trials:\n",
      " - PPO_GAIAPredictorsContinuousV9_8_clip_param=0.9,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tPENDING\n",
      " - PPO_GAIAPredictorsContinuousV9_9_clip_param=1.0,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tPENDING\n",
      " - PPO_GAIAPredictorsContinuousV9_10_clip_param=1.1,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tPENDING\n",
      "RUNNING trials:\n",
      " - PPO_GAIAPredictorsContinuousV9_7_clip_param=0.8,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tRUNNING\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-31 17:04:28,689\tWARNING util.py:64 -- The `start_trial` operation took 0.17278313636779785 seconds to complete, which may be a performance bottleneck.\n",
      "2019-07-31 17:04:28,821\tWARNING util.py:64 -- The `experiment_checkpoint` operation took 0.12123465538024902 seconds to complete, which may be a performance bottleneck.\n",
      "2019-07-31 17:04:29,109\tERROR trial_runner.py:487 -- Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/Nicholas/.venv/lib/python3.6/site-packages/ray/tune/trial_runner.py\", line 468, in _process_trial\n",
      "    result, terminate=(decision == TrialScheduler.STOP))\n",
      "  File \"/home/Nicholas/.venv/lib/python3.6/site-packages/ray/tune/trial.py\", line 533, in update_last_result\n",
      "    self.result_logger.on_result(self.last_result)\n",
      "  File \"/home/Nicholas/.venv/lib/python3.6/site-packages/ray/tune/logger.py\", line 239, in on_result\n",
      "    _logger.on_result(result)\n",
      "  File \"/home/Nicholas/.venv/lib/python3.6/site-packages/trading_gym/ray/logger.py\", line 69, in on_result\n",
      "    for fold in result['trading-gym']:\n",
      "KeyError: 'trading-gym'\n",
      "2019-07-31 17:04:29,113\tINFO ray_trial_executor.py:187 -- Destroying actor for trial PPO_GAIAPredictorsContinuousV9_7_clip_param=0.8,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2019-07-31 17:04:29,179\tWARNING signature.py:108 -- The function with_updates has a **kwargs argument, which is currently not supported.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=99333)\u001b[0m 2019-07-31 17:04:29,105\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=99332)\u001b[0m 2019-07-31 17:04:32,857\tWARNING ppo.py:151 -- FYI: By default, the value function will not share layers with the policy model ('vf_share_layers': False).\n",
      "\u001b[2m\u001b[36m(pid=99336)\u001b[0m 2019-07-31 17:04:33,357\tWARNING ppo.py:151 -- FYI: By default, the value function will not share layers with the policy model ('vf_share_layers': False).\n",
      "\u001b[2m\u001b[36m(pid=99332)\u001b[0m 2019-07-31 17:04:36,803\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 0 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=99332)\u001b[0m 2019-07-31 17:04:36.804453: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=99336)\u001b[0m 2019-07-31 17:04:37,047\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 0 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=99336)\u001b[0m 2019-07-31 17:04:37.054879: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=99336)\u001b[0m 2019-07-31 17:04:37,489\tINFO dynamic_tf_policy.py:313 -- Initializing loss function with dummy input:\n",
      "\u001b[2m\u001b[36m(pid=99336)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99336)\u001b[0m { 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99336)\u001b[0m   'actions': <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99336)\u001b[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99336)\u001b[0m   'behaviour_logits': <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 4) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99336)\u001b[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,\n",
      "\u001b[2m\u001b[36m(pid=99336)\u001b[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99336)\u001b[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99336)\u001b[0m   'prev_actions': <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99336)\u001b[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99336)\u001b[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99336)\u001b[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99336)\u001b[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}\n",
      "\u001b[2m\u001b[36m(pid=99336)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99332)\u001b[0m 2019-07-31 17:04:37,629\tINFO dynamic_tf_policy.py:313 -- Initializing loss function with dummy input:\n",
      "\u001b[2m\u001b[36m(pid=99332)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99332)\u001b[0m { 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99332)\u001b[0m   'actions': <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99332)\u001b[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99332)\u001b[0m   'behaviour_logits': <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 4) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99332)\u001b[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,\n",
      "\u001b[2m\u001b[36m(pid=99332)\u001b[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99332)\u001b[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99332)\u001b[0m   'prev_actions': <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99332)\u001b[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99332)\u001b[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99332)\u001b[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99332)\u001b[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}\n",
      "\u001b[2m\u001b[36m(pid=99332)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99336)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=99336)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99336)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=99336)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99332)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=99332)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99332)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=99332)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99336)\u001b[0m 2019-07-31 17:04:40,587\tINFO rollout_worker.py:719 -- Built policy map: {'default_policy': <ray.rllib.policy.tf_policy_template.PPOTFPolicy object at 0x7f9035374588>}\n",
      "\u001b[2m\u001b[36m(pid=99336)\u001b[0m 2019-07-31 17:04:40,587\tINFO rollout_worker.py:720 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7f9035374240>}\n",
      "\u001b[2m\u001b[36m(pid=99336)\u001b[0m 2019-07-31 17:04:40,587\tINFO rollout_worker.py:333 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7f90353740f0>}\n",
      "\u001b[2m\u001b[36m(pid=99336)\u001b[0m 2019-07-31 17:04:40,867\tINFO multi_gpu_optimizer.py:79 -- LocalMultiGPUOptimizer devices ['/cpu:0']\n",
      "\u001b[2m\u001b[36m(pid=99332)\u001b[0m 2019-07-31 17:04:40,989\tINFO rollout_worker.py:719 -- Built policy map: {'default_policy': <ray.rllib.policy.tf_policy_template.PPOTFPolicy object at 0x7ff6a84a5630>}\n",
      "\u001b[2m\u001b[36m(pid=99332)\u001b[0m 2019-07-31 17:04:40,989\tINFO rollout_worker.py:720 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7ff6a84a52e8>}\n",
      "\u001b[2m\u001b[36m(pid=99332)\u001b[0m 2019-07-31 17:04:40,989\tINFO rollout_worker.py:333 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7ff6a84a5198>}\n",
      "\u001b[2m\u001b[36m(pid=99332)\u001b[0m 2019-07-31 17:04:41,114\tINFO multi_gpu_optimizer.py:79 -- LocalMultiGPUOptimizer devices ['/cpu:0']\n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m 2019-07-31 17:04:55,546\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 1 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m 2019-07-31 17:04:55.791466: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m 2019-07-31 17:04:56,864\tINFO dynamic_tf_policy.py:313 -- Initializing loss function with dummy input:\n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m { 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m   'actions': <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m   'behaviour_logits': <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 4) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,\n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m   'prev_actions': <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}\n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m 2019-07-31 17:05:03,034\tINFO rollout_worker.py:428 -- Generating sample batch of size 200\n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m 2019-07-31 17:05:03,086\tINFO sampler.py:308 -- Raw obs from env: { 0: { 'agent0': np.ndarray((5,), dtype=float64, min=0.0, max=1.0, mean=0.2)}}\n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m 2019-07-31 17:05:03,086\tINFO sampler.py:309 -- Info return from env: {0: {'agent0': None}}\n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m 2019-07-31 17:05:03,087\tINFO sampler.py:407 -- Preprocessed obs: np.ndarray((5,), dtype=float64, min=0.0, max=1.0, mean=0.2)\n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m 2019-07-31 17:05:03,087\tINFO sampler.py:411 -- Filtered obs: np.ndarray((5,), dtype=float64, min=0.0, max=1.0, mean=0.2)\n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m 2019-07-31 17:05:03,089\tINFO sampler.py:525 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m                                   'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m                                   'obs': np.ndarray((5,), dtype=float64, min=0.0, max=1.0, mean=0.2),\n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m 2019-07-31 17:05:03,089\tINFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m 2019-07-31 17:05:03,236\tINFO sampler.py:552 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m { 'default_policy': ( np.ndarray((1, 2), dtype=float32, min=-0.241, max=0.339, mean=0.049),\n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m                       [],\n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m                       { 'action_prob': np.ndarray((1,), dtype=float32, min=0.146, max=0.146, mean=0.146),\n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m                         'behaviour_logits': np.ndarray((1, 4), dtype=float32, min=-0.007, max=0.001, mean=-0.002),\n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=0.0, max=0.0, mean=0.0)})}\n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m 2019-07-31 17:05:03,795\tINFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m { 'agent0': { 'data': { 'action_prob': np.ndarray((41,), dtype=float32, min=0.001, max=0.156, mean=0.074),\n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m                         'actions': np.ndarray((41, 2), dtype=float32, min=-2.298, max=3.08, mean=0.025),\n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m                         'advantages': np.ndarray((41,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m                         'agent_index': np.ndarray((41,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m                         'behaviour_logits': np.ndarray((41, 4), dtype=float32, min=-0.009, max=0.004, mean=-0.002),\n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m                         'dones': np.ndarray((41,), dtype=bool, min=0.0, max=1.0, mean=0.024),\n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m                         'eps_id': np.ndarray((41,), dtype=int64, min=152460614.0, max=152460614.0, mean=152460614.0),\n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m                         'infos': np.ndarray((41,), dtype=object, head={'time': Timestamp('2015-08-20 00:00:00'), 'nlv': 99.99900004545226, 'nr_contracts': np.ndarray((3,), dtype=float64, min=0.0, max=90.909, mean=30.308), 'bid_prices': np.ndarray((3,), dtype=float64, min=1.0, max=6040.663, mean=2204.015), 'ask_prices': np.ndarray((3,), dtype=float64, min=1.0, max=6041.267, mean=2204.235), 'cost_of_spread': np.ndarray((3,), dtype=float64, min=0.0, max=0.0, mean=0.0), 'cost_of_commissions': np.ndarray((3,), dtype=float64, min=0.0, max=0.0, mean=0.0), 'profit_on_idle_cash': 0.0, 'margin': 0, 'weights_target': np.ndarray((3,), dtype=float64, min=0.0, max=0.909, mean=0.333), 'weights_drift': np.ndarray((3,), dtype=float64, min=0.0, max=1.0, mean=0.333), 'weights_diff': np.ndarray((3,), dtype=float64, min=-0.091, max=0.091, mean=-0.0), 'positions_value_diff': np.ndarray((3,), dtype=float64, min=-9.091, max=9.091, mean=-0.0), 'acq_prices': np.ndarray((3,), dtype=float64, min=1.0, max=6040.965, mean=2204.135), 'nr_contracts_diff': np.ndarray((3,), dtype=float64, min=-9.091, max=0.016, mean=-3.025), 'nlv_pre': 100.0}),\n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m                         'new_obs': np.ndarray((41, 5), dtype=float32, min=0.0, max=1.0, mean=0.352),\n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m                         'obs': np.ndarray((41, 5), dtype=float32, min=0.0, max=1.0, mean=0.347),\n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m                         'prev_actions': np.ndarray((41, 2), dtype=float32, min=-2.298, max=3.08, mean=0.008),\n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m                         'prev_rewards': np.ndarray((41,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m                         'rewards': np.ndarray((41,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m                         't': np.ndarray((41,), dtype=int64, min=0.0, max=40.0, mean=20.0),\n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m                         'unroll_id': np.ndarray((41,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m                         'value_targets': np.ndarray((41,), dtype=float64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m                         'vf_preds': np.ndarray((41,), dtype=float32, min=0.0, max=0.0, mean=0.0)},\n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m               'type': 'SampleBatch'}}\n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m 2019-07-31 17:05:06,132\tINFO rollout_worker.py:462 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m { 'data': { 'action_prob': np.ndarray((205,), dtype=float32, min=0.0, max=0.156, mean=0.082),\n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m             'actions': np.ndarray((205, 2), dtype=float32, min=-3.509, max=3.22, mean=0.05),\n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m             'advantages': np.ndarray((205,), dtype=float32, min=-0.02, max=0.02, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m             'agent_index': np.ndarray((205,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m             'behaviour_logits': np.ndarray((205, 4), dtype=float32, min=-0.009, max=0.008, mean=0.002),\n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m             'dones': np.ndarray((205,), dtype=bool, min=0.0, max=1.0, mean=0.024),\n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m             'eps_id': np.ndarray((205,), dtype=int64, min=152460614.0, max=1969128963.0, mean=1115745477.8),\n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m             'infos': np.ndarray((205,), dtype=object, head={'time': Timestamp('2015-08-20 00:00:00'), 'nlv': 99.99900004545226, 'nr_contracts': np.ndarray((3,), dtype=float64, min=0.0, max=90.909, mean=30.308), 'bid_prices': np.ndarray((3,), dtype=float64, min=1.0, max=6040.663, mean=2204.015), 'ask_prices': np.ndarray((3,), dtype=float64, min=1.0, max=6041.267, mean=2204.235), 'cost_of_spread': np.ndarray((3,), dtype=float64, min=0.0, max=0.0, mean=0.0), 'cost_of_commissions': np.ndarray((3,), dtype=float64, min=0.0, max=0.0, mean=0.0), 'profit_on_idle_cash': 0.0, 'margin': 0, 'weights_target': np.ndarray((3,), dtype=float64, min=0.0, max=0.909, mean=0.333), 'weights_drift': np.ndarray((3,), dtype=float64, min=0.0, max=1.0, mean=0.333), 'weights_diff': np.ndarray((3,), dtype=float64, min=-0.091, max=0.091, mean=-0.0), 'positions_value_diff': np.ndarray((3,), dtype=float64, min=-9.091, max=9.091, mean=-0.0), 'acq_prices': np.ndarray((3,), dtype=float64, min=1.0, max=6040.965, mean=2204.135), 'nr_contracts_diff': np.ndarray((3,), dtype=float64, min=-9.091, max=0.016, mean=-3.025), 'nlv_pre': 100.0}),\n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m             'new_obs': np.ndarray((205, 5), dtype=float32, min=-4.434, max=2.777, mean=0.126),\n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m             'obs': np.ndarray((205, 5), dtype=float32, min=-4.434, max=4.419, mean=0.126),\n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m             'prev_actions': np.ndarray((205, 2), dtype=float32, min=-2.5, max=3.22, mean=0.045),\n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m             'prev_rewards': np.ndarray((205,), dtype=float32, min=-0.015, max=0.009, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m             'rewards': np.ndarray((205,), dtype=float32, min=-0.015, max=0.009, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m             't': np.ndarray((205,), dtype=int64, min=0.0, max=40.0, mean=20.0),\n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m             'unroll_id': np.ndarray((205,), dtype=int64, min=0.0, max=4.0, mean=2.0),\n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m             'value_targets': np.ndarray((205,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m             'vf_preds': np.ndarray((205,), dtype=float32, min=0.0, max=0.0, mean=0.0)},\n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m   'type': 'SampleBatch'}\n",
      "\u001b[2m\u001b[36m(pid=99234)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m 2019-07-31 17:05:07,730\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 1 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m 2019-07-31 17:05:07.807791: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m 2019-07-31 17:05:08,647\tINFO dynamic_tf_policy.py:313 -- Initializing loss function with dummy input:\n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m { 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m   'actions': <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m   'behaviour_logits': <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 4) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,\n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m   'prev_actions': <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}\n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=101206)\u001b[0m 2019-07-31 17:05:09,535\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 2 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=101206)\u001b[0m 2019-07-31 17:05:09.594038: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=101214)\u001b[0m 2019-07-31 17:05:10,757\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 2 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=101214)\u001b[0m 2019-07-31 17:05:10.892867: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=101206)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=101206)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=101206)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=101206)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=101214)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=101214)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=101214)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=101214)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m 2019-07-31 17:05:14,751\tINFO rollout_worker.py:428 -- Generating sample batch of size 200\n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m 2019-07-31 17:05:14,822\tINFO sampler.py:308 -- Raw obs from env: { 0: { 'agent0': np.ndarray((5,), dtype=float64, min=-1.0, max=0.208, mean=-0.139)}}\n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m 2019-07-31 17:05:14,841\tINFO sampler.py:309 -- Info return from env: {0: {'agent0': None}}\n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m 2019-07-31 17:05:14,841\tINFO sampler.py:407 -- Preprocessed obs: np.ndarray((5,), dtype=float64, min=-1.0, max=0.208, mean=-0.139)\n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m 2019-07-31 17:05:14,869\tINFO sampler.py:411 -- Filtered obs: np.ndarray((5,), dtype=float64, min=-1.0, max=0.208, mean=-0.139)\n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m 2019-07-31 17:05:14,871\tINFO sampler.py:525 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m                                   'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m                                   'obs': np.ndarray((5,), dtype=float64, min=-1.0, max=0.208, mean=-0.139),\n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m 2019-07-31 17:05:14,871\tINFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m 2019-07-31 17:05:15,182\tINFO sampler.py:552 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m { 'default_policy': ( np.ndarray((1, 2), dtype=float32, min=-1.364, max=0.891, mean=-0.236),\n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m                       [],\n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m                       { 'action_prob': np.ndarray((1,), dtype=float32, min=0.042, max=0.042, mean=0.042),\n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m                         'behaviour_logits': np.ndarray((1, 4), dtype=float32, min=-0.008, max=-0.0, mean=-0.004),\n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=0.0, max=0.0, mean=0.0)})}\n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m 2019-07-31 17:05:15,925\tINFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m { 'agent0': { 'data': { 'action_prob': np.ndarray((41,), dtype=float32, min=0.003, max=0.158, mean=0.079),\n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m                         'actions': np.ndarray((41, 2), dtype=float32, min=-1.929, max=2.585, mean=-0.036),\n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m                         'advantages': np.ndarray((41,), dtype=float32, min=-0.003, max=0.004, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m                         'agent_index': np.ndarray((41,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m                         'behaviour_logits': np.ndarray((41, 4), dtype=float32, min=-0.015, max=0.008, mean=-0.004),\n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m                         'dones': np.ndarray((41,), dtype=bool, min=0.0, max=1.0, mean=0.024),\n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m                         'eps_id': np.ndarray((41,), dtype=int64, min=769172555.0, max=769172555.0, mean=769172555.0),\n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m                         'infos': np.ndarray((41,), dtype=object, head={'time': Timestamp('2016-09-09 00:00:00'), 'nlv': 99.99900004545226, 'nr_contracts': np.ndarray((3,), dtype=float64, min=0.0, max=90.909, mean=30.308), 'bid_prices': np.ndarray((3,), dtype=float64, min=1.0, max=6422.585, mean=2341.624), 'ask_prices': np.ndarray((3,), dtype=float64, min=1.0, max=6423.227, mean=2341.858), 'cost_of_spread': np.ndarray((3,), dtype=float64, min=0.0, max=0.0, mean=0.0), 'cost_of_commissions': np.ndarray((3,), dtype=float64, min=0.0, max=0.0, mean=0.0), 'profit_on_idle_cash': 0.0, 'margin': 0, 'weights_target': np.ndarray((3,), dtype=float64, min=0.0, max=0.909, mean=0.333), 'weights_drift': np.ndarray((3,), dtype=float64, min=0.0, max=1.0, mean=0.333), 'weights_diff': np.ndarray((3,), dtype=float64, min=-0.091, max=0.091, mean=-0.0), 'positions_value_diff': np.ndarray((3,), dtype=float64, min=-9.091, max=9.091, mean=-0.0), 'acq_prices': np.ndarray((3,), dtype=float64, min=1.0, max=6422.906, mean=2341.751), 'nr_contracts_diff': np.ndarray((3,), dtype=float64, min=-9.091, max=0.015, mean=-3.025), 'nlv_pre': 100.0}),\n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m                         'new_obs': np.ndarray((41, 5), dtype=float32, min=-5.824, max=5.875, mean=0.028),\n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m                         'obs': np.ndarray((41, 5), dtype=float32, min=-5.824, max=5.875, mean=0.02),\n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m                         'prev_actions': np.ndarray((41, 2), dtype=float32, min=-1.929, max=2.298, mean=-0.082),\n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m                         'prev_rewards': np.ndarray((41,), dtype=float32, min=-0.002, max=0.004, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m                         'rewards': np.ndarray((41,), dtype=float32, min=-0.002, max=0.004, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m                         't': np.ndarray((41,), dtype=int64, min=0.0, max=40.0, mean=20.0),\n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m                         'unroll_id': np.ndarray((41,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m                         'value_targets': np.ndarray((41,), dtype=float64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m                         'vf_preds': np.ndarray((41,), dtype=float32, min=0.0, max=0.0, mean=0.0)},\n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m               'type': 'SampleBatch'}}\n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m 2019-07-31 17:05:19,127\tINFO rollout_worker.py:462 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m { 'data': { 'action_prob': np.ndarray((205,), dtype=float32, min=0.001, max=0.159, mean=0.083),\n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m             'actions': np.ndarray((205, 2), dtype=float32, min=-2.797, max=2.585, mean=0.014),\n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m             'advantages': np.ndarray((205,), dtype=float32, min=-0.008, max=0.009, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m             'agent_index': np.ndarray((205,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m             'behaviour_logits': np.ndarray((205, 4), dtype=float32, min=-0.015, max=0.008, mean=-0.004),\n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m             'dones': np.ndarray((205,), dtype=bool, min=0.0, max=1.0, mean=0.024),\n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m             'eps_id': np.ndarray((205,), dtype=int64, min=623983711.0, max=954031265.0, mean=747275201.0),\n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m             'infos': np.ndarray((205,), dtype=object, head={'time': Timestamp('2016-09-09 00:00:00'), 'nlv': 99.99900004545226, 'nr_contracts': np.ndarray((3,), dtype=float64, min=0.0, max=90.909, mean=30.308), 'bid_prices': np.ndarray((3,), dtype=float64, min=1.0, max=6422.585, mean=2341.624), 'ask_prices': np.ndarray((3,), dtype=float64, min=1.0, max=6423.227, mean=2341.858), 'cost_of_spread': np.ndarray((3,), dtype=float64, min=0.0, max=0.0, mean=0.0), 'cost_of_commissions': np.ndarray((3,), dtype=float64, min=0.0, max=0.0, mean=0.0), 'profit_on_idle_cash': 0.0, 'margin': 0, 'weights_target': np.ndarray((3,), dtype=float64, min=0.0, max=0.909, mean=0.333), 'weights_drift': np.ndarray((3,), dtype=float64, min=0.0, max=1.0, mean=0.333), 'weights_diff': np.ndarray((3,), dtype=float64, min=-0.091, max=0.091, mean=-0.0), 'positions_value_diff': np.ndarray((3,), dtype=float64, min=-9.091, max=9.091, mean=-0.0), 'acq_prices': np.ndarray((3,), dtype=float64, min=1.0, max=6422.906, mean=2341.751), 'nr_contracts_diff': np.ndarray((3,), dtype=float64, min=-9.091, max=0.015, mean=-3.025), 'nlv_pre': 100.0}),\n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m             'new_obs': np.ndarray((205, 5), dtype=float32, min=-6.653, max=5.875, mean=0.086),\n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m             'obs': np.ndarray((205, 5), dtype=float32, min=-6.653, max=5.875, mean=0.088),\n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m             'prev_actions': np.ndarray((205, 2), dtype=float32, min=-2.797, max=2.501, mean=0.006),\n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m             'prev_rewards': np.ndarray((205,), dtype=float32, min=-0.007, max=0.005, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m             'rewards': np.ndarray((205,), dtype=float32, min=-0.007, max=0.005, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m             't': np.ndarray((205,), dtype=int64, min=0.0, max=40.0, mean=20.0),\n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m             'unroll_id': np.ndarray((205,), dtype=int64, min=0.0, max=4.0, mean=2.0),\n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m             'value_targets': np.ndarray((205,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m             'vf_preds': np.ndarray((205,), dtype=float32, min=0.0, max=0.0, mean=0.0)},\n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m   'type': 'SampleBatch'}\n",
      "\u001b[2m\u001b[36m(pid=101225)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99336)\u001b[0m 2019-07-31 17:05:42,585\tINFO multi_gpu_impl.py:146 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(pid=99336)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99336)\u001b[0m { 'inputs': [ np.ndarray((4100, 2), dtype=float32, min=-4.05, max=3.903, mean=0.008),\n",
      "\u001b[2m\u001b[36m(pid=99336)\u001b[0m               np.ndarray((4100,), dtype=float32, min=-0.02, max=0.013, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=99336)\u001b[0m               np.ndarray((4100, 5), dtype=float32, min=-14.063, max=10.831, mean=0.12),\n",
      "\u001b[2m\u001b[36m(pid=99336)\u001b[0m               np.ndarray((4100, 2), dtype=float32, min=-4.05, max=3.903, mean=0.006),\n",
      "\u001b[2m\u001b[36m(pid=99336)\u001b[0m               np.ndarray((4100,), dtype=float32, min=-6.963, max=6.959, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=99336)\u001b[0m               np.ndarray((4100, 4), dtype=float32, min=-0.012, max=0.012, mean=0.002),\n",
      "\u001b[2m\u001b[36m(pid=99336)\u001b[0m               np.ndarray((4100,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=99336)\u001b[0m               np.ndarray((4100,), dtype=float32, min=0.0, max=0.0, mean=0.0)],\n",
      "\u001b[2m\u001b[36m(pid=99336)\u001b[0m   'placeholders': [ <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99336)\u001b[0m                     <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99336)\u001b[0m                     <tf.Tensor 'default_policy/observation:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99336)\u001b[0m                     <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99336)\u001b[0m                     <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99336)\u001b[0m                     <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 4) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99336)\u001b[0m                     <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99336)\u001b[0m                     <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>],\n",
      "\u001b[2m\u001b[36m(pid=99336)\u001b[0m   'state_inputs': []}\n",
      "\u001b[2m\u001b[36m(pid=99336)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99336)\u001b[0m 2019-07-31 17:05:42,586\tINFO multi_gpu_impl.py:191 -- Divided 4100 rollout sequences, each of length 1, among 1 devices.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-31 17:05:48,032\tERROR trial_runner.py:487 -- Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/Nicholas/.venv/lib/python3.6/site-packages/ray/tune/trial_runner.py\", line 468, in _process_trial\n",
      "    result, terminate=(decision == TrialScheduler.STOP))\n",
      "  File \"/home/Nicholas/.venv/lib/python3.6/site-packages/ray/tune/trial.py\", line 533, in update_last_result\n",
      "    self.result_logger.on_result(self.last_result)\n",
      "  File \"/home/Nicholas/.venv/lib/python3.6/site-packages/ray/tune/logger.py\", line 239, in on_result\n",
      "    _logger.on_result(result)\n",
      "  File \"/home/Nicholas/.venv/lib/python3.6/site-packages/trading_gym/ray/logger.py\", line 69, in on_result\n",
      "    for fold in result['trading-gym']:\n",
      "KeyError: 'trading-gym'\n",
      "2019-07-31 17:05:48,037\tINFO ray_trial_executor.py:187 -- Destroying actor for trial PPO_GAIAPredictorsContinuousV9_8_clip_param=0.9,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2019-07-31 17:05:48,119\tWARNING signature.py:108 -- The function with_updates has a **kwargs argument, which is currently not supported.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=99336)\u001b[0m 2019-07-31 17:05:48,028\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 40.4/67.5 GB\n",
      "Result logdir: logs/clip_exps_aric/clip_exps\n",
      "Number of trials: 11 ({'ERROR': 9, 'RUNNING': 1, 'PENDING': 1})\n",
      "ERROR trials:\n",
      " - PPO_GAIAPredictorsContinuousV9_0_clip_param=0.1,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tERROR, 1 failures: logs/clip_exps_aric/clip_exps/PPO_GAIAPredictorsContinuousV9_0_clip_param=0.1,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05_2019-07-31_16-59-24r06fnmw4/error_2019-07-31_17-00-59.txt, [3 CPUs, 0 GPUs], [pid=89162], 55 s, 1 iter, 4100 ts, 0.00508 rew\n",
      " - PPO_GAIAPredictorsContinuousV9_1_clip_param=0.2,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tERROR, 1 failures: logs/clip_exps_aric/clip_exps/PPO_GAIAPredictorsContinuousV9_1_clip_param=0.2,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05_2019-07-31_16-59-259aw8hwth/error_2019-07-31_17-00-56.txt, [3 CPUs, 0 GPUs], [pid=89159], 49 s, 1 iter, 4100 ts, 0.00465 rew\n",
      " - PPO_GAIAPredictorsContinuousV9_2_clip_param=0.3,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tERROR, 1 failures: logs/clip_exps_aric/clip_exps/PPO_GAIAPredictorsContinuousV9_2_clip_param=0.3,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05_2019-07-31_17-00-56qkigs8s9/error_2019-07-31_17-02-11.txt, [3 CPUs, 0 GPUs], [pid=89156], 45 s, 1 iter, 4100 ts, 0.00346 rew\n",
      " - PPO_GAIAPredictorsContinuousV9_3_clip_param=0.4,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tERROR, 1 failures: logs/clip_exps_aric/clip_exps/PPO_GAIAPredictorsContinuousV9_3_clip_param=0.4,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05_2019-07-31_17-00-59cwpbby9k/error_2019-07-31_17-02-15.txt, [3 CPUs, 0 GPUs], [pid=89160], 42 s, 1 iter, 4100 ts, 0.00395 rew\n",
      " - PPO_GAIAPredictorsContinuousV9_4_clip_param=0.5,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tERROR, 1 failures: logs/clip_exps_aric/clip_exps/PPO_GAIAPredictorsContinuousV9_4_clip_param=0.5,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05_2019-07-31_17-02-11yrg8eih7/error_2019-07-31_17-03-30.txt, [3 CPUs, 0 GPUs], [pid=98159], 45 s, 1 iter, 4100 ts, 0.00312 rew\n",
      " - PPO_GAIAPredictorsContinuousV9_5_clip_param=0.6,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tERROR, 1 failures: logs/clip_exps_aric/clip_exps/PPO_GAIAPredictorsContinuousV9_5_clip_param=0.6,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05_2019-07-31_17-02-15dfj8taa2/error_2019-07-31_17-03-29.txt, [3 CPUs, 0 GPUs], [pid=98157], 37 s, 1 iter, 4100 ts, 0.0025 rew\n",
      " - PPO_GAIAPredictorsContinuousV9_6_clip_param=0.7,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tERROR, 1 failures: logs/clip_exps_aric/clip_exps/PPO_GAIAPredictorsContinuousV9_6_clip_param=0.7,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05_2019-07-31_17-03-2999481rgf/error_2019-07-31_17-04-28.txt, [3 CPUs, 0 GPUs], [pid=99342], 31 s, 1 iter, 4100 ts, 0.0051 rew\n",
      " - PPO_GAIAPredictorsContinuousV9_7_clip_param=0.8,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tERROR, 1 failures: logs/clip_exps_aric/clip_exps/PPO_GAIAPredictorsContinuousV9_7_clip_param=0.8,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05_2019-07-31_17-03-30bnvjxa9c/error_2019-07-31_17-04-29.txt, [3 CPUs, 0 GPUs], [pid=99333], 30 s, 1 iter, 4100 ts, 0.0019 rew\n",
      " - PPO_GAIAPredictorsContinuousV9_8_clip_param=0.9,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tERROR, 1 failures: logs/clip_exps_aric/clip_exps/PPO_GAIAPredictorsContinuousV9_8_clip_param=0.9,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05_2019-07-31_17-04-28uxu2wixi/error_2019-07-31_17-05-48.txt, [3 CPUs, 0 GPUs], [pid=99336], 49 s, 1 iter, 4100 ts, 0.00381 rew\n",
      "PENDING trials:\n",
      " - PPO_GAIAPredictorsContinuousV9_10_clip_param=1.1,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tPENDING\n",
      "RUNNING trials:\n",
      " - PPO_GAIAPredictorsContinuousV9_9_clip_param=1.0,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tRUNNING\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-31 17:05:48,264\tWARNING util.py:64 -- The `start_trial` operation took 0.15347862243652344 seconds to complete, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=99332)\u001b[0m 2019-07-31 17:05:50,346\tINFO multi_gpu_impl.py:146 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(pid=99332)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99332)\u001b[0m { 'inputs': [ np.ndarray((4100, 2), dtype=float32, min=-3.655, max=4.069, mean=-0.018),\n",
      "\u001b[2m\u001b[36m(pid=99332)\u001b[0m               np.ndarray((4100,), dtype=float32, min=-0.048, max=0.029, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=99332)\u001b[0m               np.ndarray((4100, 5), dtype=float32, min=-14.063, max=11.169, mean=0.143),\n",
      "\u001b[2m\u001b[36m(pid=99332)\u001b[0m               np.ndarray((4100, 2), dtype=float32, min=-3.655, max=4.069, mean=-0.017),\n",
      "\u001b[2m\u001b[36m(pid=99332)\u001b[0m               np.ndarray((4100,), dtype=float32, min=-10.408, max=8.573, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=99332)\u001b[0m               np.ndarray((4100, 4), dtype=float32, min=-0.016, max=0.015, mean=-0.002),\n",
      "\u001b[2m\u001b[36m(pid=99332)\u001b[0m               np.ndarray((4100,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=99332)\u001b[0m               np.ndarray((4100,), dtype=float32, min=0.0, max=0.0, mean=0.0)],\n",
      "\u001b[2m\u001b[36m(pid=99332)\u001b[0m   'placeholders': [ <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99332)\u001b[0m                     <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99332)\u001b[0m                     <tf.Tensor 'default_policy/observation:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99332)\u001b[0m                     <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99332)\u001b[0m                     <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99332)\u001b[0m                     <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 4) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99332)\u001b[0m                     <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=99332)\u001b[0m                     <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>],\n",
      "\u001b[2m\u001b[36m(pid=99332)\u001b[0m   'state_inputs': []}\n",
      "\u001b[2m\u001b[36m(pid=99332)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=99332)\u001b[0m 2019-07-31 17:05:50,346\tINFO multi_gpu_impl.py:191 -- Divided 4100 rollout sequences, each of length 1, among 1 devices.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-31 17:05:53,736\tERROR trial_runner.py:487 -- Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/Nicholas/.venv/lib/python3.6/site-packages/ray/tune/trial_runner.py\", line 468, in _process_trial\n",
      "    result, terminate=(decision == TrialScheduler.STOP))\n",
      "  File \"/home/Nicholas/.venv/lib/python3.6/site-packages/ray/tune/trial.py\", line 533, in update_last_result\n",
      "    self.result_logger.on_result(self.last_result)\n",
      "  File \"/home/Nicholas/.venv/lib/python3.6/site-packages/ray/tune/logger.py\", line 239, in on_result\n",
      "    _logger.on_result(result)\n",
      "  File \"/home/Nicholas/.venv/lib/python3.6/site-packages/trading_gym/ray/logger.py\", line 69, in on_result\n",
      "    for fold in result['trading-gym']:\n",
      "KeyError: 'trading-gym'\n",
      "2019-07-31 17:05:53,740\tINFO ray_trial_executor.py:187 -- Destroying actor for trial PPO_GAIAPredictorsContinuousV9_9_clip_param=1.0,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=101266)\u001b[0m 2019-07-31 17:05:53,613\tWARNING ppo.py:151 -- FYI: By default, the value function will not share layers with the policy model ('vf_share_layers': False).\n",
      "\u001b[2m\u001b[36m(pid=99332)\u001b[0m 2019-07-31 17:05:53,722\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-31 17:05:54,352\tWARNING util.py:64 -- The `experiment_checkpoint` operation took 0.6098992824554443 seconds to complete, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 39.7/67.5 GB\n",
      "Result logdir: logs/clip_exps_aric/clip_exps\n",
      "Number of trials: 11 ({'ERROR': 10, 'RUNNING': 1})\n",
      "ERROR trials:\n",
      " - PPO_GAIAPredictorsContinuousV9_0_clip_param=0.1,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tERROR, 1 failures: logs/clip_exps_aric/clip_exps/PPO_GAIAPredictorsContinuousV9_0_clip_param=0.1,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05_2019-07-31_16-59-24r06fnmw4/error_2019-07-31_17-00-59.txt, [3 CPUs, 0 GPUs], [pid=89162], 55 s, 1 iter, 4100 ts, 0.00508 rew\n",
      " - PPO_GAIAPredictorsContinuousV9_1_clip_param=0.2,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tERROR, 1 failures: logs/clip_exps_aric/clip_exps/PPO_GAIAPredictorsContinuousV9_1_clip_param=0.2,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05_2019-07-31_16-59-259aw8hwth/error_2019-07-31_17-00-56.txt, [3 CPUs, 0 GPUs], [pid=89159], 49 s, 1 iter, 4100 ts, 0.00465 rew\n",
      " - PPO_GAIAPredictorsContinuousV9_2_clip_param=0.3,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tERROR, 1 failures: logs/clip_exps_aric/clip_exps/PPO_GAIAPredictorsContinuousV9_2_clip_param=0.3,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05_2019-07-31_17-00-56qkigs8s9/error_2019-07-31_17-02-11.txt, [3 CPUs, 0 GPUs], [pid=89156], 45 s, 1 iter, 4100 ts, 0.00346 rew\n",
      " - PPO_GAIAPredictorsContinuousV9_3_clip_param=0.4,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tERROR, 1 failures: logs/clip_exps_aric/clip_exps/PPO_GAIAPredictorsContinuousV9_3_clip_param=0.4,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05_2019-07-31_17-00-59cwpbby9k/error_2019-07-31_17-02-15.txt, [3 CPUs, 0 GPUs], [pid=89160], 42 s, 1 iter, 4100 ts, 0.00395 rew\n",
      " - PPO_GAIAPredictorsContinuousV9_4_clip_param=0.5,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tERROR, 1 failures: logs/clip_exps_aric/clip_exps/PPO_GAIAPredictorsContinuousV9_4_clip_param=0.5,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05_2019-07-31_17-02-11yrg8eih7/error_2019-07-31_17-03-30.txt, [3 CPUs, 0 GPUs], [pid=98159], 45 s, 1 iter, 4100 ts, 0.00312 rew\n",
      " - PPO_GAIAPredictorsContinuousV9_5_clip_param=0.6,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tERROR, 1 failures: logs/clip_exps_aric/clip_exps/PPO_GAIAPredictorsContinuousV9_5_clip_param=0.6,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05_2019-07-31_17-02-15dfj8taa2/error_2019-07-31_17-03-29.txt, [3 CPUs, 0 GPUs], [pid=98157], 37 s, 1 iter, 4100 ts, 0.0025 rew\n",
      " - PPO_GAIAPredictorsContinuousV9_6_clip_param=0.7,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tERROR, 1 failures: logs/clip_exps_aric/clip_exps/PPO_GAIAPredictorsContinuousV9_6_clip_param=0.7,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05_2019-07-31_17-03-2999481rgf/error_2019-07-31_17-04-28.txt, [3 CPUs, 0 GPUs], [pid=99342], 31 s, 1 iter, 4100 ts, 0.0051 rew\n",
      " - PPO_GAIAPredictorsContinuousV9_7_clip_param=0.8,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tERROR, 1 failures: logs/clip_exps_aric/clip_exps/PPO_GAIAPredictorsContinuousV9_7_clip_param=0.8,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05_2019-07-31_17-03-30bnvjxa9c/error_2019-07-31_17-04-29.txt, [3 CPUs, 0 GPUs], [pid=99333], 30 s, 1 iter, 4100 ts, 0.0019 rew\n",
      " - PPO_GAIAPredictorsContinuousV9_8_clip_param=0.9,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tERROR, 1 failures: logs/clip_exps_aric/clip_exps/PPO_GAIAPredictorsContinuousV9_8_clip_param=0.9,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05_2019-07-31_17-04-28uxu2wixi/error_2019-07-31_17-05-48.txt, [3 CPUs, 0 GPUs], [pid=99336], 49 s, 1 iter, 4100 ts, 0.00381 rew\n",
      " - PPO_GAIAPredictorsContinuousV9_9_clip_param=1.0,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tERROR, 1 failures: logs/clip_exps_aric/clip_exps/PPO_GAIAPredictorsContinuousV9_9_clip_param=1.0,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05_2019-07-31_17-04-290duodk0k/error_2019-07-31_17-05-53.txt, [3 CPUs, 0 GPUs], [pid=99332], 55 s, 1 iter, 4100 ts, 0.00413 rew\n",
      "RUNNING trials:\n",
      " - PPO_GAIAPredictorsContinuousV9_10_clip_param=1.1,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tRUNNING\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=101266)\u001b[0m 2019-07-31 17:05:58,099\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 0 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=101266)\u001b[0m 2019-07-31 17:05:58.101037: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=101266)\u001b[0m 2019-07-31 17:05:58,717\tINFO dynamic_tf_policy.py:313 -- Initializing loss function with dummy input:\n",
      "\u001b[2m\u001b[36m(pid=101266)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=101266)\u001b[0m { 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=101266)\u001b[0m   'actions': <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=101266)\u001b[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=101266)\u001b[0m   'behaviour_logits': <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 4) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=101266)\u001b[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,\n",
      "\u001b[2m\u001b[36m(pid=101266)\u001b[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=101266)\u001b[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=101266)\u001b[0m   'prev_actions': <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=101266)\u001b[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=101266)\u001b[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=101266)\u001b[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=101266)\u001b[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}\n",
      "\u001b[2m\u001b[36m(pid=101266)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=101266)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=101266)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=101266)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=101266)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=101266)\u001b[0m 2019-07-31 17:06:01,424\tINFO rollout_worker.py:719 -- Built policy map: {'default_policy': <ray.rllib.policy.tf_policy_template.PPOTFPolicy object at 0x7fd9664895f8>}\n",
      "\u001b[2m\u001b[36m(pid=101266)\u001b[0m 2019-07-31 17:06:03,240\tINFO rollout_worker.py:720 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7fd9664892b0>}\n",
      "\u001b[2m\u001b[36m(pid=101266)\u001b[0m 2019-07-31 17:06:03,240\tINFO rollout_worker.py:333 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7fd966489160>}\n",
      "\u001b[2m\u001b[36m(pid=101266)\u001b[0m 2019-07-31 17:06:03,389\tINFO multi_gpu_optimizer.py:79 -- LocalMultiGPUOptimizer devices ['/cpu:0']\n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m 2019-07-31 17:06:16,746\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 1 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m 2019-07-31 17:06:16.786745: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m 2019-07-31 17:06:17,664\tINFO dynamic_tf_policy.py:313 -- Initializing loss function with dummy input:\n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m { 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m   'actions': <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m   'behaviour_logits': <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 4) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,\n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m   'prev_actions': <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}\n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=101264)\u001b[0m 2019-07-31 17:06:21,874\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 2 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=101264)\u001b[0m 2019-07-31 17:06:22.002966: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m 2019-07-31 17:06:22,287\tINFO rollout_worker.py:428 -- Generating sample batch of size 200\n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m 2019-07-31 17:06:22,324\tINFO sampler.py:308 -- Raw obs from env: { 0: { 'agent0': np.ndarray((5,), dtype=float64, min=0.0, max=1.0, mean=0.2)}}\n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m 2019-07-31 17:06:22,326\tINFO sampler.py:309 -- Info return from env: {0: {'agent0': None}}\n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m 2019-07-31 17:06:22,326\tINFO sampler.py:407 -- Preprocessed obs: np.ndarray((5,), dtype=float64, min=0.0, max=1.0, mean=0.2)\n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m 2019-07-31 17:06:22,327\tINFO sampler.py:411 -- Filtered obs: np.ndarray((5,), dtype=float64, min=0.0, max=1.0, mean=0.2)\n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m 2019-07-31 17:06:22,330\tINFO sampler.py:525 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m                                   'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m                                   'obs': np.ndarray((5,), dtype=float64, min=0.0, max=1.0, mean=0.2),\n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m 2019-07-31 17:06:22,330\tINFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m 2019-07-31 17:06:22,407\tINFO sampler.py:552 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m { 'default_policy': ( np.ndarray((1, 2), dtype=float32, min=-1.564, max=-0.9, mean=-1.232),\n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m                       [],\n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m                       { 'action_prob': np.ndarray((1,), dtype=float32, min=0.031, max=0.031, mean=0.031),\n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m                         'behaviour_logits': np.ndarray((1, 4), dtype=float32, min=-0.006, max=0.002, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=0.0, max=0.0, mean=0.0)})}\n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m 2019-07-31 17:06:22,872\tINFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m { 'agent0': { 'data': { 'action_prob': np.ndarray((41,), dtype=float32, min=0.005, max=0.157, mean=0.089),\n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m                         'actions': np.ndarray((41, 2), dtype=float32, min=-1.666, max=2.448, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m                         'advantages': np.ndarray((41,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m                         'agent_index': np.ndarray((41,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m                         'behaviour_logits': np.ndarray((41, 4), dtype=float32, min=-0.006, max=0.002, mean=-0.001),\n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m                         'dones': np.ndarray((41,), dtype=bool, min=0.0, max=1.0, mean=0.024),\n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m                         'eps_id': np.ndarray((41,), dtype=int64, min=1952626356.0, max=1952626356.0, mean=1952626356.0),\n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m                         'infos': np.ndarray((41,), dtype=object, head={'time': Timestamp('2015-12-24 00:00:00'), 'nlv': 99.99900004545226, 'nr_contracts': np.ndarray((3,), dtype=float64, min=0.0, max=90.909, mean=30.308), 'bid_prices': np.ndarray((3,), dtype=float64, min=1.0, max=6120.042, mean=2228.79), 'ask_prices': np.ndarray((3,), dtype=float64, min=1.0, max=6120.654, mean=2229.013), 'cost_of_spread': np.ndarray((3,), dtype=float64, min=0.0, max=0.0, mean=0.0), 'cost_of_commissions': np.ndarray((3,), dtype=float64, min=0.0, max=0.0, mean=0.0), 'profit_on_idle_cash': 0.0, 'margin': 0, 'weights_target': np.ndarray((3,), dtype=float64, min=0.0, max=0.909, mean=0.333), 'weights_drift': np.ndarray((3,), dtype=float64, min=0.0, max=1.0, mean=0.333), 'weights_diff': np.ndarray((3,), dtype=float64, min=-0.091, max=0.091, mean=-0.0), 'positions_value_diff': np.ndarray((3,), dtype=float64, min=-9.091, max=9.091, mean=-0.0), 'acq_prices': np.ndarray((3,), dtype=float64, min=1.0, max=6120.348, mean=2228.911), 'nr_contracts_diff': np.ndarray((3,), dtype=float64, min=-9.091, max=0.016, mean=-3.025), 'nlv_pre': 100.0}),\n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m                         'new_obs': np.ndarray((41, 5), dtype=float32, min=0.0, max=1.0, mean=0.352),\n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m                         'obs': np.ndarray((41, 5), dtype=float32, min=0.0, max=1.0, mean=0.348),\n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m                         'prev_actions': np.ndarray((41, 2), dtype=float32, min=-1.666, max=2.448, mean=0.033),\n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m                         'prev_rewards': np.ndarray((41,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m                         'rewards': np.ndarray((41,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m                         't': np.ndarray((41,), dtype=int64, min=0.0, max=40.0, mean=20.0),\n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m                         'unroll_id': np.ndarray((41,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m                         'value_targets': np.ndarray((41,), dtype=float64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m                         'vf_preds': np.ndarray((41,), dtype=float32, min=0.0, max=0.0, mean=0.0)},\n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m               'type': 'SampleBatch'}}\n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=101264)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=101264)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=101264)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=101264)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m 2019-07-31 17:06:25,637\tINFO rollout_worker.py:462 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m { 'data': { 'action_prob': np.ndarray((205,), dtype=float32, min=0.0, max=0.159, mean=0.082),\n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m             'actions': np.ndarray((205, 2), dtype=float32, min=-2.94, max=2.518, mean=0.054),\n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m             'advantages': np.ndarray((205,), dtype=float32, min=-0.019, max=0.012, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m             'agent_index': np.ndarray((205,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m             'behaviour_logits': np.ndarray((205, 4), dtype=float32, min=-0.014, max=0.016, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m             'dones': np.ndarray((205,), dtype=bool, min=0.0, max=1.0, mean=0.024),\n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m             'eps_id': np.ndarray((205,), dtype=int64, min=17572891.0, max=1952626356.0, mean=1248668142.6),\n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m             'infos': np.ndarray((205,), dtype=object, head={'time': Timestamp('2015-12-24 00:00:00'), 'nlv': 99.99900004545226, 'nr_contracts': np.ndarray((3,), dtype=float64, min=0.0, max=90.909, mean=30.308), 'bid_prices': np.ndarray((3,), dtype=float64, min=1.0, max=6120.042, mean=2228.79), 'ask_prices': np.ndarray((3,), dtype=float64, min=1.0, max=6120.654, mean=2229.013), 'cost_of_spread': np.ndarray((3,), dtype=float64, min=0.0, max=0.0, mean=0.0), 'cost_of_commissions': np.ndarray((3,), dtype=float64, min=0.0, max=0.0, mean=0.0), 'profit_on_idle_cash': 0.0, 'margin': 0, 'weights_target': np.ndarray((3,), dtype=float64, min=0.0, max=0.909, mean=0.333), 'weights_drift': np.ndarray((3,), dtype=float64, min=0.0, max=1.0, mean=0.333), 'weights_diff': np.ndarray((3,), dtype=float64, min=-0.091, max=0.091, mean=-0.0), 'positions_value_diff': np.ndarray((3,), dtype=float64, min=-9.091, max=9.091, mean=-0.0), 'acq_prices': np.ndarray((3,), dtype=float64, min=1.0, max=6120.348, mean=2228.911), 'nr_contracts_diff': np.ndarray((3,), dtype=float64, min=-9.091, max=0.016, mean=-3.025), 'nlv_pre': 100.0}),\n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m             'new_obs': np.ndarray((205, 5), dtype=float32, min=-5.26, max=5.436, mean=0.16),\n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m             'obs': np.ndarray((205, 5), dtype=float32, min=-5.26, max=5.436, mean=0.162),\n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m             'prev_actions': np.ndarray((205, 2), dtype=float32, min=-2.94, max=2.518, mean=0.064),\n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m             'prev_rewards': np.ndarray((205,), dtype=float32, min=-0.012, max=0.006, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m             'rewards': np.ndarray((205,), dtype=float32, min=-0.012, max=0.006, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m             't': np.ndarray((205,), dtype=int64, min=0.0, max=40.0, mean=20.0),\n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m             'unroll_id': np.ndarray((205,), dtype=int64, min=0.0, max=4.0, mean=2.0),\n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m             'value_targets': np.ndarray((205,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m             'vf_preds': np.ndarray((205,), dtype=float32, min=0.0, max=0.0, mean=0.0)},\n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m   'type': 'SampleBatch'}\n",
      "\u001b[2m\u001b[36m(pid=101260)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=101266)\u001b[0m 2019-07-31 17:06:52,804\tINFO multi_gpu_impl.py:146 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(pid=101266)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=101266)\u001b[0m { 'inputs': [ np.ndarray((4100, 2), dtype=float32, min=-4.101, max=3.995, mean=-0.016),\n",
      "\u001b[2m\u001b[36m(pid=101266)\u001b[0m               np.ndarray((4100,), dtype=float32, min=-0.018, max=0.017, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=101266)\u001b[0m               np.ndarray((4100, 5), dtype=float32, min=-14.063, max=9.882, mean=0.144),\n",
      "\u001b[2m\u001b[36m(pid=101266)\u001b[0m               np.ndarray((4100, 2), dtype=float32, min=-4.101, max=3.995, mean=-0.016),\n",
      "\u001b[2m\u001b[36m(pid=101266)\u001b[0m               np.ndarray((4100,), dtype=float32, min=-6.711, max=7.83, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=101266)\u001b[0m               np.ndarray((4100, 4), dtype=float32, min=-0.019, max=0.016, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=101266)\u001b[0m               np.ndarray((4100,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=101266)\u001b[0m               np.ndarray((4100,), dtype=float32, min=0.0, max=0.0, mean=0.0)],\n",
      "\u001b[2m\u001b[36m(pid=101266)\u001b[0m   'placeholders': [ <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=101266)\u001b[0m                     <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=101266)\u001b[0m                     <tf.Tensor 'default_policy/observation:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=101266)\u001b[0m                     <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=101266)\u001b[0m                     <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=101266)\u001b[0m                     <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 4) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=101266)\u001b[0m                     <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=101266)\u001b[0m                     <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>],\n",
      "\u001b[2m\u001b[36m(pid=101266)\u001b[0m   'state_inputs': []}\n",
      "\u001b[2m\u001b[36m(pid=101266)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=101266)\u001b[0m 2019-07-31 17:06:52,804\tINFO multi_gpu_impl.py:191 -- Divided 4100 rollout sequences, each of length 1, among 1 devices.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-31 17:06:56,649\tERROR trial_runner.py:487 -- Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/Nicholas/.venv/lib/python3.6/site-packages/ray/tune/trial_runner.py\", line 468, in _process_trial\n",
      "    result, terminate=(decision == TrialScheduler.STOP))\n",
      "  File \"/home/Nicholas/.venv/lib/python3.6/site-packages/ray/tune/trial.py\", line 533, in update_last_result\n",
      "    self.result_logger.on_result(self.last_result)\n",
      "  File \"/home/Nicholas/.venv/lib/python3.6/site-packages/ray/tune/logger.py\", line 239, in on_result\n",
      "    _logger.on_result(result)\n",
      "  File \"/home/Nicholas/.venv/lib/python3.6/site-packages/trading_gym/ray/logger.py\", line 69, in on_result\n",
      "    for fold in result['trading-gym']:\n",
      "KeyError: 'trading-gym'\n",
      "2019-07-31 17:06:56,654\tINFO ray_trial_executor.py:187 -- Destroying actor for trial PPO_GAIAPredictorsContinuousV9_10_clip_param=1.1,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=101266)\u001b[0m 2019-07-31 17:06:56,644\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-31 17:06:57,183\tWARNING util.py:64 -- The `experiment_checkpoint` operation took 0.5248825550079346 seconds to complete, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 39.2/67.5 GB\n",
      "Result logdir: logs/clip_exps_aric/clip_exps\n",
      "Number of trials: 11 ({'ERROR': 11})\n",
      "ERROR trials:\n",
      " - PPO_GAIAPredictorsContinuousV9_0_clip_param=0.1,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tERROR, 1 failures: logs/clip_exps_aric/clip_exps/PPO_GAIAPredictorsContinuousV9_0_clip_param=0.1,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05_2019-07-31_16-59-24r06fnmw4/error_2019-07-31_17-00-59.txt, [3 CPUs, 0 GPUs], [pid=89162], 55 s, 1 iter, 4100 ts, 0.00508 rew\n",
      " - PPO_GAIAPredictorsContinuousV9_1_clip_param=0.2,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tERROR, 1 failures: logs/clip_exps_aric/clip_exps/PPO_GAIAPredictorsContinuousV9_1_clip_param=0.2,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05_2019-07-31_16-59-259aw8hwth/error_2019-07-31_17-00-56.txt, [3 CPUs, 0 GPUs], [pid=89159], 49 s, 1 iter, 4100 ts, 0.00465 rew\n",
      " - PPO_GAIAPredictorsContinuousV9_2_clip_param=0.3,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tERROR, 1 failures: logs/clip_exps_aric/clip_exps/PPO_GAIAPredictorsContinuousV9_2_clip_param=0.3,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05_2019-07-31_17-00-56qkigs8s9/error_2019-07-31_17-02-11.txt, [3 CPUs, 0 GPUs], [pid=89156], 45 s, 1 iter, 4100 ts, 0.00346 rew\n",
      " - PPO_GAIAPredictorsContinuousV9_3_clip_param=0.4,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tERROR, 1 failures: logs/clip_exps_aric/clip_exps/PPO_GAIAPredictorsContinuousV9_3_clip_param=0.4,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05_2019-07-31_17-00-59cwpbby9k/error_2019-07-31_17-02-15.txt, [3 CPUs, 0 GPUs], [pid=89160], 42 s, 1 iter, 4100 ts, 0.00395 rew\n",
      " - PPO_GAIAPredictorsContinuousV9_4_clip_param=0.5,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tERROR, 1 failures: logs/clip_exps_aric/clip_exps/PPO_GAIAPredictorsContinuousV9_4_clip_param=0.5,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05_2019-07-31_17-02-11yrg8eih7/error_2019-07-31_17-03-30.txt, [3 CPUs, 0 GPUs], [pid=98159], 45 s, 1 iter, 4100 ts, 0.00312 rew\n",
      " - PPO_GAIAPredictorsContinuousV9_5_clip_param=0.6,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tERROR, 1 failures: logs/clip_exps_aric/clip_exps/PPO_GAIAPredictorsContinuousV9_5_clip_param=0.6,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05_2019-07-31_17-02-15dfj8taa2/error_2019-07-31_17-03-29.txt, [3 CPUs, 0 GPUs], [pid=98157], 37 s, 1 iter, 4100 ts, 0.0025 rew\n",
      " - PPO_GAIAPredictorsContinuousV9_6_clip_param=0.7,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tERROR, 1 failures: logs/clip_exps_aric/clip_exps/PPO_GAIAPredictorsContinuousV9_6_clip_param=0.7,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05_2019-07-31_17-03-2999481rgf/error_2019-07-31_17-04-28.txt, [3 CPUs, 0 GPUs], [pid=99342], 31 s, 1 iter, 4100 ts, 0.0051 rew\n",
      " - PPO_GAIAPredictorsContinuousV9_7_clip_param=0.8,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tERROR, 1 failures: logs/clip_exps_aric/clip_exps/PPO_GAIAPredictorsContinuousV9_7_clip_param=0.8,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05_2019-07-31_17-03-30bnvjxa9c/error_2019-07-31_17-04-29.txt, [3 CPUs, 0 GPUs], [pid=99333], 30 s, 1 iter, 4100 ts, 0.0019 rew\n",
      " - PPO_GAIAPredictorsContinuousV9_8_clip_param=0.9,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tERROR, 1 failures: logs/clip_exps_aric/clip_exps/PPO_GAIAPredictorsContinuousV9_8_clip_param=0.9,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05_2019-07-31_17-04-28uxu2wixi/error_2019-07-31_17-05-48.txt, [3 CPUs, 0 GPUs], [pid=99336], 49 s, 1 iter, 4100 ts, 0.00381 rew\n",
      " - PPO_GAIAPredictorsContinuousV9_9_clip_param=1.0,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tERROR, 1 failures: logs/clip_exps_aric/clip_exps/PPO_GAIAPredictorsContinuousV9_9_clip_param=1.0,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05_2019-07-31_17-04-290duodk0k/error_2019-07-31_17-05-53.txt, [3 CPUs, 0 GPUs], [pid=99332], 55 s, 1 iter, 4100 ts, 0.00413 rew\n",
      " - PPO_GAIAPredictorsContinuousV9_10_clip_param=1.1,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tERROR, 1 failures: logs/clip_exps_aric/clip_exps/PPO_GAIAPredictorsContinuousV9_10_clip_param=1.1,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-0_2019-07-31_17-05-4879jz66js/error_2019-07-31_17-06-56.txt, [3 CPUs, 0 GPUs], [pid=101266], 41 s, 1 iter, 4100 ts, 0.00208 rew\n",
      "\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 39.2/67.5 GB\n",
      "Result logdir: logs/clip_exps_aric/clip_exps\n",
      "Number of trials: 11 ({'ERROR': 11})\n",
      "ERROR trials:\n",
      " - PPO_GAIAPredictorsContinuousV9_0_clip_param=0.1,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tERROR, 1 failures: logs/clip_exps_aric/clip_exps/PPO_GAIAPredictorsContinuousV9_0_clip_param=0.1,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05_2019-07-31_16-59-24r06fnmw4/error_2019-07-31_17-00-59.txt, [3 CPUs, 0 GPUs], [pid=89162], 55 s, 1 iter, 4100 ts, 0.00508 rew\n",
      " - PPO_GAIAPredictorsContinuousV9_1_clip_param=0.2,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tERROR, 1 failures: logs/clip_exps_aric/clip_exps/PPO_GAIAPredictorsContinuousV9_1_clip_param=0.2,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05_2019-07-31_16-59-259aw8hwth/error_2019-07-31_17-00-56.txt, [3 CPUs, 0 GPUs], [pid=89159], 49 s, 1 iter, 4100 ts, 0.00465 rew\n",
      " - PPO_GAIAPredictorsContinuousV9_2_clip_param=0.3,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tERROR, 1 failures: logs/clip_exps_aric/clip_exps/PPO_GAIAPredictorsContinuousV9_2_clip_param=0.3,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05_2019-07-31_17-00-56qkigs8s9/error_2019-07-31_17-02-11.txt, [3 CPUs, 0 GPUs], [pid=89156], 45 s, 1 iter, 4100 ts, 0.00346 rew\n",
      " - PPO_GAIAPredictorsContinuousV9_3_clip_param=0.4,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tERROR, 1 failures: logs/clip_exps_aric/clip_exps/PPO_GAIAPredictorsContinuousV9_3_clip_param=0.4,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05_2019-07-31_17-00-59cwpbby9k/error_2019-07-31_17-02-15.txt, [3 CPUs, 0 GPUs], [pid=89160], 42 s, 1 iter, 4100 ts, 0.00395 rew\n",
      " - PPO_GAIAPredictorsContinuousV9_4_clip_param=0.5,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tERROR, 1 failures: logs/clip_exps_aric/clip_exps/PPO_GAIAPredictorsContinuousV9_4_clip_param=0.5,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05_2019-07-31_17-02-11yrg8eih7/error_2019-07-31_17-03-30.txt, [3 CPUs, 0 GPUs], [pid=98159], 45 s, 1 iter, 4100 ts, 0.00312 rew\n",
      " - PPO_GAIAPredictorsContinuousV9_5_clip_param=0.6,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tERROR, 1 failures: logs/clip_exps_aric/clip_exps/PPO_GAIAPredictorsContinuousV9_5_clip_param=0.6,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05_2019-07-31_17-02-15dfj8taa2/error_2019-07-31_17-03-29.txt, [3 CPUs, 0 GPUs], [pid=98157], 37 s, 1 iter, 4100 ts, 0.0025 rew\n",
      " - PPO_GAIAPredictorsContinuousV9_6_clip_param=0.7,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tERROR, 1 failures: logs/clip_exps_aric/clip_exps/PPO_GAIAPredictorsContinuousV9_6_clip_param=0.7,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05_2019-07-31_17-03-2999481rgf/error_2019-07-31_17-04-28.txt, [3 CPUs, 0 GPUs], [pid=99342], 31 s, 1 iter, 4100 ts, 0.0051 rew\n",
      " - PPO_GAIAPredictorsContinuousV9_7_clip_param=0.8,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tERROR, 1 failures: logs/clip_exps_aric/clip_exps/PPO_GAIAPredictorsContinuousV9_7_clip_param=0.8,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05_2019-07-31_17-03-30bnvjxa9c/error_2019-07-31_17-04-29.txt, [3 CPUs, 0 GPUs], [pid=99333], 30 s, 1 iter, 4100 ts, 0.0019 rew\n",
      " - PPO_GAIAPredictorsContinuousV9_8_clip_param=0.9,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tERROR, 1 failures: logs/clip_exps_aric/clip_exps/PPO_GAIAPredictorsContinuousV9_8_clip_param=0.9,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05_2019-07-31_17-04-28uxu2wixi/error_2019-07-31_17-05-48.txt, [3 CPUs, 0 GPUs], [pid=99336], 49 s, 1 iter, 4100 ts, 0.00381 rew\n",
      " - PPO_GAIAPredictorsContinuousV9_9_clip_param=1.0,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tERROR, 1 failures: logs/clip_exps_aric/clip_exps/PPO_GAIAPredictorsContinuousV9_9_clip_param=1.0,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05_2019-07-31_17-04-290duodk0k/error_2019-07-31_17-05-53.txt, [3 CPUs, 0 GPUs], [pid=99332], 55 s, 1 iter, 4100 ts, 0.00413 rew\n",
      " - PPO_GAIAPredictorsContinuousV9_10_clip_param=1.1,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000:\tERROR, 1 failures: logs/clip_exps_aric/clip_exps/PPO_GAIAPredictorsContinuousV9_10_clip_param=1.1,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-0_2019-07-31_17-05-4879jz66js/error_2019-07-31_17-06-56.txt, [3 CPUs, 0 GPUs], [pid=101266], 41 s, 1 iter, 4100 ts, 0.00208 rew\n",
      "\n"
     ]
    },
    {
     "ename": "TuneError",
     "evalue": "('Trials did not complete', [PPO_GAIAPredictorsContinuousV9_0_clip_param=0.1,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000, PPO_GAIAPredictorsContinuousV9_1_clip_param=0.2,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000, PPO_GAIAPredictorsContinuousV9_2_clip_param=0.3,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000, PPO_GAIAPredictorsContinuousV9_3_clip_param=0.4,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000, PPO_GAIAPredictorsContinuousV9_4_clip_param=0.5,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000, PPO_GAIAPredictorsContinuousV9_5_clip_param=0.6,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000, PPO_GAIAPredictorsContinuousV9_6_clip_param=0.7,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000, PPO_GAIAPredictorsContinuousV9_7_clip_param=0.8,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000, PPO_GAIAPredictorsContinuousV9_8_clip_param=0.9,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000, PPO_GAIAPredictorsContinuousV9_9_clip_param=1.0,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000, PPO_GAIAPredictorsContinuousV9_10_clip_param=1.1,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTuneError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-cec307b65e85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mreuse_actors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     resume=False,)\n\u001b[0m",
      "\u001b[0;32m~/.venv/lib/python3.6/site-packages/ray/tune/tune.py\u001b[0m in \u001b[0;36mrun_experiments\u001b[0;34m(experiments, search_alg, scheduler, with_server, server_port, verbose, resume, queue_trials, reuse_actors, trial_executor, raise_on_failed_trial)\u001b[0m\n\u001b[1;32m    331\u001b[0m             \u001b[0mreuse_actors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreuse_actors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m             \u001b[0mtrial_executor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial_executor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m             raise_on_failed_trial=raise_on_failed_trial)\n\u001b[0m\u001b[1;32m    334\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venv/lib/python3.6/site-packages/ray/tune/tune.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, stop, config, resources_per_trial, num_samples, local_dir, upload_dir, trial_name_creator, loggers, sync_function, checkpoint_freq, checkpoint_at_end, export_formats, max_failures, restore, search_alg, scheduler, with_server, server_port, verbose, resume, queue_trials, reuse_actors, trial_executor, raise_on_failed_trial, return_trials, ray_auto_init)\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merrored_trials\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mraise_on_failed_trial\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTuneError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrored_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrored_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTuneError\u001b[0m: ('Trials did not complete', [PPO_GAIAPredictorsContinuousV9_0_clip_param=0.1,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000, PPO_GAIAPredictorsContinuousV9_1_clip_param=0.2,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000, PPO_GAIAPredictorsContinuousV9_2_clip_param=0.3,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000, PPO_GAIAPredictorsContinuousV9_3_clip_param=0.4,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000, PPO_GAIAPredictorsContinuousV9_4_clip_param=0.5,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000, PPO_GAIAPredictorsContinuousV9_5_clip_param=0.6,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000, PPO_GAIAPredictorsContinuousV9_6_clip_param=0.7,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000, PPO_GAIAPredictorsContinuousV9_7_clip_param=0.8,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000, PPO_GAIAPredictorsContinuousV9_8_clip_param=0.9,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000, PPO_GAIAPredictorsContinuousV9_9_clip_param=1.0,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000, PPO_GAIAPredictorsContinuousV9_10_clip_param=1.1,entropy_coeff=1e-05,cost_of_commissions=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000])"
     ]
    }
   ],
   "source": [
    "trials = tune.run_experiments(\n",
    "    experiments=experiment,\n",
    "    search_alg=tune.suggest.BasicVariantGenerator(),\n",
    "    scheduler=tune.schedulers.FIFOScheduler(),\n",
    "    verbose=1,\n",
    "    reuse_actors=False,\n",
    "    resume=False,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
