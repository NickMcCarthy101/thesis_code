{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-08-21 13:40:17,415\tINFO node.py:498 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-08-21_13-40-17_415079_76025/logs.\n",
      "2019-08-21 13:40:17,530\tINFO services.py:409 -- Waiting for redis server at 127.0.0.1:15363 to respond...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-21 13:40:17.412197\n",
      "trading_gym 0.8.1\n",
      "ray 0.7.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-08-21 13:40:17,650\tINFO services.py:409 -- Waiting for redis server at 127.0.0.1:17137 to respond...\n",
      "2019-08-21 13:40:17,653\tINFO services.py:806 -- Starting Redis shard with 10.0 GB max memory.\n",
      "2019-08-21 13:40:17,678\tINFO node.py:512 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-08-21_13-40-17_415079_76025/logs.\n",
      "2019-08-21 13:40:17,680\tWARNING services.py:1298 -- Warning: Capping object memory store to 20.0GB. To increase this further, specify `object_store_memory` when calling ray.init() or ray start.\n",
      "2019-08-21 13:40:17,681\tINFO services.py:1446 -- Starting the Plasma object store with 20.0 GB memory using /dev/shm.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '10.0.5.4',\n",
       " 'redis_address': '10.0.5.4:15363',\n",
       " 'object_store_address': '/tmp/ray/session_2019-08-21_13-40-17_415079_76025/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2019-08-21_13-40-17_415079_76025/sockets/raylet',\n",
       " 'webui_url': None,\n",
       " 'session_dir': '/tmp/ray/session_2019-08-21_13-40-17_415079_76025'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import trading_gym\n",
    "import numpy as np\n",
    "from trading_gym.registry.gaia.v8.env import GAIAPredictorsContinuousV8\n",
    "\n",
    "from datetime import datetime\n",
    "from collections import namedtuple\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import ray\n",
    "from ray import rllib, tune\n",
    "print(datetime.now())\n",
    "print(trading_gym.__name__, trading_gym.__version__)\n",
    "print(ray.__name__, ray.__version__)\n",
    "\n",
    "from trading_gym.ray.logger import calculate_tearsheet, CustomLogger\n",
    "from copy import deepcopy\n",
    "# ray.init(num_cpus=8,ignore_reinit_error=True,object_store_memory= 10*100 )\n",
    "ray.init(ignore_reinit_error=True)\n",
    "#          object_store_memory = 50000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<trading_gym.registry.gaia.v8.env.GAIAPredictorsContinuousV8 at 0x7f9ddbf9d438>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_config = dict()\n",
    "env = GAIAPredictorsContinuousV8(env_config)\n",
    "env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# There is a 'common config' that sets ray's params\n",
    "# and then default_config, which sets the PPO config \n",
    "config = rllib.agents.ppo.DEFAULT_CONFIG.copy()\n",
    "#  The env is self.explanatory \n",
    "config['env'] = GAIAPredictorsContinuousV8\n",
    "\n",
    "# This doesn't actually do anything \n",
    "env_config['cost_of_commissions'] = 0.00005  # i.e. 0.005% of traded value in dollars (realistic)\n",
    "env_config['cost_of_spread'] = 0.0001  # i.e. bid-ask spread is 0.01% (realistic)\n",
    "\n",
    "config['env_config'] = env_config \n",
    "\n",
    "config['callbacks']['on_train_result'] = tune.function(calculate_tearsheet)\n",
    "config['num_workers'] = 6\n",
    "\n",
    "config['gamma'] = 0 # tune.grid_search([0])\n",
    "config['vf_clip_param'] = 0 # tune.grid_search([0.])\n",
    "config['vf_loss_coeff'] = 0 # tune.grid_search([0.])\n",
    "config['lambda'] = 0 # tune.grid_search([0])\n",
    "\n",
    "config['use_gae'] = False #tune.grid_search([False])\n",
    "config['vf_share_layers'] = False #tune.grid_search([False])\n",
    "\n",
    "# If you do use this, have vf_share_layers as True (loss function then combines ) \n",
    "config['use_lstm']: False\n",
    "# Whether to roll out complete epsiodes or truncate them \n",
    "config['batch_mode'] = 'complete_episodes'\n",
    "\n",
    "\n",
    "# Literature suggests having different LR for actor and critic and -3 and -2 \n",
    "config['lr'] = tune.grid_search([1e-5])\n",
    "\n",
    "# Size of batches collected from each worker (number of experiences used for one iteration of SGD)\n",
    "#  Don't think I actually want to use the following. \n",
    "# config['sample_batch_size'] = tune.grid_search([256])\n",
    "\n",
    "# Increase this to maximize the amount of info(no. of experiences(think transition tuples)) we gather before making an update to policy\n",
    "config['train_batch_size'] = tune.grid_search([4000])\n",
    "# Total SGD batch size across all devices\n",
    "config['sgd_minibatch_size'] = 128\n",
    "# Number of SGD iterations in each outer loop \n",
    "config['num_sgd_iter'] = tune.grid_search([8])\n",
    "\n",
    "\n",
    "# Coefficient of entropy regularizer (i.e how much we encourage explorsation)\n",
    "config['entropy_coeff'] = tune.grid_search([1e-5])\n",
    "\n",
    "# Initial coefficient for KL divergence \n",
    "config['kl_coeff'] = tune.grid_search([0.2])\n",
    "# Target value for the KL divergence \n",
    "config['kl_target'] = tune.grid_search([0.01])\n",
    "\n",
    "# PPO clip parameter\n",
    "config['clip_param'] = tune.grid_search([0.8])\n",
    "# config['ignore_worker_failures'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.models.model import Model\n",
    "from ray.rllib.models.misc import normc_initializer, get_activation_fn\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "\n",
    "\n",
    "class MLP(Model):\n",
    "    def _build_layers_v2(self, input_dict: dict, num_outputs: int, config: dict):\n",
    "        import tensorflow.contrib.slim as slim\n",
    "\n",
    "        with tf.name_scope(\"fc_net\"):\n",
    "            last_layer = input_dict['obs']\n",
    "            activation = get_activation_fn(config.get(\"fcnet_activation\"))\n",
    "            for i, size in enumerate(config.get(\"fcnet_hiddens\"), 1):\n",
    "                last_layer = slim.fully_connected(\n",
    "                    inputs=last_layer,\n",
    "                    num_outputs=size,\n",
    "                    weights_initializer=normc_initializer(1.0),\n",
    "                    activation_fn=activation,\n",
    "                    scope=\"fc{}\".format(i),\n",
    "                )\n",
    "                \n",
    "            output = slim.fully_connected(\n",
    "                inputs=last_layer,\n",
    "                num_outputs=num_outputs,\n",
    "                weights_initializer=normc_initializer(0.01),\n",
    "                activation_fn=None,\n",
    "                scope=\"fc_out\",\n",
    "            )\n",
    "            return output, last_layer\n",
    "\n",
    "ModelCatalog.register_custom_model(MLP.__name__, MLP)\n",
    "\n",
    "config['model']['custom_model'] = MLP.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-12 10:49:21,662\tINFO tune.py:65 -- Did not find checkpoint file in logs/tran_cost_x10/clip_0.8-tc-WalkForward-750k2007.\n",
      "2019-07-12 10:49:21,664\tINFO tune.py:233 -- Starting a new experiment.\n",
      "2019-07-12 10:49:21,675\tWARNING signature.py:108 -- The function with_updates has a **kwargs argument, which is currently not supported.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_______________________________________2007____________________________________________\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 10:49:26,369\tWARNING ppo.py:151 -- FYI: By default, the value function will not share layers with the policy model ('vf_share_layers': False).\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 10:49:27,573\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 0 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 10:49:27.574178: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 10:49:34,883\tINFO dynamic_tf_policy.py:313 -- Initializing loss function with dummy input:\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m { 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m   'actions': <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m   'behaviour_logits': <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m   'prev_actions': <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 10:49:36,318\tINFO rollout_worker.py:719 -- Built policy map: {'default_policy': <ray.rllib.policy.tf_policy_template.PPOTFPolicy object at 0x7fc36344c5c0>}\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 10:49:36,318\tINFO rollout_worker.py:720 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7fc36344c198>}\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 10:49:36,318\tINFO rollout_worker.py:333 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7fc36346ff98>}\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 10:49:36,501\tINFO multi_gpu_optimizer.py:79 -- LocalMultiGPUOptimizer devices ['/cpu:0']\n",
      "\u001b[2m\u001b[36m(pid=44243)\u001b[0m 2019-07-12 10:49:43,350\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 6 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=44243)\u001b[0m 2019-07-12 10:49:43.375393: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=44245)\u001b[0m 2019-07-12 10:49:43,429\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 3 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=44245)\u001b[0m 2019-07-12 10:49:43.456405: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=44238)\u001b[0m 2019-07-12 10:49:43,591\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 5 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=44244)\u001b[0m 2019-07-12 10:49:43,556\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 2 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=44244)\u001b[0m 2019-07-12 10:49:43.583339: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m 2019-07-12 10:49:43,562\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 1 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m 2019-07-12 10:49:43.585215: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=44238)\u001b[0m 2019-07-12 10:49:43.614451: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=44242)\u001b[0m 2019-07-12 10:49:44,084\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 4 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=44242)\u001b[0m 2019-07-12 10:49:44.110305: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m 2019-07-12 10:49:57,333\tINFO dynamic_tf_policy.py:313 -- Initializing loss function with dummy input:\n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m { 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m   'actions': <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m   'behaviour_logits': <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,\n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m   'prev_actions': <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}\n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=44245)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=44245)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=44245)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=44245)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=44243)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=44243)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=44243)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=44243)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=44242)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=44242)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=44242)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=44242)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=44238)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=44238)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=44238)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=44238)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=44244)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=44244)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=44244)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=44244)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m 2019-07-12 10:49:59,909\tINFO rollout_worker.py:428 -- Generating sample batch of size 200\n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m 2019-07-12 10:49:59,932\tINFO sampler.py:308 -- Raw obs from env: { 0: { 'agent0': np.ndarray((5,), dtype=float64, min=-1.0, max=3.353, mean=0.789)}}\n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m 2019-07-12 10:49:59,932\tINFO sampler.py:309 -- Info return from env: {0: {'agent0': None}}\n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m 2019-07-12 10:49:59,933\tINFO sampler.py:407 -- Preprocessed obs: np.ndarray((5,), dtype=float64, min=-1.0, max=3.353, mean=0.789)\n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m 2019-07-12 10:49:59,933\tINFO sampler.py:411 -- Filtered obs: np.ndarray((5,), dtype=float64, min=-1.0, max=3.353, mean=0.789)\n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m 2019-07-12 10:49:59,934\tINFO sampler.py:525 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m                                   'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m                                   'obs': np.ndarray((5,), dtype=float64, min=-1.0, max=3.353, mean=0.789),\n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m 2019-07-12 10:49:59,934\tINFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m 2019-07-12 10:50:00,039\tINFO sampler.py:552 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m { 'default_policy': ( np.ndarray((1, 2), dtype=float32, min=0.009, max=0.991, mean=0.5),\n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m                       [],\n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m                       { 'action_prob': np.ndarray((1,), dtype=float32, min=0.984, max=0.984, mean=0.984),\n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m                         'behaviour_logits': np.ndarray((1, 2), dtype=float32, min=-0.002, max=0.004, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=0.0, max=0.0, mean=0.0)})}\n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m 2019-07-12 10:50:00,208\tINFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m { 'agent0': { 'data': { 'action_prob': np.ndarray((20,), dtype=float32, min=0.984, max=1.006, mean=0.999),\n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m                         'actions': np.ndarray((20, 2), dtype=float32, min=0.009, max=0.991, mean=0.5),\n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m                         'advantages': np.ndarray((20,), dtype=float32, min=-0.005, max=0.004, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m                         'agent_index': np.ndarray((20,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m                         'behaviour_logits': np.ndarray((20, 2), dtype=float32, min=-0.004, max=0.004, mean=-0.002),\n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m                         'dones': np.ndarray((20,), dtype=bool, min=0.0, max=1.0, mean=0.05),\n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m                         'eps_id': np.ndarray((20,), dtype=int64, min=169410995.0, max=169410995.0, mean=169410995.0),\n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m                         'infos': np.ndarray((20,), dtype=object, head={'time': Timestamp('2006-02-06 00:00:00'), 'nlv': 99.85004997389528, 'nr_contracts': np.ndarray((3,), dtype=float64, min=-0.05, max=0.296, mean=0.082), 'bid_prices': np.ndarray((3,), dtype=float64, min=1.0, max=3019.673, mean=1118.259), 'ask_prices': np.ndarray((3,), dtype=float64, min=1.0, max=3022.695, mean=1119.377), 'cost_of_spread': np.ndarray((3,), dtype=float64, min=0.0, max=0.05, mean=0.017), 'cost_of_commissions': np.ndarray((3,), dtype=float64, min=0.0, max=0.05, mean=0.017), 'profit_on_idle_cash': 0.0, 'margin': 0, 'weights_target': np.ndarray((3,), dtype=float32, min=0.0, max=0.991, mean=0.333), 'weights_drift': np.ndarray((3,), dtype=float64, min=0.0, max=1.0, mean=0.333), 'weights_diff': np.ndarray((3,), dtype=float64, min=-1.0, max=0.991, mean=0.0), 'positions_value_diff': np.ndarray((3,), dtype=float64, min=-100.0, max=99.14, mean=0.0), 'acq_prices': np.ndarray((3,), dtype=float64, min=1.0, max=3022.695, mean=1119.377), 'nr_contracts_diff': np.ndarray((3,), dtype=float64, min=-100.0, max=0.296, mean=-33.234), 'nlv_pre': 100.0}),\n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m                         'new_obs': np.ndarray((20, 5), dtype=float32, min=-1.529, max=2.251, mean=0.325),\n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m                         'obs': np.ndarray((20, 5), dtype=float32, min=-1.529, max=3.353, mean=0.353),\n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m                         'prev_actions': np.ndarray((20, 2), dtype=float32, min=0.0, max=0.991, mean=0.475),\n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m                         'prev_rewards': np.ndarray((20,), dtype=float32, min=-0.004, max=0.004, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m                         'rewards': np.ndarray((20,), dtype=float32, min=-0.005, max=0.004, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m                         't': np.ndarray((20,), dtype=int64, min=0.0, max=19.0, mean=9.5),\n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m                         'unroll_id': np.ndarray((20,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m                         'value_targets': np.ndarray((20,), dtype=float64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m                         'vf_preds': np.ndarray((20,), dtype=float32, min=0.0, max=0.0, mean=0.0)},\n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m               'type': 'SampleBatch'}}\n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m 2019-07-12 10:50:01,496\tINFO rollout_worker.py:462 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m { 'data': { 'action_prob': np.ndarray((200,), dtype=float32, min=0.984, max=1.01, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m             'actions': np.ndarray((200, 2), dtype=float32, min=0.002, max=0.998, mean=0.5),\n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m             'advantages': np.ndarray((200,), dtype=float32, min=-0.016, max=0.027, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m             'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m             'behaviour_logits': np.ndarray((200, 2), dtype=float32, min=-0.009, max=0.009, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m             'dones': np.ndarray((200,), dtype=bool, min=0.0, max=1.0, mean=0.05),\n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m             'eps_id': np.ndarray((200,), dtype=int64, min=169410995.0, max=1713332704.0, mean=890472995.6),\n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m             'infos': np.ndarray((200,), dtype=object, head={'time': Timestamp('2006-02-06 00:00:00'), 'nlv': 99.85004997389528, 'nr_contracts': np.ndarray((3,), dtype=float64, min=-0.05, max=0.296, mean=0.082), 'bid_prices': np.ndarray((3,), dtype=float64, min=1.0, max=3019.673, mean=1118.259), 'ask_prices': np.ndarray((3,), dtype=float64, min=1.0, max=3022.695, mean=1119.377), 'cost_of_spread': np.ndarray((3,), dtype=float64, min=0.0, max=0.05, mean=0.017), 'cost_of_commissions': np.ndarray((3,), dtype=float64, min=0.0, max=0.05, mean=0.017), 'profit_on_idle_cash': 0.0, 'margin': 0, 'weights_target': np.ndarray((3,), dtype=float32, min=0.0, max=0.991, mean=0.333), 'weights_drift': np.ndarray((3,), dtype=float64, min=0.0, max=1.0, mean=0.333), 'weights_diff': np.ndarray((3,), dtype=float64, min=-1.0, max=0.991, mean=0.0), 'positions_value_diff': np.ndarray((3,), dtype=float64, min=-100.0, max=99.14, mean=0.0), 'acq_prices': np.ndarray((3,), dtype=float64, min=1.0, max=3022.695, mean=1119.377), 'nr_contracts_diff': np.ndarray((3,), dtype=float64, min=-100.0, max=0.296, mean=-33.234), 'nlv_pre': 100.0}),\n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m             'new_obs': np.ndarray((200, 5), dtype=float32, min=-2.537, max=3.478, mean=0.295),\n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m             'obs': np.ndarray((200, 5), dtype=float32, min=-4.742, max=3.478, mean=0.282),\n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m             'prev_actions': np.ndarray((200, 2), dtype=float32, min=0.0, max=0.998, mean=0.475),\n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m             'prev_rewards': np.ndarray((200,), dtype=float32, min=-0.016, max=0.027, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m             'rewards': np.ndarray((200,), dtype=float32, min=-0.016, max=0.027, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m             't': np.ndarray((200,), dtype=int64, min=0.0, max=19.0, mean=9.5),\n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m             'unroll_id': np.ndarray((200,), dtype=int64, min=0.0, max=9.0, mean=4.5),\n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m             'value_targets': np.ndarray((200,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m             'vf_preds': np.ndarray((200,), dtype=float32, min=0.0, max=0.0, mean=0.0)},\n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m   'type': 'SampleBatch'}\n",
      "\u001b[2m\u001b[36m(pid=44240)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 10:50:06,152\tINFO multi_gpu_impl.py:146 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m { 'inputs': [ np.ndarray((4000, 2), dtype=float32, min=0.0, max=1.0, mean=0.475),\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m               np.ndarray((4000,), dtype=float32, min=-0.033, max=0.027, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m               np.ndarray((4000, 5), dtype=float32, min=-14.063, max=12.847, mean=0.233),\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m               np.ndarray((4000, 2), dtype=float32, min=0.0, max=1.0, mean=0.5),\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m               np.ndarray((4000,), dtype=float32, min=-7.827, max=6.436, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m               np.ndarray((4000, 2), dtype=float32, min=-0.009, max=0.012, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m               np.ndarray((4000,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m               np.ndarray((4000,), dtype=float32, min=0.0, max=0.0, mean=0.0)],\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m   'placeholders': [ <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m                     <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m                     <tf.Tensor 'default_policy/observation:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m                     <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m                     <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m                     <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m                     <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m                     <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>],\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m   'state_inputs': []}\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 10:50:06,152\tINFO multi_gpu_impl.py:191 -- Divided 4000 rollout sequences, each of length 1, among 1 devices.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 10:50:08,365\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 10:50:09,667\tINFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 10:50:17,601\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 10:50:26,659\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 10:50:37,024\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 10:50:46,748\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 10:50:56,589\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 10:51:08,186\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 10:51:17,865\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 10:51:28,158\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 10:51:39,019\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 10:51:50,231\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 10:52:00,126\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 10:52:11,546\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 10:52:21,927\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 10:52:31,736\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 10:52:42,609\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 10:52:51,886\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 10:53:01,055\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 10:53:11,733\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 10:53:21,124\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 10:53:30,698\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 10:53:41,433\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 10:53:50,662\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 10:54:01,072\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 10:54:11,108\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 10:54:20,255\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 10:54:29,859\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 10:54:39,719\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 10:54:49,067\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 10:54:58,511\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 10:55:07,799\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 10:55:17,864\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 10:55:27,140\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 10:55:36,602\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 10:55:46,531\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 10:55:55,409\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 10:56:04,684\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 10:56:14,632\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 10:56:22,824\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 10:56:31,926\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 10:56:41,190\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 10:56:50,408\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 10:56:59,184\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 10:57:08,326\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 10:57:18,479\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 10:57:27,815\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 10:57:37,555\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 10:57:46,832\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 10:57:56,377\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 10:58:05,628\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 10:58:16,381\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 10:58:26,384\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 10:58:36,424\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 10:58:46,096\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 10:58:55,413\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 10:59:04,818\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 10:59:14,678\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 10:59:25,531\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 10:59:35,783\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 10:59:46,713\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 10:59:58,279\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:00:08,094\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:00:18,551\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:00:28,290\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:00:38,011\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:00:47,429\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:00:58,385\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:01:07,688\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:01:17,147\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:01:28,161\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:01:37,394\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:01:47,167\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:01:57,807\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:02:07,039\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:02:16,915\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:02:28,363\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:02:38,810\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:02:49,511\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:03:02,203\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:03:14,577\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:03:24,259\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:03:35,555\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:03:45,666\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:03:56,632\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:04:07,190\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:04:17,526\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:04:28,600\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:04:39,030\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:04:49,858\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:05:00,423\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:05:10,037\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:05:20,278\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:05:31,255\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:05:42,044\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:05:52,100\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:06:04,067\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:06:15,293\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:06:25,883\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:06:37,253\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:06:47,365\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:06:57,864\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:07:09,208\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:07:18,588\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:07:28,777\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:07:40,080\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:07:49,426\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:07:59,141\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:08:10,875\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:08:21,054\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:08:31,974\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:08:45,157\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:08:55,067\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:09:04,948\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:09:15,374\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:09:24,747\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:09:34,164\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:09:44,784\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:09:54,106\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:10:03,575\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:10:14,132\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:10:23,184\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:10:32,733\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:10:43,351\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:10:52,147\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:11:02,220\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:11:12,469\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:11:21,024\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:11:30,702\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:11:40,516\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:11:50,097\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:11:59,316\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:12:08,608\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:12:19,162\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:12:28,364\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:12:38,061\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:12:48,606\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:12:57,375\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:13:07,227\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:13:17,367\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:13:26,910\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:13:36,365\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:13:46,870\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:13:55,951\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:14:05,349\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:14:15,314\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:14:25,204\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:14:34,488\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:14:44,316\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:14:54,566\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:15:03,680\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:15:13,128\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:15:23,665\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:15:32,719\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:15:41,868\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:15:52,625\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:16:01,570\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:16:11,312\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:16:21,214\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:16:30,211\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:16:39,355\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:16:48,803\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:16:59,015\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:17:08,135\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:17:17,357\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:17:28,022\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:17:36,761\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:17:46,146\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:17:56,422\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:18:05,457\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:18:14,996\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:18:24,218\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:18:34,513\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:18:43,822\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:18:53,397\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:19:03,612\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:19:13,103\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:19:22,581\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:19:33,232\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:19:42,612\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:19:52,382\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:20:03,282\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:20:12,450\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:20:22,296\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:20:33,314\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:20:42,626\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:20:52,303\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:21:02,819\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44241)\u001b[0m 2019-07-12 11:21:11,954\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-12 11:21:14,752\tINFO ray_trial_executor.py:187 -- Destroying actor for trial PPO_GAIAPredictorsContinuousV8_0_clip_param=0.8,entropy_coeff=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2019-07-12 11:21:14,802\tINFO tune.py:65 -- Did not find checkpoint file in logs/tran_cost_x10/clip_0.8-tc-WalkForward-750k2008.\n",
      "2019-07-12 11:21:14,804\tINFO tune.py:233 -- Starting a new experiment.\n",
      "2019-07-12 11:21:14,852\tWARNING signature.py:108 -- The function with_updates has a **kwargs argument, which is currently not supported.\n",
      "2019-07-12 11:21:14,931\tWARNING util.py:64 -- The `start_trial` operation took 0.11095070838928223 seconds to complete, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_______________________________________2008____________________________________________\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:21:16,935\tWARNING ppo.py:151 -- FYI: By default, the value function will not share layers with the policy model ('vf_share_layers': False).\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:21:18,030\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 0 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:21:18.031442: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:21:25,446\tINFO dynamic_tf_policy.py:313 -- Initializing loss function with dummy input:\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m { 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m   'actions': <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m   'behaviour_logits': <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m   'prev_actions': <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:21:27,111\tINFO rollout_worker.py:719 -- Built policy map: {'default_policy': <ray.rllib.policy.tf_policy_template.PPOTFPolicy object at 0x7f1587dd21d0>}\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:21:27,111\tINFO rollout_worker.py:720 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7f1587e73d68>}\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:21:27,111\tINFO rollout_worker.py:333 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7f1587e73ba8>}\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:21:27,433\tINFO multi_gpu_optimizer.py:79 -- LocalMultiGPUOptimizer devices ['/cpu:0']\n",
      "\u001b[2m\u001b[36m(pid=61582)\u001b[0m 2019-07-12 11:21:41,946\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 4 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=61582)\u001b[0m 2019-07-12 11:21:41.977167: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m 2019-07-12 11:21:42,282\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 1 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m 2019-07-12 11:21:42.315177: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=61559)\u001b[0m 2019-07-12 11:21:42,360\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 5 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=61559)\u001b[0m 2019-07-12 11:21:42.416877: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=61558)\u001b[0m 2019-07-12 11:21:42,980\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 2 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=61558)\u001b[0m 2019-07-12 11:21:43.006830: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=61583)\u001b[0m 2019-07-12 11:21:43,210\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 6 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=61583)\u001b[0m 2019-07-12 11:21:43.232844: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=61574)\u001b[0m 2019-07-12 11:21:43,609\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 3 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=61574)\u001b[0m 2019-07-12 11:21:43.632059: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=61582)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=61582)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61582)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=61582)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61558)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=61558)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61558)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=61558)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61559)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=61559)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61559)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=61559)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m 2019-07-12 11:21:58,337\tINFO dynamic_tf_policy.py:313 -- Initializing loss function with dummy input:\n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m { 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m   'actions': <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m   'behaviour_logits': <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,\n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m   'prev_actions': <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}\n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61574)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=61574)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61574)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=61574)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61583)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=61583)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61583)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=61583)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m 2019-07-12 11:22:01,617\tINFO rollout_worker.py:428 -- Generating sample batch of size 200\n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m 2019-07-12 11:22:01,652\tINFO sampler.py:308 -- Raw obs from env: { 0: { 'agent0': np.ndarray((5,), dtype=float64, min=-1.0, max=0.695, mean=0.005)}}\n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m 2019-07-12 11:22:01,653\tINFO sampler.py:309 -- Info return from env: {0: {'agent0': None}}\n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m 2019-07-12 11:22:01,653\tINFO sampler.py:407 -- Preprocessed obs: np.ndarray((5,), dtype=float64, min=-1.0, max=0.695, mean=0.005)\n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m 2019-07-12 11:22:01,654\tINFO sampler.py:411 -- Filtered obs: np.ndarray((5,), dtype=float64, min=-1.0, max=0.695, mean=0.005)\n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m 2019-07-12 11:22:01,655\tINFO sampler.py:525 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m                                   'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m                                   'obs': np.ndarray((5,), dtype=float64, min=-1.0, max=0.695, mean=0.005),\n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m 2019-07-12 11:22:01,655\tINFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m 2019-07-12 11:22:01,854\tINFO sampler.py:552 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m { 'default_policy': ( np.ndarray((1, 2), dtype=float32, min=0.337, max=0.663, mean=0.5),\n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m                       [],\n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m                       { 'action_prob': np.ndarray((1,), dtype=float32, min=1.004, max=1.004, mean=1.004),\n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m                         'behaviour_logits': np.ndarray((1, 2), dtype=float32, min=0.0, max=0.007, mean=0.003),\n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=0.0, max=0.0, mean=0.0)})}\n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m 2019-07-12 11:22:02,040\tINFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m { 'agent0': { 'data': { 'action_prob': np.ndarray((20,), dtype=float32, min=0.99, max=1.028, mean=1.001),\n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m                         'actions': np.ndarray((20, 2), dtype=float32, min=0.001, max=0.999, mean=0.5),\n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m                         'advantages': np.ndarray((20,), dtype=float32, min=-0.008, max=0.008, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m                         'agent_index': np.ndarray((20,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m                         'behaviour_logits': np.ndarray((20, 2), dtype=float32, min=-0.008, max=0.014, mean=-0.001),\n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m                         'dones': np.ndarray((20,), dtype=bool, min=0.0, max=1.0, mean=0.05),\n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m                         'eps_id': np.ndarray((20,), dtype=int64, min=1737648480.0, max=1737648480.0, mean=1737648480.0),\n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m                         'infos': np.ndarray((20,), dtype=object, head={'time': Timestamp('2005-01-11 00:00:00'), 'nlv': 99.85004997054364, 'nr_contracts': np.ndarray((3,), dtype=float64, min=-0.05, max=0.201, mean=0.055), 'bid_prices': np.ndarray((3,), dtype=float64, min=1.0, max=2724.253, mean=1018.007), 'ask_prices': np.ndarray((3,), dtype=float64, min=1.0, max=2726.979, mean=1019.025), 'cost_of_spread': np.ndarray((3,), dtype=float64, min=0.0, max=0.033, mean=0.017), 'cost_of_commissions': np.ndarray((3,), dtype=float64, min=0.0, max=0.033, mean=0.017), 'profit_on_idle_cash': 0.0, 'margin': 0, 'weights_target': np.ndarray((3,), dtype=float32, min=0.0, max=0.663, mean=0.333), 'weights_drift': np.ndarray((3,), dtype=float64, min=0.0, max=1.0, mean=0.333), 'weights_diff': np.ndarray((3,), dtype=float64, min=-1.0, max=0.663, mean=0.0), 'positions_value_diff': np.ndarray((3,), dtype=float64, min=-100.0, max=66.299, mean=0.0), 'acq_prices': np.ndarray((3,), dtype=float64, min=1.0, max=2726.979, mean=1019.025), 'nr_contracts_diff': np.ndarray((3,), dtype=float64, min=-100.0, max=0.201, mean=-33.262), 'nlv_pre': 100.0}),\n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m                         'new_obs': np.ndarray((20, 5), dtype=float32, min=-1.0, max=2.529, mean=0.14),\n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m                         'obs': np.ndarray((20, 5), dtype=float32, min=-1.0, max=2.529, mean=0.129),\n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m                         'prev_actions': np.ndarray((20, 2), dtype=float32, min=0.0, max=0.999, mean=0.475),\n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m                         'prev_rewards': np.ndarray((20,), dtype=float32, min=-0.008, max=0.008, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m                         'rewards': np.ndarray((20,), dtype=float32, min=-0.008, max=0.008, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m                         't': np.ndarray((20,), dtype=int64, min=0.0, max=19.0, mean=9.5),\n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m                         'unroll_id': np.ndarray((20,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m                         'value_targets': np.ndarray((20,), dtype=float64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m                         'vf_preds': np.ndarray((20,), dtype=float32, min=0.0, max=0.0, mean=0.0)},\n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m               'type': 'SampleBatch'}}\n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m 2019-07-12 11:22:03,605\tINFO rollout_worker.py:462 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m { 'data': { 'action_prob': np.ndarray((200,), dtype=float32, min=0.984, max=1.039, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m             'actions': np.ndarray((200, 2), dtype=float32, min=0.0, max=1.0, mean=0.5),\n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m             'advantages': np.ndarray((200,), dtype=float32, min=-0.033, max=0.034, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m             'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m             'behaviour_logits': np.ndarray((200, 2), dtype=float32, min=-0.017, max=0.016, mean=-0.001),\n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m             'dones': np.ndarray((200,), dtype=bool, min=0.0, max=1.0, mean=0.05),\n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m             'eps_id': np.ndarray((200,), dtype=int64, min=400033341.0, max=1829862882.0, mean=1237151470.0),\n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m             'infos': np.ndarray((200,), dtype=object, head={'time': Timestamp('2005-01-11 00:00:00'), 'nlv': 99.85004997054364, 'nr_contracts': np.ndarray((3,), dtype=float64, min=-0.05, max=0.201, mean=0.055), 'bid_prices': np.ndarray((3,), dtype=float64, min=1.0, max=2724.253, mean=1018.007), 'ask_prices': np.ndarray((3,), dtype=float64, min=1.0, max=2726.979, mean=1019.025), 'cost_of_spread': np.ndarray((3,), dtype=float64, min=0.0, max=0.033, mean=0.017), 'cost_of_commissions': np.ndarray((3,), dtype=float64, min=0.0, max=0.033, mean=0.017), 'profit_on_idle_cash': 0.0, 'margin': 0, 'weights_target': np.ndarray((3,), dtype=float32, min=0.0, max=0.663, mean=0.333), 'weights_drift': np.ndarray((3,), dtype=float64, min=0.0, max=1.0, mean=0.333), 'weights_diff': np.ndarray((3,), dtype=float64, min=-1.0, max=0.663, mean=0.0), 'positions_value_diff': np.ndarray((3,), dtype=float64, min=-100.0, max=66.299, mean=0.0), 'acq_prices': np.ndarray((3,), dtype=float64, min=1.0, max=2726.979, mean=1019.025), 'nr_contracts_diff': np.ndarray((3,), dtype=float64, min=-100.0, max=0.201, mean=-33.262), 'nlv_pre': 100.0}),\n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m             'new_obs': np.ndarray((200, 5), dtype=float32, min=-7.803, max=3.708, mean=0.258),\n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m             'obs': np.ndarray((200, 5), dtype=float32, min=-7.803, max=4.296, mean=0.257),\n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m             'prev_actions': np.ndarray((200, 2), dtype=float32, min=0.0, max=1.0, mean=0.475),\n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m             'prev_rewards': np.ndarray((200,), dtype=float32, min=-0.033, max=0.034, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m             'rewards': np.ndarray((200,), dtype=float32, min=-0.033, max=0.034, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m             't': np.ndarray((200,), dtype=int64, min=0.0, max=19.0, mean=9.5),\n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m             'unroll_id': np.ndarray((200,), dtype=int64, min=0.0, max=9.0, mean=4.5),\n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m             'value_targets': np.ndarray((200,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m             'vf_preds': np.ndarray((200,), dtype=float32, min=0.0, max=0.0, mean=0.0)},\n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m   'type': 'SampleBatch'}\n",
      "\u001b[2m\u001b[36m(pid=61557)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:22:06,903\tINFO multi_gpu_impl.py:146 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m { 'inputs': [ np.ndarray((4000, 2), dtype=float32, min=0.0, max=1.0, mean=0.475),\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m               np.ndarray((4000,), dtype=float32, min=-0.068, max=0.047, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m               np.ndarray((4000, 5), dtype=float32, min=-14.063, max=12.847, mean=0.24),\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m               np.ndarray((4000, 2), dtype=float32, min=0.0, max=1.0, mean=0.5),\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m               np.ndarray((4000,), dtype=float32, min=-12.782, max=8.931, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m               np.ndarray((4000, 2), dtype=float32, min=-0.019, max=0.021, mean=-0.001),\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m               np.ndarray((4000,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m               np.ndarray((4000,), dtype=float32, min=0.0, max=0.0, mean=0.0)],\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m   'placeholders': [ <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m                     <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m                     <tf.Tensor 'default_policy/observation:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m                     <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m                     <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m                     <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m                     <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m                     <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>],\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m   'state_inputs': []}\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:22:06,903\tINFO multi_gpu_impl.py:191 -- Divided 4000 rollout sequences, each of length 1, among 1 devices.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:22:08,884\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:22:10,115\tINFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:22:18,388\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:22:27,908\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:22:38,704\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:22:48,757\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:22:58,569\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:23:09,307\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:23:18,563\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:23:28,382\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:23:39,037\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:23:48,253\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:23:57,859\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:24:07,783\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:24:17,777\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:24:27,335\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:24:37,810\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:24:46,791\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:24:56,047\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:25:05,801\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:25:16,080\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:25:25,417\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:25:35,236\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:25:45,934\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:25:55,265\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-12 11:25:58,130\tWARNING util.py:64 -- The `fetch_result` operation took 0.18250274658203125 seconds to complete, which may be a performance bottleneck.\n",
      "2019-07-12 11:25:58,138\tWARNING util.py:64 -- The `process_trial` operation took 0.19099187850952148 seconds to complete, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:26:04,892\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:26:16,182\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:26:25,562\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:26:35,373\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:26:46,570\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:26:55,627\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:27:05,426\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:27:16,083\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:27:25,329\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:27:35,483\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:27:46,069\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:27:55,219\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:28:04,878\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:28:15,105\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:28:24,158\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:28:34,253\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:28:44,329\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:28:53,852\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:29:03,111\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:29:12,495\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:29:23,112\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:29:32,879\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:29:43,844\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:29:52,630\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:30:01,866\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:30:13,000\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:30:22,202\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:30:31,099\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:30:41,626\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:30:51,240\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:31:00,073\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:31:09,718\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:31:20,272\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:31:28,650\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:31:38,163\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:31:49,729\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:31:59,016\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:32:08,677\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:32:20,507\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:32:30,195\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:32:39,934\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:32:51,624\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:33:01,139\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:33:10,804\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:33:22,492\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:33:32,674\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:33:43,441\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:33:55,067\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:34:04,056\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:34:13,862\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:34:24,949\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:34:34,287\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:34:44,289\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:34:55,519\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:35:04,718\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:35:14,705\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:35:26,483\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:35:35,998\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:35:45,835\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:35:57,492\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:36:06,820\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:36:16,599\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:36:28,335\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:36:37,536\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:36:47,001\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:36:57,924\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:37:07,520\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:37:16,787\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:37:28,671\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:37:39,348\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:37:49,307\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:38:00,729\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:38:10,775\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:38:20,535\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:38:32,174\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:38:42,027\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:38:51,938\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:39:04,108\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:39:14,808\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:39:25,581\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:39:37,319\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:39:46,949\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:39:57,526\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:40:09,140\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:40:18,900\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:40:29,600\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:40:40,888\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:40:50,346\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:41:01,102\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:41:12,166\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:41:21,742\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:41:32,669\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:41:43,810\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:41:53,537\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:42:03,815\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:42:14,860\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:42:24,935\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:42:36,423\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:42:46,604\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:42:56,311\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:43:07,713\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:43:18,082\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:43:27,749\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:43:39,726\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:43:49,789\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:43:59,326\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:44:10,938\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:44:21,060\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:44:30,786\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:44:42,237\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:44:52,313\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:45:02,000\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:45:13,587\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:45:23,613\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:45:33,634\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:45:45,053\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:45:54,913\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:46:04,989\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:46:16,573\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:46:26,603\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:46:36,819\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:46:48,456\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:46:57,985\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:47:08,247\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:47:19,909\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:47:29,362\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:47:39,261\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:47:51,010\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:48:00,571\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:48:10,467\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:48:22,225\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:48:31,729\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:48:41,765\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:48:53,455\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:49:02,653\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:49:12,345\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:49:23,602\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:49:32,805\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:49:42,316\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:49:53,603\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:50:03,285\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:50:12,895\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:50:23,968\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:50:33,259\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:50:42,791\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:50:53,491\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:51:03,371\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:51:12,749\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:51:23,282\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:51:32,967\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:51:42,074\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:51:52,907\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:52:02,726\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:52:11,820\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:52:22,223\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:52:32,395\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:52:41,122\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:52:51,472\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:53:02,275\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:53:10,913\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:53:20,473\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:53:31,691\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:53:40,448\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=44239)\u001b[0m 2019-07-12 11:53:49,771\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-12 11:53:52,801\tINFO ray_trial_executor.py:187 -- Destroying actor for trial PPO_GAIAPredictorsContinuousV8_0_clip_param=0.8,entropy_coeff=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2019-07-12 11:53:52,859\tINFO tune.py:65 -- Did not find checkpoint file in logs/tran_cost_x10/clip_0.8-tc-WalkForward-750k2009.\n",
      "2019-07-12 11:53:52,865\tINFO tune.py:233 -- Starting a new experiment.\n",
      "2019-07-12 11:53:52,897\tWARNING signature.py:108 -- The function with_updates has a **kwargs argument, which is currently not supported.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_______________________________________2009____________________________________________\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 11:53:55,532\tWARNING ppo.py:151 -- FYI: By default, the value function will not share layers with the policy model ('vf_share_layers': False).\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 11:53:56,873\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 0 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 11:53:56.874156: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 11:54:04,245\tINFO dynamic_tf_policy.py:313 -- Initializing loss function with dummy input:\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m { 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m   'actions': <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m   'behaviour_logits': <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m   'prev_actions': <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 11:54:05,720\tINFO rollout_worker.py:719 -- Built policy map: {'default_policy': <ray.rllib.policy.tf_policy_template.PPOTFPolicy object at 0x7f98e07191d0>}\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 11:54:05,720\tINFO rollout_worker.py:720 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7f98e07b9d68>}\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 11:54:05,720\tINFO rollout_worker.py:333 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7f98e07b9ba8>}\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 11:54:05,842\tINFO multi_gpu_optimizer.py:79 -- LocalMultiGPUOptimizer devices ['/cpu:0']\n",
      "\u001b[2m\u001b[36m(pid=61658)\u001b[0m 2019-07-12 11:54:09,115\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 4 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=61658)\u001b[0m 2019-07-12 11:54:09.140523: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=61618)\u001b[0m 2019-07-12 11:54:09,367\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 3 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=61618)\u001b[0m 2019-07-12 11:54:09.419532: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=61621)\u001b[0m 2019-07-12 11:54:09,757\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 5 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=61621)\u001b[0m 2019-07-12 11:54:09.788702: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=61616)\u001b[0m 2019-07-12 11:54:09,902\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 6 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=61616)\u001b[0m 2019-07-12 11:54:09.956286: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=61655)\u001b[0m 2019-07-12 11:54:10,151\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 2 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=61655)\u001b[0m 2019-07-12 11:54:10.210065: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m 2019-07-12 11:54:10,184\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 1 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m 2019-07-12 11:54:10.208560: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=61621)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=61621)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61621)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=61621)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m 2019-07-12 11:54:23,662\tINFO dynamic_tf_policy.py:313 -- Initializing loss function with dummy input:\n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m { 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m   'actions': <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m   'behaviour_logits': <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,\n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m   'prev_actions': <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}\n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61618)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=61618)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61618)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=61618)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61658)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=61658)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61658)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=61658)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61655)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=61655)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61655)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=61655)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=61616)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=61616)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61616)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=61616)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m 2019-07-12 11:54:26,779\tINFO rollout_worker.py:428 -- Generating sample batch of size 200\n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m 2019-07-12 11:54:26,813\tINFO sampler.py:308 -- Raw obs from env: { 0: { 'agent0': np.ndarray((5,), dtype=float64, min=-1.0, max=1.548, mean=0.256)}}\n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m 2019-07-12 11:54:26,813\tINFO sampler.py:309 -- Info return from env: {0: {'agent0': None}}\n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m 2019-07-12 11:54:26,814\tINFO sampler.py:407 -- Preprocessed obs: np.ndarray((5,), dtype=float64, min=-1.0, max=1.548, mean=0.256)\n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m 2019-07-12 11:54:26,814\tINFO sampler.py:411 -- Filtered obs: np.ndarray((5,), dtype=float64, min=-1.0, max=1.548, mean=0.256)\n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m 2019-07-12 11:54:26,815\tINFO sampler.py:525 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m                                   'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m                                   'obs': np.ndarray((5,), dtype=float64, min=-1.0, max=1.548, mean=0.256),\n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m 2019-07-12 11:54:26,816\tINFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m 2019-07-12 11:54:27,014\tINFO sampler.py:552 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m { 'default_policy': ( np.ndarray((1, 2), dtype=float32, min=0.248, max=0.752, mean=0.5),\n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m                       [],\n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m                       { 'action_prob': np.ndarray((1,), dtype=float32, min=0.999, max=0.999, mean=0.999),\n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m                         'behaviour_logits': np.ndarray((1, 2), dtype=float32, min=0.003, max=0.008, mean=0.006),\n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=0.0, max=0.0, mean=0.0)})}\n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m 2019-07-12 11:54:27,145\tINFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m { 'agent0': { 'data': { 'action_prob': np.ndarray((20,), dtype=float32, min=0.987, max=1.005, mean=0.998),\n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m                         'actions': np.ndarray((20, 2), dtype=float32, min=0.012, max=0.988, mean=0.5),\n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m                         'advantages': np.ndarray((20,), dtype=float32, min=-0.015, max=0.021, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m                         'agent_index': np.ndarray((20,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m                         'behaviour_logits': np.ndarray((20, 2), dtype=float32, min=-0.007, max=0.014, mean=0.004),\n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m                         'dones': np.ndarray((20,), dtype=bool, min=0.0, max=1.0, mean=0.05),\n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m                         'eps_id': np.ndarray((20,), dtype=int64, min=1583347578.0, max=1583347578.0, mean=1583347578.0),\n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m                         'infos': np.ndarray((20,), dtype=object, head={'time': Timestamp('2006-05-18 00:00:00'), 'nlv': 99.85004997724693, 'nr_contracts': np.ndarray((3,), dtype=float64, min=-0.05, max=0.077, mean=0.017), 'bid_prices': np.ndarray((3,), dtype=float64, min=1.0, max=3027.648, mean=1117.284), 'ask_prices': np.ndarray((3,), dtype=float64, min=1.0, max=3030.678, mean=1118.401), 'cost_of_spread': np.ndarray((3,), dtype=float64, min=0.0, max=0.038, mean=0.017), 'cost_of_commissions': np.ndarray((3,), dtype=float64, min=0.0, max=0.038, mean=0.017), 'profit_on_idle_cash': 0.0, 'margin': 0, 'weights_target': np.ndarray((3,), dtype=float32, min=0.0, max=0.752, mean=0.333), 'weights_drift': np.ndarray((3,), dtype=float64, min=0.0, max=1.0, mean=0.333), 'weights_diff': np.ndarray((3,), dtype=float64, min=-1.0, max=0.752, mean=-0.0), 'positions_value_diff': np.ndarray((3,), dtype=float64, min=-100.0, max=75.178, mean=-0.0), 'acq_prices': np.ndarray((3,), dtype=float64, min=1.0, max=3030.678, mean=1118.401), 'nr_contracts_diff': np.ndarray((3,), dtype=float64, min=-100.0, max=0.077, mean=-33.299), 'nlv_pre': 100.0}),\n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m                         'new_obs': np.ndarray((20, 5), dtype=float32, min=-7.803, max=3.708, mean=0.191),\n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m                         'obs': np.ndarray((20, 5), dtype=float32, min=-7.803, max=3.708, mean=0.184),\n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m                         'prev_actions': np.ndarray((20, 2), dtype=float32, min=0.0, max=0.988, mean=0.475),\n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m                         'prev_rewards': np.ndarray((20,), dtype=float32, min=-0.015, max=0.009, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m                         'rewards': np.ndarray((20,), dtype=float32, min=-0.015, max=0.021, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m                         't': np.ndarray((20,), dtype=int64, min=0.0, max=19.0, mean=9.5),\n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m                         'unroll_id': np.ndarray((20,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m                         'value_targets': np.ndarray((20,), dtype=float64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m                         'vf_preds': np.ndarray((20,), dtype=float32, min=0.0, max=0.0, mean=0.0)},\n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m               'type': 'SampleBatch'}}\n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m 2019-07-12 11:54:28,772\tINFO rollout_worker.py:462 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m { 'data': { 'action_prob': np.ndarray((200,), dtype=float32, min=0.966, max=1.027, mean=1.001),\n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m             'actions': np.ndarray((200, 2), dtype=float32, min=0.001, max=0.999, mean=0.5),\n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m             'advantages': np.ndarray((200,), dtype=float32, min=-0.015, max=0.021, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m             'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m             'behaviour_logits': np.ndarray((200, 2), dtype=float32, min=-0.008, max=0.014, mean=0.004),\n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m             'dones': np.ndarray((200,), dtype=bool, min=0.0, max=1.0, mean=0.05),\n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m             'eps_id': np.ndarray((200,), dtype=int64, min=452330236.0, max=1867195236.0, mean=1218278134.4),\n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m             'infos': np.ndarray((200,), dtype=object, head={'time': Timestamp('2006-05-18 00:00:00'), 'nlv': 99.85004997724693, 'nr_contracts': np.ndarray((3,), dtype=float64, min=-0.05, max=0.077, mean=0.017), 'bid_prices': np.ndarray((3,), dtype=float64, min=1.0, max=3027.648, mean=1117.284), 'ask_prices': np.ndarray((3,), dtype=float64, min=1.0, max=3030.678, mean=1118.401), 'cost_of_spread': np.ndarray((3,), dtype=float64, min=0.0, max=0.038, mean=0.017), 'cost_of_commissions': np.ndarray((3,), dtype=float64, min=0.0, max=0.038, mean=0.017), 'profit_on_idle_cash': 0.0, 'margin': 0, 'weights_target': np.ndarray((3,), dtype=float32, min=0.0, max=0.752, mean=0.333), 'weights_drift': np.ndarray((3,), dtype=float64, min=0.0, max=1.0, mean=0.333), 'weights_diff': np.ndarray((3,), dtype=float64, min=-1.0, max=0.752, mean=-0.0), 'positions_value_diff': np.ndarray((3,), dtype=float64, min=-100.0, max=75.178, mean=-0.0), 'acq_prices': np.ndarray((3,), dtype=float64, min=1.0, max=3030.678, mean=1118.401), 'nr_contracts_diff': np.ndarray((3,), dtype=float64, min=-100.0, max=0.077, mean=-33.299), 'nlv_pre': 100.0}),\n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m             'new_obs': np.ndarray((200, 5), dtype=float32, min=-7.803, max=6.657, mean=0.294),\n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m             'obs': np.ndarray((200, 5), dtype=float32, min=-7.803, max=6.657, mean=0.294),\n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m             'prev_actions': np.ndarray((200, 2), dtype=float32, min=0.0, max=0.999, mean=0.475),\n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m             'prev_rewards': np.ndarray((200,), dtype=float32, min=-0.015, max=0.021, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m             'rewards': np.ndarray((200,), dtype=float32, min=-0.015, max=0.021, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m             't': np.ndarray((200,), dtype=int64, min=0.0, max=19.0, mean=9.5),\n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m             'unroll_id': np.ndarray((200,), dtype=int64, min=0.0, max=9.0, mean=4.5),\n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m             'value_targets': np.ndarray((200,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m             'vf_preds': np.ndarray((200,), dtype=float32, min=0.0, max=0.0, mean=0.0)},\n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m   'type': 'SampleBatch'}\n",
      "\u001b[2m\u001b[36m(pid=61712)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 11:54:33,330\tINFO multi_gpu_impl.py:146 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m { 'inputs': [ np.ndarray((4000, 2), dtype=float32, min=0.0, max=1.0, mean=0.475),\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m               np.ndarray((4000,), dtype=float32, min=-0.075, max=0.047, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m               np.ndarray((4000, 5), dtype=float32, min=-8.926, max=11.969, mean=0.268),\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m               np.ndarray((4000, 2), dtype=float32, min=0.0, max=1.0, mean=0.5),\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m               np.ndarray((4000,), dtype=float32, min=-15.709, max=9.749, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m               np.ndarray((4000, 2), dtype=float32, min=-0.009, max=0.014, mean=0.002),\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m               np.ndarray((4000,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m               np.ndarray((4000,), dtype=float32, min=0.0, max=0.0, mean=0.0)],\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m   'placeholders': [ <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m                     <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m                     <tf.Tensor 'default_policy/observation:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m                     <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m                     <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m                     <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m                     <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m                     <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>],\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m   'state_inputs': []}\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 11:54:33,331\tINFO multi_gpu_impl.py:191 -- Divided 4000 rollout sequences, each of length 1, among 1 devices.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 11:54:35,623\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 11:54:36,959\tINFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 11:54:45,377\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 11:54:55,296\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 11:55:08,464\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 11:55:23,924\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 11:55:34,815\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 11:55:45,896\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 11:55:55,747\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 11:56:07,798\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 11:56:16,761\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 11:56:26,522\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 11:56:38,529\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 11:56:47,405\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 11:56:57,893\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 11:57:10,404\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 11:57:19,071\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 11:57:29,452\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 11:57:41,507\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 11:57:50,885\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 11:58:00,698\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 11:58:11,328\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 11:58:20,408\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 11:58:30,747\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 11:58:43,280\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 11:58:51,897\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 11:59:01,341\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 11:59:12,021\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 11:59:21,170\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 11:59:31,385\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 11:59:43,600\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 11:59:52,454\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:00:01,375\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:00:12,355\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:00:22,472\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:00:31,531\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:00:42,285\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:00:51,326\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:00:59,711\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:01:09,577\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:01:19,652\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:01:27,950\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:01:37,643\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:01:48,443\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:01:56,808\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:02:06,143\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:02:17,079\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:02:25,853\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:02:34,769\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:02:45,237\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:02:54,727\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:03:03,569\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:03:14,063\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:03:24,215\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:03:32,785\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:03:42,310\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:03:52,804\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:04:01,528\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:04:10,470\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:04:22,605\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:04:31,813\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:04:41,521\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:04:53,496\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:05:02,456\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:05:11,350\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:05:21,767\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:05:30,964\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:05:40,230\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:05:50,924\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:06:00,391\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:06:09,406\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:06:19,577\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:06:30,614\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:06:41,157\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:06:51,254\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:07:04,219\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:07:15,835\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:07:29,119\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:07:40,406\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:07:50,157\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:08:00,762\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:08:09,155\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:08:18,507\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:08:29,056\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:08:37,910\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:08:46,591\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:08:56,903\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:09:06,678\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:09:16,047\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:09:27,583\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:09:37,868\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:09:47,327\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:09:57,283\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:10:09,159\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:10:21,554\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:10:35,959\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:10:49,073\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:11:01,157\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:11:12,677\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:11:22,942\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:11:34,323\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:11:44,510\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:11:53,873\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:12:03,986\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:12:13,843\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:12:22,418\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:12:32,896\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:12:42,591\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:12:52,373\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:13:03,645\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:13:14,707\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:13:24,890\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:13:35,737\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:13:46,098\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:13:55,576\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:14:06,059\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:14:16,276\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:14:25,186\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:14:35,733\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:14:45,636\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:14:54,103\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:15:03,585\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:15:14,036\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:15:22,467\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:15:31,784\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:15:42,957\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:15:51,835\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:16:01,580\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:16:14,010\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:16:24,994\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:16:34,140\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:16:43,623\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:16:53,647\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:17:02,606\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:17:13,053\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:17:22,559\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:17:31,228\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:17:41,915\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:17:51,802\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:18:00,333\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:18:09,433\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:18:18,917\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:18:28,422\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:18:38,473\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:18:47,618\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:18:57,869\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:19:06,780\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:19:16,039\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:19:26,748\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:19:35,748\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:19:45,753\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:19:55,314\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:20:03,824\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:20:13,384\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:20:23,897\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:20:32,571\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:20:41,514\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:20:50,820\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:21:00,776\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:21:10,237\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:21:20,444\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:21:29,886\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:21:38,807\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:21:47,944\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:21:58,003\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:22:07,175\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:22:16,623\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:22:25,801\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:22:35,590\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:22:44,771\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:22:53,970\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:23:04,109\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:23:12,947\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:23:22,180\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:23:31,839\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:23:40,671\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:23:49,996\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:23:59,762\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:24:11,048\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:24:22,220\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:24:33,972\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:24:43,938\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:24:54,241\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:25:04,855\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:25:15,856\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:25:26,751\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:25:38,153\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:25:47,507\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61661)\u001b[0m 2019-07-12 12:25:57,636\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-12 12:26:00,561\tINFO ray_trial_executor.py:187 -- Destroying actor for trial PPO_GAIAPredictorsContinuousV8_0_clip_param=0.8,entropy_coeff=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2019-07-12 12:26:00,618\tINFO tune.py:65 -- Did not find checkpoint file in logs/tran_cost_x10/clip_0.8-tc-WalkForward-750k2010.\n",
      "2019-07-12 12:26:00,619\tINFO tune.py:233 -- Starting a new experiment.\n",
      "2019-07-12 12:26:00,635\tWARNING signature.py:108 -- The function with_updates has a **kwargs argument, which is currently not supported.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_______________________________________2010____________________________________________\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:26:02,523\tWARNING ppo.py:151 -- FYI: By default, the value function will not share layers with the policy model ('vf_share_layers': False).\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:26:03,600\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 0 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:26:03.601567: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:26:11,909\tINFO dynamic_tf_policy.py:313 -- Initializing loss function with dummy input:\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m { 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m   'actions': <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m   'behaviour_logits': <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m   'prev_actions': <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:26:13,308\tINFO rollout_worker.py:719 -- Built policy map: {'default_policy': <ray.rllib.policy.tf_policy_template.PPOTFPolicy object at 0x7f0096703160>}\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:26:13,308\tINFO rollout_worker.py:720 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7f00967a5cf8>}\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:26:13,308\tINFO rollout_worker.py:333 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7f0096703048>}\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:26:13,464\tINFO multi_gpu_optimizer.py:79 -- LocalMultiGPUOptimizer devices ['/cpu:0']\n",
      "\u001b[2m\u001b[36m(pid=94376)\u001b[0m 2019-07-12 12:26:27,905\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 5 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=94376)\u001b[0m 2019-07-12 12:26:27.942756: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=94357)\u001b[0m 2019-07-12 12:26:28,087\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 2 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=94357)\u001b[0m 2019-07-12 12:26:28.174327: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m 2019-07-12 12:26:28,288\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 1 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m 2019-07-12 12:26:28.328442: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=94360)\u001b[0m 2019-07-12 12:26:28,823\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 4 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=94360)\u001b[0m 2019-07-12 12:26:28.885965: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=94375)\u001b[0m 2019-07-12 12:26:28,922\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 6 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=94375)\u001b[0m 2019-07-12 12:26:28.949902: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=94359)\u001b[0m 2019-07-12 12:26:29,521\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 3 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=94359)\u001b[0m 2019-07-12 12:26:29.558065: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m 2019-07-12 12:26:44,553\tINFO dynamic_tf_policy.py:313 -- Initializing loss function with dummy input:\n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m { 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m   'actions': <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m   'behaviour_logits': <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,\n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m   'prev_actions': <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}\n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=94376)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=94376)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=94376)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=94376)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=94357)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=94357)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=94357)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=94357)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=94375)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=94375)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=94375)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=94375)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=94359)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=94359)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=94359)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=94359)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=94360)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=94360)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=94360)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=94360)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m 2019-07-12 12:26:47,673\tINFO rollout_worker.py:428 -- Generating sample batch of size 200\n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m 2019-07-12 12:26:47,710\tINFO sampler.py:308 -- Raw obs from env: { 0: { 'agent0': np.ndarray((5,), dtype=float64, min=-1.0, max=3.782, mean=0.915)}}\n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m 2019-07-12 12:26:47,710\tINFO sampler.py:309 -- Info return from env: {0: {'agent0': None}}\n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m 2019-07-12 12:26:47,711\tINFO sampler.py:407 -- Preprocessed obs: np.ndarray((5,), dtype=float64, min=-1.0, max=3.782, mean=0.915)\n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m 2019-07-12 12:26:47,711\tINFO sampler.py:411 -- Filtered obs: np.ndarray((5,), dtype=float64, min=-1.0, max=3.782, mean=0.915)\n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m 2019-07-12 12:26:47,712\tINFO sampler.py:525 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m                                   'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m                                   'obs': np.ndarray((5,), dtype=float64, min=-1.0, max=3.782, mean=0.915),\n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m 2019-07-12 12:26:47,713\tINFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m 2019-07-12 12:26:47,857\tINFO sampler.py:552 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m { 'default_policy': ( np.ndarray((1, 2), dtype=float32, min=0.251, max=0.749, mean=0.5),\n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m                       [],\n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m                       { 'action_prob': np.ndarray((1,), dtype=float32, min=0.998, max=0.998, mean=0.998),\n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m                         'behaviour_logits': np.ndarray((1, 2), dtype=float32, min=-0.001, max=0.003, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=0.0, max=0.0, mean=0.0)})}\n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m 2019-07-12 12:26:48,060\tINFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m { 'agent0': { 'data': { 'action_prob': np.ndarray((20,), dtype=float32, min=0.96, max=1.006, mean=0.998),\n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m                         'actions': np.ndarray((20, 2), dtype=float32, min=0.002, max=0.998, mean=0.5),\n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m                         'advantages': np.ndarray((20,), dtype=float32, min=-0.015, max=0.006, mean=-0.002),\n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m                         'agent_index': np.ndarray((20,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m                         'behaviour_logits': np.ndarray((20, 2), dtype=float32, min=-0.001, max=0.011, mean=0.004),\n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m                         'dones': np.ndarray((20,), dtype=bool, min=0.0, max=1.0, mean=0.05),\n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m                         'eps_id': np.ndarray((20,), dtype=int64, min=44854656.0, max=44854656.0, mean=44854656.0),\n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m                         'infos': np.ndarray((20,), dtype=object, head={'time': Timestamp('2005-09-20 00:00:00'), 'nlv': 99.8500499750125, 'nr_contracts': np.ndarray((3,), dtype=float64, min=-0.05, max=0.074, mean=0.017), 'bid_prices': np.ndarray((3,), dtype=float64, min=1.0, max=2885.61, mean=1074.406), 'ask_prices': np.ndarray((3,), dtype=float64, min=1.0, max=2888.498, mean=1075.48), 'cost_of_spread': np.ndarray((3,), dtype=float64, min=0.0, max=0.037, mean=0.017), 'cost_of_commissions': np.ndarray((3,), dtype=float64, min=0.0, max=0.037, mean=0.017), 'profit_on_idle_cash': 0.0, 'margin': 0, 'weights_target': np.ndarray((3,), dtype=float32, min=0.0, max=0.749, mean=0.333), 'weights_drift': np.ndarray((3,), dtype=float64, min=0.0, max=1.0, mean=0.333), 'weights_diff': np.ndarray((3,), dtype=float64, min=-1.0, max=0.749, mean=0.0), 'positions_value_diff': np.ndarray((3,), dtype=float64, min=-100.0, max=74.919, mean=0.0), 'acq_prices': np.ndarray((3,), dtype=float64, min=1.0, max=2888.498, mean=1075.48), 'nr_contracts_diff': np.ndarray((3,), dtype=float64, min=-100.0, max=0.074, mean=-33.3), 'nlv_pre': 100.0}),\n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m                         'new_obs': np.ndarray((20, 5), dtype=float32, min=-4.47, max=2.652, mean=0.329),\n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m                         'obs': np.ndarray((20, 5), dtype=float32, min=-1.892, max=3.782, mean=0.399),\n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m                         'prev_actions': np.ndarray((20, 2), dtype=float32, min=0.0, max=0.998, mean=0.475),\n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m                         'prev_rewards': np.ndarray((20,), dtype=float32, min=-0.015, max=0.006, mean=-0.001),\n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m                         'rewards': np.ndarray((20,), dtype=float32, min=-0.015, max=0.006, mean=-0.002),\n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m                         't': np.ndarray((20,), dtype=int64, min=0.0, max=19.0, mean=9.5),\n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m                         'unroll_id': np.ndarray((20,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m                         'value_targets': np.ndarray((20,), dtype=float64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m                         'vf_preds': np.ndarray((20,), dtype=float32, min=0.0, max=0.0, mean=0.0)},\n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m               'type': 'SampleBatch'}}\n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m 2019-07-12 12:26:49,692\tINFO rollout_worker.py:462 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m { 'data': { 'action_prob': np.ndarray((200,), dtype=float32, min=0.96, max=1.01, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m             'actions': np.ndarray((200, 2), dtype=float32, min=0.001, max=0.999, mean=0.5),\n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m             'advantages': np.ndarray((200,), dtype=float32, min=-0.027, max=0.021, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m             'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m             'behaviour_logits': np.ndarray((200, 2), dtype=float32, min=-0.01, max=0.013, mean=0.003),\n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m             'dones': np.ndarray((200,), dtype=bool, min=0.0, max=1.0, mean=0.05),\n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m             'eps_id': np.ndarray((200,), dtype=int64, min=44854656.0, max=1664770393.0, mean=818657366.4),\n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m             'infos': np.ndarray((200,), dtype=object, head={'time': Timestamp('2005-09-20 00:00:00'), 'nlv': 99.8500499750125, 'nr_contracts': np.ndarray((3,), dtype=float64, min=-0.05, max=0.074, mean=0.017), 'bid_prices': np.ndarray((3,), dtype=float64, min=1.0, max=2885.61, mean=1074.406), 'ask_prices': np.ndarray((3,), dtype=float64, min=1.0, max=2888.498, mean=1075.48), 'cost_of_spread': np.ndarray((3,), dtype=float64, min=0.0, max=0.037, mean=0.017), 'cost_of_commissions': np.ndarray((3,), dtype=float64, min=0.0, max=0.037, mean=0.017), 'profit_on_idle_cash': 0.0, 'margin': 0, 'weights_target': np.ndarray((3,), dtype=float32, min=0.0, max=0.749, mean=0.333), 'weights_drift': np.ndarray((3,), dtype=float64, min=0.0, max=1.0, mean=0.333), 'weights_diff': np.ndarray((3,), dtype=float64, min=-1.0, max=0.749, mean=0.0), 'positions_value_diff': np.ndarray((3,), dtype=float64, min=-100.0, max=74.919, mean=0.0), 'acq_prices': np.ndarray((3,), dtype=float64, min=1.0, max=2888.498, mean=1075.48), 'nr_contracts_diff': np.ndarray((3,), dtype=float64, min=-100.0, max=0.074, mean=-33.3), 'nlv_pre': 100.0}),\n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m             'new_obs': np.ndarray((200, 5), dtype=float32, min=-4.948, max=6.657, mean=0.315),\n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m             'obs': np.ndarray((200, 5), dtype=float32, min=-5.822, max=10.483, mean=0.321),\n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m             'prev_actions': np.ndarray((200, 2), dtype=float32, min=0.0, max=0.999, mean=0.475),\n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m             'prev_rewards': np.ndarray((200,), dtype=float32, min=-0.027, max=0.021, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m             'rewards': np.ndarray((200,), dtype=float32, min=-0.027, max=0.021, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m             't': np.ndarray((200,), dtype=int64, min=0.0, max=19.0, mean=9.5),\n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m             'unroll_id': np.ndarray((200,), dtype=int64, min=0.0, max=9.0, mean=4.5),\n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m             'value_targets': np.ndarray((200,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m             'vf_preds': np.ndarray((200,), dtype=float32, min=0.0, max=0.0, mean=0.0)},\n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m   'type': 'SampleBatch'}\n",
      "\u001b[2m\u001b[36m(pid=94358)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:26:54,576\tINFO multi_gpu_impl.py:146 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m { 'inputs': [ np.ndarray((4000, 2), dtype=float32, min=0.0, max=0.999, mean=0.475),\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m               np.ndarray((4000,), dtype=float32, min=-0.045, max=0.043, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m               np.ndarray((4000, 5), dtype=float32, min=-8.926, max=12.677, mean=0.252),\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m               np.ndarray((4000, 2), dtype=float32, min=0.001, max=0.999, mean=0.5),\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m               np.ndarray((4000,), dtype=float32, min=-8.39, max=7.973, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m               np.ndarray((4000, 2), dtype=float32, min=-0.014, max=0.014, mean=0.002),\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m               np.ndarray((4000,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m               np.ndarray((4000,), dtype=float32, min=0.0, max=0.0, mean=0.0)],\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m   'placeholders': [ <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m                     <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m                     <tf.Tensor 'default_policy/observation:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m                     <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m                     <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m                     <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m                     <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m                     <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>],\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m   'state_inputs': []}\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:26:54,576\tINFO multi_gpu_impl.py:191 -- Divided 4000 rollout sequences, each of length 1, among 1 devices.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:26:56,782\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:26:58,062\tINFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:27:07,614\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:27:18,602\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:27:28,472\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:27:39,305\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:27:49,988\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:27:59,791\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:28:10,287\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:28:21,754\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:28:32,074\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:28:41,361\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:28:52,304\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:29:02,548\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:29:12,293\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:29:21,916\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:29:31,965\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:29:42,073\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:29:51,334\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:30:01,535\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:30:11,517\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:30:20,839\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:30:31,096\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:30:41,337\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:30:50,327\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:31:00,138\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:31:10,721\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:31:20,484\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:31:31,138\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:31:42,018\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:31:51,754\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:32:02,349\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:32:13,046\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:32:23,151\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:32:32,655\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:32:43,266\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:32:53,700\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:33:03,404\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:33:14,023\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:33:23,860\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:33:34,491\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:33:46,156\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:33:57,177\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:34:06,964\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:34:17,761\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:34:28,099\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:34:37,774\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:34:48,817\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:34:59,126\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:35:09,063\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:35:20,677\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:35:31,594\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:35:41,686\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:35:52,958\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:36:03,531\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:36:13,529\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:36:24,197\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:36:34,577\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:36:44,579\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:36:55,144\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:37:04,775\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:37:14,323\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:37:25,193\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:37:35,391\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:37:45,175\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:37:56,394\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:38:06,313\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:38:16,299\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:38:26,936\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:38:36,377\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:38:45,983\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:38:56,799\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:39:06,848\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:39:16,278\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:39:26,982\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:39:37,671\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:39:47,339\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:39:58,712\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:40:08,672\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:40:17,880\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:40:29,113\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:40:38,906\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:40:48,839\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:40:59,325\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:41:09,535\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:41:18,463\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:41:29,206\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:41:39,142\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:41:48,013\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:41:58,388\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:42:08,133\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:42:17,386\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:42:27,150\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:42:37,111\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:42:46,050\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:42:55,504\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:43:06,030\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:43:15,261\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:43:24,525\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:43:35,963\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:43:45,584\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:43:54,902\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:44:05,650\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:44:14,875\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:44:24,030\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:44:34,599\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:44:44,399\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:44:53,122\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:45:03,306\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:45:13,282\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:45:22,031\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:45:31,898\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:45:42,214\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:45:50,928\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:46:00,227\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:46:10,856\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:46:19,832\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:46:29,070\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:46:40,031\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:46:49,211\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:46:57,812\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:47:08,235\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:47:17,960\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:47:26,444\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:47:36,500\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:47:46,737\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:47:55,647\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:48:05,076\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:48:15,852\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:48:24,774\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:48:34,010\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:48:44,650\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:48:54,100\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:49:03,152\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:49:13,622\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:49:23,384\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:49:32,373\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:49:42,716\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:49:52,430\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:50:01,042\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:50:10,964\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:50:21,298\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:50:30,117\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:50:39,457\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:50:50,050\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:50:59,220\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:51:08,345\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:51:18,922\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:51:28,574\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:51:37,602\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:51:48,103\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:51:57,531\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:52:06,529\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:52:15,782\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:52:26,360\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:52:35,498\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:52:44,802\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:52:55,356\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:53:04,145\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:53:13,406\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:53:24,020\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:53:33,308\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:53:42,651\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:53:53,429\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:54:03,379\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:54:12,921\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:54:23,673\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:54:33,699\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:54:42,959\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:54:53,943\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:55:03,689\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:55:12,739\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:55:23,703\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:55:33,825\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:55:42,793\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:55:53,023\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:56:03,485\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:56:12,571\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:56:22,383\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:56:33,600\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:56:42,827\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:56:52,575\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:57:03,517\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:57:12,428\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:57:22,034\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:57:33,148\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:57:42,627\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:57:51,993\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61617)\u001b[0m 2019-07-12 12:58:03,070\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-12 12:58:06,023\tINFO ray_trial_executor.py:187 -- Destroying actor for trial PPO_GAIAPredictorsContinuousV8_0_clip_param=0.8,entropy_coeff=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2019-07-12 12:58:06,070\tINFO tune.py:65 -- Did not find checkpoint file in logs/tran_cost_x10/clip_0.8-tc-WalkForward-750k2011.\n",
      "2019-07-12 12:58:06,071\tINFO tune.py:233 -- Starting a new experiment.\n",
      "2019-07-12 12:58:06,115\tWARNING signature.py:108 -- The function with_updates has a **kwargs argument, which is currently not supported.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_______________________________________2011____________________________________________\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 12:58:08,463\tWARNING ppo.py:151 -- FYI: By default, the value function will not share layers with the policy model ('vf_share_layers': False).\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 12:58:09,581\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 0 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 12:58:09.582887: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 12:58:16,341\tINFO dynamic_tf_policy.py:313 -- Initializing loss function with dummy input:\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m { 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m   'actions': <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m   'behaviour_logits': <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m   'prev_actions': <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 12:58:17,925\tINFO rollout_worker.py:719 -- Built policy map: {'default_policy': <ray.rllib.policy.tf_policy_template.PPOTFPolicy object at 0x7f5a982ab1d0>}\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 12:58:17,925\tINFO rollout_worker.py:720 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7f5a9834bd68>}\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 12:58:17,925\tINFO rollout_worker.py:333 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7f5a9834bba8>}\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 12:58:18,089\tINFO multi_gpu_optimizer.py:79 -- LocalMultiGPUOptimizer devices ['/cpu:0']\n",
      "\u001b[2m\u001b[36m(pid=94417)\u001b[0m 2019-07-12 12:58:22,008\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 4 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=94417)\u001b[0m 2019-07-12 12:58:22.035596: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=94419)\u001b[0m 2019-07-12 12:58:22,187\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 6 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=94419)\u001b[0m 2019-07-12 12:58:22.210009: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m 2019-07-12 12:58:22,414\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 1 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m 2019-07-12 12:58:22.463327: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=94440)\u001b[0m 2019-07-12 12:58:22,758\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 3 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=94440)\u001b[0m 2019-07-12 12:58:22.803907: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=94418)\u001b[0m 2019-07-12 12:58:22,842\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 2 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=94418)\u001b[0m 2019-07-12 12:58:22.866702: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=94495)\u001b[0m 2019-07-12 12:58:23,047\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 5 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=94495)\u001b[0m 2019-07-12 12:58:23.102021: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m 2019-07-12 12:58:38,894\tINFO dynamic_tf_policy.py:313 -- Initializing loss function with dummy input:\n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m { 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m   'actions': <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m   'behaviour_logits': <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,\n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m   'prev_actions': <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}\n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=94418)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=94418)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=94418)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=94418)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=94419)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=94419)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=94419)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=94419)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=94417)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=94417)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=94417)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=94417)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=94495)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=94495)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=94495)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=94495)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=94440)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=94440)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=94440)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=94440)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m 2019-07-12 12:58:41,789\tINFO rollout_worker.py:428 -- Generating sample batch of size 200\n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m 2019-07-12 12:58:41,808\tINFO sampler.py:308 -- Raw obs from env: { 0: { 'agent0': np.ndarray((5,), dtype=float64, min=0.0, max=1.0, mean=0.2)}}\n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m 2019-07-12 12:58:41,808\tINFO sampler.py:309 -- Info return from env: {0: {'agent0': None}}\n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m 2019-07-12 12:58:41,809\tINFO sampler.py:407 -- Preprocessed obs: np.ndarray((5,), dtype=float64, min=0.0, max=1.0, mean=0.2)\n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m 2019-07-12 12:58:41,809\tINFO sampler.py:411 -- Filtered obs: np.ndarray((5,), dtype=float64, min=0.0, max=1.0, mean=0.2)\n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m 2019-07-12 12:58:41,810\tINFO sampler.py:525 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m                                   'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m                                   'obs': np.ndarray((5,), dtype=float64, min=0.0, max=1.0, mean=0.2),\n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m 2019-07-12 12:58:41,811\tINFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m 2019-07-12 12:58:41,907\tINFO sampler.py:552 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m { 'default_policy': ( np.ndarray((1, 2), dtype=float32, min=0.417, max=0.583, mean=0.5),\n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m                       [],\n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m                       { 'action_prob': np.ndarray((1,), dtype=float32, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m                         'behaviour_logits': np.ndarray((1, 2), dtype=float32, min=0.0, max=0.001, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=0.0, max=0.0, mean=0.0)})}\n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m 2019-07-12 12:58:41,984\tINFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m { 'agent0': { 'data': { 'action_prob': np.ndarray((20,), dtype=float32, min=0.996, max=1.009, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m                         'actions': np.ndarray((20, 2), dtype=float32, min=0.048, max=0.952, mean=0.5),\n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m                         'advantages': np.ndarray((20,), dtype=float32, min=-0.003, max=0.004, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m                         'agent_index': np.ndarray((20,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m                         'behaviour_logits': np.ndarray((20, 2), dtype=float32, min=-0.006, max=0.011, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m                         'dones': np.ndarray((20,), dtype=bool, min=0.0, max=1.0, mean=0.05),\n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m                         'eps_id': np.ndarray((20,), dtype=int64, min=1698914520.0, max=1698914520.0, mean=1698914520.0),\n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m                         'infos': np.ndarray((20,), dtype=object, head={'time': Timestamp('2008-07-09 00:00:00'), 'nlv': 99.8500499750125, 'nr_contracts': np.ndarray((3,), dtype=float64, min=-0.05, max=0.258, mean=0.069), 'bid_prices': np.ndarray((3,), dtype=float64, min=1.0, max=3124.679, mean=1170.971), 'ask_prices': np.ndarray((3,), dtype=float64, min=1.0, max=3127.805, mean=1172.142), 'cost_of_spread': np.ndarray((3,), dtype=float64, min=0.0, max=0.05, mean=0.017), 'cost_of_commissions': np.ndarray((3,), dtype=float64, min=0.0, max=0.05, mean=0.017), 'profit_on_idle_cash': 0.0, 'margin': 0, 'weights_target': np.ndarray((3,), dtype=float64, min=0.0, max=1.0, mean=0.333), 'weights_drift': np.ndarray((3,), dtype=float64, min=0.0, max=1.0, mean=0.333), 'weights_diff': np.ndarray((3,), dtype=float64, min=-1.0, max=1.0, mean=0.0), 'positions_value_diff': np.ndarray((3,), dtype=float64, min=-100.0, max=100.0, mean=0.0), 'acq_prices': np.ndarray((3,), dtype=float64, min=1.0, max=3126.242, mean=1171.621), 'nr_contracts_diff': np.ndarray((3,), dtype=float64, min=-100.0, max=0.258, mean=-33.247), 'nlv_pre': 100.0}),\n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m                         'new_obs': np.ndarray((20, 5), dtype=float32, min=-8.926, max=6.381, mean=0.245),\n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m                         'obs': np.ndarray((20, 5), dtype=float32, min=-8.926, max=6.381, mean=0.228),\n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m                         'prev_actions': np.ndarray((20, 2), dtype=float32, min=0.0, max=0.952, mean=0.475),\n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m                         'prev_rewards': np.ndarray((20,), dtype=float32, min=-0.003, max=0.003, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m                         'rewards': np.ndarray((20,), dtype=float32, min=-0.003, max=0.004, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m                         't': np.ndarray((20,), dtype=int64, min=0.0, max=19.0, mean=9.5),\n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m                         'unroll_id': np.ndarray((20,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m                         'value_targets': np.ndarray((20,), dtype=float64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m                         'vf_preds': np.ndarray((20,), dtype=float32, min=0.0, max=0.0, mean=0.0)},\n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m               'type': 'SampleBatch'}}\n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m 2019-07-12 12:58:43,326\tINFO rollout_worker.py:462 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m { 'data': { 'action_prob': np.ndarray((200,), dtype=float32, min=0.966, max=1.014, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m             'actions': np.ndarray((200, 2), dtype=float32, min=0.004, max=0.996, mean=0.5),\n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m             'advantages': np.ndarray((200,), dtype=float32, min=-0.017, max=0.033, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m             'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m             'behaviour_logits': np.ndarray((200, 2), dtype=float32, min=-0.011, max=0.011, mean=-0.001),\n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m             'dones': np.ndarray((200,), dtype=bool, min=0.0, max=1.0, mean=0.05),\n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m             'eps_id': np.ndarray((200,), dtype=int64, min=235146731.0, max=1964017121.0, mean=1126010807.8),\n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m             'infos': np.ndarray((200,), dtype=object, head={'time': Timestamp('2008-07-09 00:00:00'), 'nlv': 99.8500499750125, 'nr_contracts': np.ndarray((3,), dtype=float64, min=-0.05, max=0.258, mean=0.069), 'bid_prices': np.ndarray((3,), dtype=float64, min=1.0, max=3124.679, mean=1170.971), 'ask_prices': np.ndarray((3,), dtype=float64, min=1.0, max=3127.805, mean=1172.142), 'cost_of_spread': np.ndarray((3,), dtype=float64, min=0.0, max=0.05, mean=0.017), 'cost_of_commissions': np.ndarray((3,), dtype=float64, min=0.0, max=0.05, mean=0.017), 'profit_on_idle_cash': 0.0, 'margin': 0, 'weights_target': np.ndarray((3,), dtype=float64, min=0.0, max=1.0, mean=0.333), 'weights_drift': np.ndarray((3,), dtype=float64, min=0.0, max=1.0, mean=0.333), 'weights_diff': np.ndarray((3,), dtype=float64, min=-1.0, max=1.0, mean=0.0), 'positions_value_diff': np.ndarray((3,), dtype=float64, min=-100.0, max=100.0, mean=0.0), 'acq_prices': np.ndarray((3,), dtype=float64, min=1.0, max=3126.242, mean=1171.621), 'nr_contracts_diff': np.ndarray((3,), dtype=float64, min=-100.0, max=0.258, mean=-33.247), 'nlv_pre': 100.0}),\n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m             'new_obs': np.ndarray((200, 5), dtype=float32, min=-8.926, max=6.381, mean=0.188),\n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m             'obs': np.ndarray((200, 5), dtype=float32, min=-8.926, max=6.381, mean=0.18),\n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m             'prev_actions': np.ndarray((200, 2), dtype=float32, min=0.0, max=0.996, mean=0.475),\n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m             'prev_rewards': np.ndarray((200,), dtype=float32, min=-0.015, max=0.033, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m             'rewards': np.ndarray((200,), dtype=float32, min=-0.017, max=0.033, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m             't': np.ndarray((200,), dtype=int64, min=0.0, max=19.0, mean=9.5),\n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m             'unroll_id': np.ndarray((200,), dtype=int64, min=0.0, max=9.0, mean=4.5),\n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m             'value_targets': np.ndarray((200,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m             'vf_preds': np.ndarray((200,), dtype=float32, min=0.0, max=0.0, mean=0.0)},\n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m   'type': 'SampleBatch'}\n",
      "\u001b[2m\u001b[36m(pid=94472)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 12:58:48,160\tINFO multi_gpu_impl.py:146 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m { 'inputs': [ np.ndarray((4000, 2), dtype=float32, min=0.0, max=1.0, mean=0.475),\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m               np.ndarray((4000,), dtype=float32, min=-0.046, max=0.039, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m               np.ndarray((4000, 5), dtype=float32, min=-8.926, max=11.08, mean=0.254),\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m               np.ndarray((4000, 2), dtype=float32, min=0.0, max=1.0, mean=0.5),\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m               np.ndarray((4000,), dtype=float32, min=-9.763, max=9.958, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m               np.ndarray((4000, 2), dtype=float32, min=-0.015, max=0.011, mean=-0.001),\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m               np.ndarray((4000,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m               np.ndarray((4000,), dtype=float32, min=0.0, max=0.0, mean=0.0)],\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m   'placeholders': [ <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m                     <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m                     <tf.Tensor 'default_policy/observation:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m                     <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m                     <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m                     <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m                     <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m                     <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>],\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m   'state_inputs': []}\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 12:58:48,160\tINFO multi_gpu_impl.py:191 -- Divided 4000 rollout sequences, each of length 1, among 1 devices.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 12:58:50,065\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 12:58:51,211\tINFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 12:59:00,909\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 12:59:10,580\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 12:59:19,391\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 12:59:29,542\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 12:59:40,349\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 12:59:50,346\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 12:59:59,806\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:00:10,639\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:00:19,717\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:00:29,157\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:00:40,228\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:00:49,632\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:00:58,970\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:01:10,208\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:01:19,439\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:01:28,647\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:01:39,634\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:01:49,318\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:01:58,563\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:02:09,019\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:02:19,192\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:02:27,983\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:02:38,569\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:02:48,323\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:02:57,529\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:03:07,930\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:03:17,675\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:03:26,722\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:03:36,683\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:03:47,459\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:03:56,978\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:04:07,474\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:04:18,492\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:04:27,989\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:04:38,596\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:04:49,933\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:05:00,215\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:05:10,678\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:05:21,729\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:05:31,375\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:05:42,166\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:05:53,642\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:06:03,318\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:06:14,437\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:06:25,755\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:06:35,912\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:06:47,435\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:06:58,176\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:07:07,578\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:07:17,883\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:07:27,632\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:07:36,446\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:07:46,595\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:07:56,423\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:08:05,327\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:08:15,057\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:08:25,357\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:08:34,220\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:08:43,753\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:08:54,358\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:09:03,927\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:09:13,547\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:09:24,087\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:09:33,662\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:09:43,764\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:09:55,131\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:10:05,016\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:10:15,321\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:10:26,106\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:10:35,479\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:10:44,152\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:10:54,105\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:11:03,856\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:11:12,591\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:11:22,311\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:11:32,727\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:11:41,349\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:11:50,816\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:12:01,250\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:12:10,124\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:12:19,057\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:12:29,766\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:12:39,301\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:12:48,122\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:12:58,313\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:13:08,156\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:13:16,961\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:13:26,867\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:13:36,639\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:13:45,515\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:13:54,918\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:14:05,258\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:14:14,203\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:14:23,255\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:14:33,810\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:14:43,434\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:14:52,189\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:15:02,698\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:15:12,198\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:15:21,192\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:15:31,207\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:15:41,046\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:15:49,960\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:15:59,157\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:16:09,658\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:16:18,431\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:16:27,802\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:16:38,557\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:16:48,010\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:16:57,113\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:17:07,749\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:17:17,003\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:17:26,084\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:17:35,984\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:17:45,806\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:17:54,623\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:18:04,353\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:18:14,704\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:18:23,603\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:18:33,246\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:18:43,831\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:18:52,905\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:19:01,828\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:19:12,625\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:19:21,936\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:19:30,770\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:19:41,606\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:19:51,268\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:19:59,733\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:20:09,889\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:20:19,742\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:20:28,598\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:20:38,080\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:20:48,560\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:20:57,337\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:21:06,414\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:21:17,433\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:21:26,969\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:21:36,055\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:21:46,540\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:21:56,531\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:22:05,432\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:22:16,202\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:22:25,987\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:22:34,887\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:22:44,945\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:22:54,797\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:23:03,510\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:23:12,826\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:23:23,263\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:23:32,492\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:23:41,597\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:23:52,511\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:24:02,263\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:24:11,109\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:24:21,660\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:24:31,127\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:24:40,167\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:24:50,559\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:25:00,203\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:25:09,218\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:25:18,762\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:25:29,145\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:25:38,027\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:25:47,255\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:25:57,837\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:26:07,411\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:26:16,826\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:26:27,562\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:26:36,979\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:26:46,123\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:26:56,389\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:27:06,057\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:27:14,854\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:27:25,080\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:27:35,059\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:27:44,114\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:27:54,447\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:28:04,728\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:28:13,465\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:28:23,406\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:28:33,899\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:28:42,708\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:28:51,876\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:29:02,933\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:29:12,548\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94492)\u001b[0m 2019-07-12 13:29:21,544\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-12 13:29:24,375\tINFO ray_trial_executor.py:187 -- Destroying actor for trial PPO_GAIAPredictorsContinuousV8_0_clip_param=0.8,entropy_coeff=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2019-07-12 13:29:24,474\tINFO tune.py:65 -- Did not find checkpoint file in logs/tran_cost_x10/clip_0.8-tc-WalkForward-750k2012.\n",
      "2019-07-12 13:29:24,475\tINFO tune.py:233 -- Starting a new experiment.\n",
      "2019-07-12 13:29:24,496\tWARNING signature.py:108 -- The function with_updates has a **kwargs argument, which is currently not supported.\n",
      "2019-07-12 13:29:24,628\tWARNING util.py:64 -- The `start_trial` operation took 0.14337944984436035 seconds to complete, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_______________________________________2012____________________________________________\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:29:26,813\tWARNING ppo.py:151 -- FYI: By default, the value function will not share layers with the policy model ('vf_share_layers': False).\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:29:27,997\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 0 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:29:27.997660: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:29:35,513\tINFO dynamic_tf_policy.py:313 -- Initializing loss function with dummy input:\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m { 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m   'actions': <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m   'behaviour_logits': <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m   'prev_actions': <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:29:37,171\tINFO rollout_worker.py:719 -- Built policy map: {'default_policy': <ray.rllib.policy.tf_policy_template.PPOTFPolicy object at 0x7f9695f5b1d0>}\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:29:37,171\tINFO rollout_worker.py:720 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7f9695ffbd68>}\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:29:37,171\tINFO rollout_worker.py:333 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7f9695ffbba8>}\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:29:37,415\tINFO multi_gpu_optimizer.py:79 -- LocalMultiGPUOptimizer devices ['/cpu:0']\n",
      "\u001b[2m\u001b[36m(pid=125803)\u001b[0m 2019-07-12 13:29:50,916\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 6 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=125803)\u001b[0m 2019-07-12 13:29:50.943874: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m 2019-07-12 13:29:51,202\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 1 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m 2019-07-12 13:29:51.228693: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=125787)\u001b[0m 2019-07-12 13:29:51,527\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 3 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=125787)\u001b[0m 2019-07-12 13:29:51.552905: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=125795)\u001b[0m 2019-07-12 13:29:51,761\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 2 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=125795)\u001b[0m 2019-07-12 13:29:51.785064: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=125785)\u001b[0m 2019-07-12 13:29:51,854\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 5 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=125785)\u001b[0m 2019-07-12 13:29:51.910730: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=125804)\u001b[0m 2019-07-12 13:29:51,982\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 4 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=125804)\u001b[0m 2019-07-12 13:29:52.042413: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m 2019-07-12 13:30:05,735\tINFO dynamic_tf_policy.py:313 -- Initializing loss function with dummy input:\n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m { 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m   'actions': <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m   'behaviour_logits': <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,\n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m   'prev_actions': <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}\n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=125803)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=125803)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=125803)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=125803)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=125795)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=125795)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=125795)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=125795)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=125804)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=125804)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=125804)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=125804)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=125785)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=125785)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=125785)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=125785)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=125787)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=125787)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=125787)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=125787)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m 2019-07-12 13:30:09,326\tINFO rollout_worker.py:428 -- Generating sample batch of size 200\n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m 2019-07-12 13:30:09,358\tINFO sampler.py:308 -- Raw obs from env: { 0: { 'agent0': np.ndarray((5,), dtype=float64, min=0.0, max=1.0, mean=0.2)}}\n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m 2019-07-12 13:30:09,360\tINFO sampler.py:309 -- Info return from env: {0: {'agent0': None}}\n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m 2019-07-12 13:30:09,360\tINFO sampler.py:407 -- Preprocessed obs: np.ndarray((5,), dtype=float64, min=0.0, max=1.0, mean=0.2)\n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m 2019-07-12 13:30:09,360\tINFO sampler.py:411 -- Filtered obs: np.ndarray((5,), dtype=float64, min=0.0, max=1.0, mean=0.2)\n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m 2019-07-12 13:30:09,362\tINFO sampler.py:525 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m                                   'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m                                   'obs': np.ndarray((5,), dtype=float64, min=0.0, max=1.0, mean=0.2),\n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m 2019-07-12 13:30:09,362\tINFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m 2019-07-12 13:30:09,484\tINFO sampler.py:552 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m { 'default_policy': ( np.ndarray((1, 2), dtype=float32, min=0.269, max=0.731, mean=0.5),\n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m                       [],\n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m                       { 'action_prob': np.ndarray((1,), dtype=float32, min=1.001, max=1.001, mean=1.001),\n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m                         'behaviour_logits': np.ndarray((1, 2), dtype=float32, min=-0.001, max=0.001, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=0.0, max=0.0, mean=0.0)})}\n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m 2019-07-12 13:30:09,624\tINFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m { 'agent0': { 'data': { 'action_prob': np.ndarray((20,), dtype=float32, min=0.993, max=1.016, mean=1.001),\n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m                         'actions': np.ndarray((20, 2), dtype=float32, min=0.02, max=0.98, mean=0.5),\n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m                         'advantages': np.ndarray((20,), dtype=float32, min=-0.005, max=0.0, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m                         'agent_index': np.ndarray((20,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m                         'behaviour_logits': np.ndarray((20, 2), dtype=float32, min=-0.007, max=0.009, mean=0.003),\n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m                         'dones': np.ndarray((20,), dtype=bool, min=0.0, max=1.0, mean=0.05),\n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m                         'eps_id': np.ndarray((20,), dtype=int64, min=1683414021.0, max=1683414021.0, mean=1683414021.0),\n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m                         'infos': np.ndarray((20,), dtype=object, head={'time': Timestamp('2008-07-08 00:00:00'), 'nlv': 99.8500499750125, 'nr_contracts': np.ndarray((3,), dtype=float64, min=-0.05, max=0.259, mean=0.07), 'bid_prices': np.ndarray((3,), dtype=float64, min=1.0, max=3193.482, mean=1193.472), 'ask_prices': np.ndarray((3,), dtype=float64, min=1.0, max=3196.678, mean=1194.666), 'cost_of_spread': np.ndarray((3,), dtype=float64, min=0.0, max=0.05, mean=0.017), 'cost_of_commissions': np.ndarray((3,), dtype=float64, min=0.0, max=0.05, mean=0.017), 'profit_on_idle_cash': 0.0, 'margin': 0, 'weights_target': np.ndarray((3,), dtype=float64, min=0.0, max=1.0, mean=0.333), 'weights_drift': np.ndarray((3,), dtype=float64, min=0.0, max=1.0, mean=0.333), 'weights_diff': np.ndarray((3,), dtype=float64, min=-1.0, max=1.0, mean=0.0), 'positions_value_diff': np.ndarray((3,), dtype=float64, min=-100.0, max=100.0, mean=0.0), 'acq_prices': np.ndarray((3,), dtype=float64, min=1.0, max=3195.08, mean=1194.134), 'nr_contracts_diff': np.ndarray((3,), dtype=float64, min=-100.0, max=0.259, mean=-33.247), 'nlv_pre': 100.0}),\n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m                         'new_obs': np.ndarray((20, 5), dtype=float32, min=-8.926, max=6.381, mean=0.238),\n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m                         'obs': np.ndarray((20, 5), dtype=float32, min=-8.926, max=6.381, mean=0.231),\n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m                         'prev_actions': np.ndarray((20, 2), dtype=float32, min=0.0, max=0.98, mean=0.475),\n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m                         'prev_rewards': np.ndarray((20,), dtype=float32, min=-0.005, max=0.0, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m                         'rewards': np.ndarray((20,), dtype=float32, min=-0.005, max=0.0, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m                         't': np.ndarray((20,), dtype=int64, min=0.0, max=19.0, mean=9.5),\n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m                         'unroll_id': np.ndarray((20,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m                         'value_targets': np.ndarray((20,), dtype=float64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m                         'vf_preds': np.ndarray((20,), dtype=float32, min=0.0, max=0.0, mean=0.0)},\n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m               'type': 'SampleBatch'}}\n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m 2019-07-12 13:30:11,100\tINFO rollout_worker.py:462 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m { 'data': { 'action_prob': np.ndarray((200,), dtype=float32, min=0.981, max=1.017, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m             'actions': np.ndarray((200, 2), dtype=float32, min=0.0, max=1.0, mean=0.5),\n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m             'advantages': np.ndarray((200,), dtype=float32, min=-0.018, max=0.024, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m             'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m             'behaviour_logits': np.ndarray((200, 2), dtype=float32, min=-0.008, max=0.009, mean=0.003),\n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m             'dones': np.ndarray((200,), dtype=bool, min=0.0, max=1.0, mean=0.05),\n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m             'eps_id': np.ndarray((200,), dtype=int64, min=363699786.0, max=1959191122.0, mean=1354585258.7),\n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m             'infos': np.ndarray((200,), dtype=object, head={'time': Timestamp('2008-07-08 00:00:00'), 'nlv': 99.8500499750125, 'nr_contracts': np.ndarray((3,), dtype=float64, min=-0.05, max=0.259, mean=0.07), 'bid_prices': np.ndarray((3,), dtype=float64, min=1.0, max=3193.482, mean=1193.472), 'ask_prices': np.ndarray((3,), dtype=float64, min=1.0, max=3196.678, mean=1194.666), 'cost_of_spread': np.ndarray((3,), dtype=float64, min=0.0, max=0.05, mean=0.017), 'cost_of_commissions': np.ndarray((3,), dtype=float64, min=0.0, max=0.05, mean=0.017), 'profit_on_idle_cash': 0.0, 'margin': 0, 'weights_target': np.ndarray((3,), dtype=float64, min=0.0, max=1.0, mean=0.333), 'weights_drift': np.ndarray((3,), dtype=float64, min=0.0, max=1.0, mean=0.333), 'weights_diff': np.ndarray((3,), dtype=float64, min=-1.0, max=1.0, mean=0.0), 'positions_value_diff': np.ndarray((3,), dtype=float64, min=-100.0, max=100.0, mean=0.0), 'acq_prices': np.ndarray((3,), dtype=float64, min=1.0, max=3195.08, mean=1194.134), 'nr_contracts_diff': np.ndarray((3,), dtype=float64, min=-100.0, max=0.259, mean=-33.247), 'nlv_pre': 100.0}),\n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m             'new_obs': np.ndarray((200, 5), dtype=float32, min=-8.926, max=6.381, mean=0.201),\n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m             'obs': np.ndarray((200, 5), dtype=float32, min=-8.926, max=13.391, mean=0.205),\n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m             'prev_actions': np.ndarray((200, 2), dtype=float32, min=0.0, max=1.0, mean=0.475),\n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m             'prev_rewards': np.ndarray((200,), dtype=float32, min=-0.018, max=0.024, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m             'rewards': np.ndarray((200,), dtype=float32, min=-0.018, max=0.024, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m             't': np.ndarray((200,), dtype=int64, min=0.0, max=19.0, mean=9.5),\n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m             'unroll_id': np.ndarray((200,), dtype=int64, min=0.0, max=9.0, mean=4.5),\n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m             'value_targets': np.ndarray((200,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m             'vf_preds': np.ndarray((200,), dtype=float32, min=0.0, max=0.0, mean=0.0)},\n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m   'type': 'SampleBatch'}\n",
      "\u001b[2m\u001b[36m(pid=125786)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:30:15,759\tINFO multi_gpu_impl.py:146 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m { 'inputs': [ np.ndarray((4000, 2), dtype=float32, min=0.0, max=1.0, mean=0.475),\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m               np.ndarray((4000,), dtype=float32, min=-0.033, max=0.031, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m               np.ndarray((4000, 5), dtype=float32, min=-14.063, max=13.391, mean=0.239),\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m               np.ndarray((4000, 2), dtype=float32, min=0.0, max=1.0, mean=0.5),\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m               np.ndarray((4000,), dtype=float32, min=-7.193, max=6.724, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m               np.ndarray((4000, 2), dtype=float32, min=-0.012, max=0.011, mean=0.003),\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m               np.ndarray((4000,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m               np.ndarray((4000,), dtype=float32, min=0.0, max=0.0, mean=0.0)],\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m   'placeholders': [ <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m                     <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m                     <tf.Tensor 'default_policy/observation:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m                     <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m                     <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m                     <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m                     <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m                     <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>],\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m   'state_inputs': []}\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:30:15,759\tINFO multi_gpu_impl.py:191 -- Divided 4000 rollout sequences, each of length 1, among 1 devices.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:30:17,629\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:30:18,920\tINFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:30:27,563\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:30:38,774\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:30:48,370\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:30:58,027\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:31:08,848\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:31:18,822\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:31:28,218\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:31:39,165\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:31:49,116\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:31:59,394\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:32:11,955\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:32:22,697\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:32:33,397\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:32:45,322\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:32:55,829\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:33:07,398\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:33:18,699\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:33:28,554\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:33:39,790\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:33:49,925\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:33:59,503\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:34:09,734\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:34:20,418\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:34:29,733\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:34:41,734\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:34:52,025\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:35:00,886\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:35:11,530\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:35:21,239\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:35:30,333\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:35:40,384\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:35:50,776\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:35:59,923\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:36:09,569\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:36:20,289\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:36:29,571\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:36:38,854\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:36:50,095\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:37:00,418\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:37:10,349\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:37:21,429\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:37:31,614\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:37:41,967\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:37:53,279\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:38:03,259\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:38:12,871\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:38:24,400\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:38:34,334\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:38:44,049\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:38:54,847\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:39:04,183\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:39:14,064\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:39:24,816\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:39:34,515\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:39:44,146\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:39:55,207\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:40:04,564\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:40:13,569\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:40:24,566\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:40:35,680\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:40:44,859\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:40:55,971\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:41:05,678\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:41:14,727\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:41:25,722\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:41:36,064\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:41:45,132\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:41:56,162\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:42:06,294\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:42:15,669\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:42:26,595\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:42:37,445\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:42:47,321\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:42:57,942\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:43:08,148\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:43:17,532\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:43:27,878\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:43:38,259\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:43:48,432\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:43:58,802\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:44:08,924\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:44:18,238\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:44:28,162\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:44:38,027\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:44:47,077\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:44:56,912\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:45:06,622\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:45:16,096\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:45:25,346\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:45:35,183\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:45:45,406\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:45:54,572\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:46:04,653\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:46:15,034\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:46:24,369\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:46:35,153\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:46:45,312\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:46:54,291\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:47:03,794\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:47:13,640\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:47:23,584\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:47:33,731\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:47:43,708\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:47:52,996\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:48:02,878\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:48:13,010\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:48:21,869\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:48:31,289\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:48:41,888\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:48:51,618\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:49:00,914\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:49:11,475\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:49:21,184\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:49:30,125\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:49:40,919\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:49:50,590\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:49:59,949\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:50:10,459\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:50:20,079\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:50:29,360\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:50:39,943\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:50:50,024\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:50:59,131\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:51:10,013\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:51:20,285\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:51:29,907\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:51:40,891\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:51:51,463\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:52:01,220\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:52:12,209\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:52:22,622\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:52:32,150\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:52:42,393\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:52:52,766\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:53:02,270\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:53:12,113\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:53:22,992\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:53:32,406\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:53:42,050\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:53:52,905\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:54:02,651\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:54:12,512\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:54:23,044\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:54:32,872\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:54:42,320\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:54:53,307\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:55:03,215\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:55:12,612\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:55:23,616\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:55:33,675\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:55:43,531\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:55:53,425\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:56:03,876\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:56:13,713\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:56:24,646\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:56:34,472\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:56:43,873\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:56:55,418\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:57:05,730\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:57:15,237\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:57:26,248\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:57:36,024\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:57:45,294\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:57:56,368\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:58:06,375\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:58:15,755\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:58:26,689\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:58:36,973\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:58:46,400\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:58:57,250\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:59:07,309\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:59:16,528\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:59:27,068\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:59:37,335\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:59:46,903\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 13:59:56,844\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 14:00:07,667\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 14:00:17,441\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 14:00:27,828\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 14:00:37,962\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 14:00:47,541\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 14:00:57,889\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 14:01:07,988\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 14:01:17,399\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 14:01:27,553\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 14:01:38,173\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=94422)\u001b[0m 2019-07-12 14:01:47,506\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-12 14:01:50,242\tINFO ray_trial_executor.py:187 -- Destroying actor for trial PPO_GAIAPredictorsContinuousV8_0_clip_param=0.8,entropy_coeff=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2019-07-12 14:01:50,291\tINFO tune.py:65 -- Did not find checkpoint file in logs/tran_cost_x10/clip_0.8-tc-WalkForward-750k2013.\n",
      "2019-07-12 14:01:50,292\tINFO tune.py:233 -- Starting a new experiment.\n",
      "2019-07-12 14:01:50,307\tWARNING signature.py:108 -- The function with_updates has a **kwargs argument, which is currently not supported.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_______________________________________2013____________________________________________\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:01:52,612\tWARNING ppo.py:151 -- FYI: By default, the value function will not share layers with the policy model ('vf_share_layers': False).\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:01:53,583\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 0 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:01:53.584318: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:02:01,363\tINFO dynamic_tf_policy.py:313 -- Initializing loss function with dummy input:\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m { 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m   'actions': <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m   'behaviour_logits': <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m   'prev_actions': <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:02:02,872\tINFO rollout_worker.py:719 -- Built policy map: {'default_policy': <ray.rllib.policy.tf_policy_template.PPOTFPolicy object at 0x7f10c33a3160>}\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:02:02,872\tINFO rollout_worker.py:720 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7f10c3445cf8>}\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:02:02,872\tINFO rollout_worker.py:333 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7f10c33a3048>}\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:02:03,043\tINFO multi_gpu_optimizer.py:79 -- LocalMultiGPUOptimizer devices ['/cpu:0']\n",
      "\u001b[2m\u001b[36m(pid=125860)\u001b[0m 2019-07-12 14:02:07,041\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 5 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=125860)\u001b[0m 2019-07-12 14:02:07.066008: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=125888)\u001b[0m 2019-07-12 14:02:07,047\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 2 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=125888)\u001b[0m 2019-07-12 14:02:07.070502: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m 2019-07-12 14:02:07,394\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 1 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m 2019-07-12 14:02:07.426349: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=125850)\u001b[0m 2019-07-12 14:02:07,507\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 6 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=125850)\u001b[0m 2019-07-12 14:02:07.528628: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=125847)\u001b[0m 2019-07-12 14:02:07,547\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 4 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=125847)\u001b[0m 2019-07-12 14:02:07.576285: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=125896)\u001b[0m 2019-07-12 14:02:07,594\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 3 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=125896)\u001b[0m 2019-07-12 14:02:07.639843: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=125850)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=125850)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=125850)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=125850)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m 2019-07-12 14:02:22,064\tINFO dynamic_tf_policy.py:313 -- Initializing loss function with dummy input:\n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m { 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m   'actions': <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m   'behaviour_logits': <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,\n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m   'prev_actions': <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}\n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=125888)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=125888)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=125888)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=125888)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=125860)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=125860)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=125860)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=125860)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=125847)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=125847)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=125847)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=125847)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=125896)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=125896)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=125896)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=125896)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m 2019-07-12 14:02:24,897\tINFO rollout_worker.py:428 -- Generating sample batch of size 200\n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m 2019-07-12 14:02:24,949\tINFO sampler.py:308 -- Raw obs from env: { 0: { 'agent0': np.ndarray((5,), dtype=float64, min=-1.0, max=1.573, mean=0.264)}}\n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m 2019-07-12 14:02:24,949\tINFO sampler.py:309 -- Info return from env: {0: {'agent0': None}}\n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m 2019-07-12 14:02:24,949\tINFO sampler.py:407 -- Preprocessed obs: np.ndarray((5,), dtype=float64, min=-1.0, max=1.573, mean=0.264)\n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m 2019-07-12 14:02:24,950\tINFO sampler.py:411 -- Filtered obs: np.ndarray((5,), dtype=float64, min=-1.0, max=1.573, mean=0.264)\n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m 2019-07-12 14:02:24,951\tINFO sampler.py:525 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m                                   'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m                                   'obs': np.ndarray((5,), dtype=float64, min=-1.0, max=1.573, mean=0.264),\n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m 2019-07-12 14:02:24,956\tINFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m 2019-07-12 14:02:25,111\tINFO sampler.py:552 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m { 'default_policy': ( np.ndarray((1, 2), dtype=float32, min=0.132, max=0.868, mean=0.5),\n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m                       [],\n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m                       { 'action_prob': np.ndarray((1,), dtype=float32, min=1.007, max=1.007, mean=1.007),\n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m                         'behaviour_logits': np.ndarray((1, 2), dtype=float32, min=-0.009, max=-0.002, mean=-0.005),\n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=0.0, max=0.0, mean=0.0)})}\n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m 2019-07-12 14:02:25,230\tINFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m { 'agent0': { 'data': { 'action_prob': np.ndarray((20,), dtype=float32, min=0.992, max=1.009, mean=1.002),\n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m                         'actions': np.ndarray((20, 2), dtype=float32, min=0.009, max=0.991, mean=0.5),\n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m                         'advantages': np.ndarray((20,), dtype=float32, min=-0.012, max=0.008, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m                         'agent_index': np.ndarray((20,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m                         'behaviour_logits': np.ndarray((20, 2), dtype=float32, min=-0.009, max=0.003, mean=-0.001),\n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m                         'dones': np.ndarray((20,), dtype=bool, min=0.0, max=1.0, mean=0.05),\n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m                         'eps_id': np.ndarray((20,), dtype=int64, min=186238.0, max=186238.0, mean=186238.0),\n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m                         'infos': np.ndarray((20,), dtype=object, head={'time': Timestamp('2007-06-25 00:00:00'), 'nlv': 99.85004997501248, 'nr_contracts': np.ndarray((3,), dtype=float64, min=-0.05, max=0.039, mean=0.004), 'bid_prices': np.ndarray((3,), dtype=float64, min=1.0, max=3660.19, mean=1333.088), 'ask_prices': np.ndarray((3,), dtype=float64, min=1.0, max=3663.852, mean=1334.421), 'cost_of_spread': np.ndarray((3,), dtype=float64, min=0.0, max=0.043, mean=0.017), 'cost_of_commissions': np.ndarray((3,), dtype=float64, min=0.0, max=0.043, mean=0.017), 'profit_on_idle_cash': 0.0, 'margin': 0, 'weights_target': np.ndarray((3,), dtype=float32, min=0.0, max=0.868, mean=0.333), 'weights_drift': np.ndarray((3,), dtype=float64, min=0.0, max=1.0, mean=0.333), 'weights_diff': np.ndarray((3,), dtype=float64, min=-1.0, max=0.868, mean=0.0), 'positions_value_diff': np.ndarray((3,), dtype=float64, min=-100.0, max=86.785, mean=0.0), 'acq_prices': np.ndarray((3,), dtype=float64, min=1.0, max=3663.852, mean=1334.421), 'nr_contracts_diff': np.ndarray((3,), dtype=float64, min=-100.0, max=0.039, mean=-33.312), 'nlv_pre': 100.0}),\n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m                         'new_obs': np.ndarray((20, 5), dtype=float32, min=-1.277, max=1.414, mean=0.16),\n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m                         'obs': np.ndarray((20, 5), dtype=float32, min=-1.277, max=1.573, mean=0.165),\n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m                         'prev_actions': np.ndarray((20, 2), dtype=float32, min=0.0, max=0.991, mean=0.475),\n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m                         'prev_rewards': np.ndarray((20,), dtype=float32, min=-0.012, max=0.008, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m                         'rewards': np.ndarray((20,), dtype=float32, min=-0.012, max=0.008, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m                         't': np.ndarray((20,), dtype=int64, min=0.0, max=19.0, mean=9.5),\n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m                         'unroll_id': np.ndarray((20,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m                         'value_targets': np.ndarray((20,), dtype=float64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m                         'vf_preds': np.ndarray((20,), dtype=float32, min=0.0, max=0.0, mean=0.0)},\n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m               'type': 'SampleBatch'}}\n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m 2019-07-12 14:02:26,601\tINFO rollout_worker.py:462 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m { 'data': { 'action_prob': np.ndarray((200,), dtype=float32, min=0.987, max=1.042, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m             'actions': np.ndarray((200, 2), dtype=float32, min=0.001, max=0.999, mean=0.5),\n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m             'advantages': np.ndarray((200,), dtype=float32, min=-0.018, max=0.014, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m             'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m             'behaviour_logits': np.ndarray((200, 2), dtype=float32, min=-0.009, max=0.006, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m             'dones': np.ndarray((200,), dtype=bool, min=0.0, max=1.0, mean=0.05),\n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m             'eps_id': np.ndarray((200,), dtype=int64, min=186238.0, max=1872936753.0, mean=776889868.2),\n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m             'infos': np.ndarray((200,), dtype=object, head={'time': Timestamp('2007-06-25 00:00:00'), 'nlv': 99.85004997501248, 'nr_contracts': np.ndarray((3,), dtype=float64, min=-0.05, max=0.039, mean=0.004), 'bid_prices': np.ndarray((3,), dtype=float64, min=1.0, max=3660.19, mean=1333.088), 'ask_prices': np.ndarray((3,), dtype=float64, min=1.0, max=3663.852, mean=1334.421), 'cost_of_spread': np.ndarray((3,), dtype=float64, min=0.0, max=0.043, mean=0.017), 'cost_of_commissions': np.ndarray((3,), dtype=float64, min=0.0, max=0.043, mean=0.017), 'profit_on_idle_cash': 0.0, 'margin': 0, 'weights_target': np.ndarray((3,), dtype=float32, min=0.0, max=0.868, mean=0.333), 'weights_drift': np.ndarray((3,), dtype=float64, min=0.0, max=1.0, mean=0.333), 'weights_diff': np.ndarray((3,), dtype=float64, min=-1.0, max=0.868, mean=0.0), 'positions_value_diff': np.ndarray((3,), dtype=float64, min=-100.0, max=86.785, mean=0.0), 'acq_prices': np.ndarray((3,), dtype=float64, min=1.0, max=3663.852, mean=1334.421), 'nr_contracts_diff': np.ndarray((3,), dtype=float64, min=-100.0, max=0.039, mean=-33.312), 'nlv_pre': 100.0}),\n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m             'new_obs': np.ndarray((200, 5), dtype=float32, min=-3.09, max=1.909, mean=0.186),\n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m             'obs': np.ndarray((200, 5), dtype=float32, min=-3.09, max=3.455, mean=0.193),\n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m             'prev_actions': np.ndarray((200, 2), dtype=float32, min=0.0, max=0.999, mean=0.475),\n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m             'prev_rewards': np.ndarray((200,), dtype=float32, min=-0.018, max=0.014, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m             'rewards': np.ndarray((200,), dtype=float32, min=-0.018, max=0.014, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m             't': np.ndarray((200,), dtype=int64, min=0.0, max=19.0, mean=9.5),\n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m             'unroll_id': np.ndarray((200,), dtype=int64, min=0.0, max=9.0, mean=4.5),\n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m             'value_targets': np.ndarray((200,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m             'vf_preds': np.ndarray((200,), dtype=float32, min=0.0, max=0.0, mean=0.0)},\n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m   'type': 'SampleBatch'}\n",
      "\u001b[2m\u001b[36m(pid=125870)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:02:30,318\tINFO multi_gpu_impl.py:146 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m { 'inputs': [ np.ndarray((4000, 2), dtype=float32, min=0.0, max=1.0, mean=0.475),\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m               np.ndarray((4000,), dtype=float32, min=-0.051, max=0.042, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m               np.ndarray((4000, 5), dtype=float32, min=-14.063, max=10.774, mean=0.227),\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m               np.ndarray((4000, 2), dtype=float32, min=0.0, max=1.0, mean=0.5),\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m               np.ndarray((4000,), dtype=float32, min=-10.018, max=8.171, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m               np.ndarray((4000, 2), dtype=float32, min=-0.013, max=0.013, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m               np.ndarray((4000,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m               np.ndarray((4000,), dtype=float32, min=0.0, max=0.0, mean=0.0)],\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m   'placeholders': [ <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m                     <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m                     <tf.Tensor 'default_policy/observation:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m                     <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m                     <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m                     <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m                     <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m                     <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>],\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m   'state_inputs': []}\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:02:30,318\tINFO multi_gpu_impl.py:191 -- Divided 4000 rollout sequences, each of length 1, among 1 devices.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:02:32,508\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:02:33,831\tINFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:02:42,940\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:02:52,666\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:03:02,848\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:03:13,256\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:03:23,525\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:03:32,861\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:03:43,580\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:03:54,179\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:04:05,925\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:04:17,501\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:04:29,656\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:04:43,752\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:04:56,711\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:05:09,009\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:05:24,162\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:05:36,947\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:05:49,152\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:05:59,012\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:06:09,637\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:06:24,135\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:06:35,234\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:06:48,082\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:06:58,453\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:07:07,754\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:07:20,210\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:07:33,186\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:07:45,893\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:08:03,941\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:08:16,723\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:08:27,155\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:08:39,613\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:08:52,618\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:09:03,875\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:09:14,563\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:09:25,026\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:09:35,321\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:09:46,494\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:09:57,499\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:10:07,919\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:10:18,676\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:10:28,665\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:10:39,507\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:10:49,650\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:10:59,836\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:11:09,830\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:11:19,741\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:11:29,766\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:11:39,631\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:11:50,297\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:12:00,215\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:12:09,713\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:12:20,373\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:12:30,951\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:12:40,704\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:12:51,030\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:13:00,667\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:13:10,443\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:13:20,570\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:13:29,980\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:13:39,824\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:13:49,736\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:13:58,877\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:14:08,656\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:14:18,803\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:14:27,586\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:14:37,710\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:14:48,045\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:14:57,314\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:15:07,228\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:15:16,933\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:15:26,235\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:15:35,979\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:15:46,103\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:15:55,949\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:16:05,419\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:16:15,601\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:16:25,672\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:16:35,298\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:16:45,075\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:16:54,838\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:17:03,593\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:17:13,224\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:17:23,203\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:17:32,143\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:17:41,645\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:17:51,548\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:18:00,984\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:18:10,275\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:18:20,115\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:18:30,266\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:18:39,209\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:18:48,919\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:18:59,068\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:19:07,972\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:19:17,406\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:19:27,560\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:19:36,523\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:19:45,904\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:19:55,595\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:20:05,212\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:20:14,383\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:20:23,825\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:20:33,899\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:20:43,071\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:20:52,652\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:21:02,730\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:21:12,561\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:21:22,664\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:21:33,677\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:21:43,703\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:21:54,486\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:22:05,638\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:22:15,743\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:22:26,378\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:22:37,107\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:22:47,039\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:22:57,779\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:23:08,860\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:23:18,693\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:23:29,451\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:23:40,214\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:23:50,091\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:24:00,559\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:24:11,470\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:24:21,252\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:24:31,798\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:24:42,921\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:24:53,012\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:25:03,515\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:25:14,571\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:25:24,582\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:25:35,689\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:25:46,160\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:25:56,189\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:26:06,991\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:26:17,832\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:26:28,177\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:26:38,682\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:26:49,015\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:26:59,137\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:27:09,818\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:27:20,348\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:27:30,580\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:27:41,435\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:27:51,772\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:28:02,102\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:28:13,142\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:28:22,986\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:28:33,417\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:28:44,057\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:28:54,440\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:29:04,689\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:29:15,501\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:29:25,623\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:29:36,324\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:29:46,938\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:29:57,560\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:30:08,177\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:30:18,787\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:30:28,662\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:30:39,616\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:30:50,416\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:30:59,953\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:31:10,630\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:31:21,361\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:31:31,098\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:31:41,346\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:31:52,173\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:32:01,874\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:32:12,241\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:32:23,288\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:32:32,902\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:32:43,209\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:32:54,378\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:33:04,575\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:33:15,090\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:33:25,632\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:33:35,411\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:33:45,775\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:33:56,353\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:34:06,153\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:34:16,519\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:34:26,958\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:34:36,639\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:34:47,565\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:34:58,341\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125889)\u001b[0m 2019-07-12 14:35:09,589\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-12 14:35:12,722\tINFO ray_trial_executor.py:187 -- Destroying actor for trial PPO_GAIAPredictorsContinuousV8_0_clip_param=0.8,entropy_coeff=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2019-07-12 14:35:12,777\tINFO tune.py:65 -- Did not find checkpoint file in logs/tran_cost_x10/clip_0.8-tc-WalkForward-750k2014.\n",
      "2019-07-12 14:35:12,779\tINFO tune.py:233 -- Starting a new experiment.\n",
      "2019-07-12 14:35:12,795\tWARNING signature.py:108 -- The function with_updates has a **kwargs argument, which is currently not supported.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_______________________________________2014____________________________________________\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:35:15,160\tWARNING ppo.py:151 -- FYI: By default, the value function will not share layers with the policy model ('vf_share_layers': False).\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:35:16,287\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 0 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:35:16.288268: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:35:23,767\tINFO dynamic_tf_policy.py:313 -- Initializing loss function with dummy input:\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m { 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m   'actions': <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m   'behaviour_logits': <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m   'prev_actions': <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:35:25,510\tINFO rollout_worker.py:719 -- Built policy map: {'default_policy': <ray.rllib.policy.tf_policy_template.PPOTFPolicy object at 0x7f9abb661160>}\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:35:25,510\tINFO rollout_worker.py:720 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7f9abb703cf8>}\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:35:25,511\tINFO rollout_worker.py:333 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7f9abb661048>}\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:35:25,752\tINFO multi_gpu_optimizer.py:79 -- LocalMultiGPUOptimizer devices ['/cpu:0']\n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m 2019-07-12 14:35:41,009\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 1 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m 2019-07-12 14:35:41.081676: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=27805)\u001b[0m 2019-07-12 14:35:41,287\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 3 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=27814)\u001b[0m 2019-07-12 14:35:41,255\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 4 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=27814)\u001b[0m 2019-07-12 14:35:41.279096: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=27805)\u001b[0m 2019-07-12 14:35:41.310729: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=27823)\u001b[0m 2019-07-12 14:35:42,006\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 6 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=27823)\u001b[0m 2019-07-12 14:35:42.029497: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=27806)\u001b[0m 2019-07-12 14:35:42,159\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 2 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=27824)\u001b[0m 2019-07-12 14:35:42,161\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 5 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=27806)\u001b[0m 2019-07-12 14:35:42.180329: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=27824)\u001b[0m 2019-07-12 14:35:42.195482: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m 2019-07-12 14:35:57,445\tINFO dynamic_tf_policy.py:313 -- Initializing loss function with dummy input:\n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m { 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m   'actions': <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m   'behaviour_logits': <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,\n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m   'prev_actions': <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}\n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27805)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=27805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27805)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=27805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27814)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=27814)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27814)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=27814)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27806)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=27806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27806)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=27806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27823)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=27823)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27823)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=27823)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m 2019-07-12 14:36:00,919\tINFO rollout_worker.py:428 -- Generating sample batch of size 200\n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m 2019-07-12 14:36:00,955\tINFO sampler.py:308 -- Raw obs from env: { 0: { 'agent0': np.ndarray((5,), dtype=float64, min=0.0, max=1.0, mean=0.2)}}\n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m 2019-07-12 14:36:00,956\tINFO sampler.py:309 -- Info return from env: {0: {'agent0': None}}\n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m 2019-07-12 14:36:00,956\tINFO sampler.py:407 -- Preprocessed obs: np.ndarray((5,), dtype=float64, min=0.0, max=1.0, mean=0.2)\n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m 2019-07-12 14:36:00,956\tINFO sampler.py:411 -- Filtered obs: np.ndarray((5,), dtype=float64, min=0.0, max=1.0, mean=0.2)\n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m 2019-07-12 14:36:00,959\tINFO sampler.py:525 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m                                   'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m                                   'obs': np.ndarray((5,), dtype=float64, min=0.0, max=1.0, mean=0.2),\n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m 2019-07-12 14:36:00,959\tINFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m 2019-07-12 14:36:01,195\tINFO sampler.py:552 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m { 'default_policy': ( np.ndarray((1, 2), dtype=float32, min=0.081, max=0.919, mean=0.5),\n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m                       [],\n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m                       { 'action_prob': np.ndarray((1,), dtype=float32, min=1.003, max=1.003, mean=1.003),\n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m                         'behaviour_logits': np.ndarray((1, 2), dtype=float32, min=-0.002, max=-0.0, mean=-0.001),\n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=0.0, max=0.0, mean=0.0)})}\n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m 2019-07-12 14:36:01,400\tINFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m { 'agent0': { 'data': { 'action_prob': np.ndarray((20,), dtype=float32, min=0.992, max=1.03, mean=1.001),\n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m                         'actions': np.ndarray((20, 2), dtype=float32, min=0.002, max=0.998, mean=0.5),\n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m                         'advantages': np.ndarray((20,), dtype=float32, min=0.0, max=0.028, mean=0.002),\n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m                         'agent_index': np.ndarray((20,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m                         'behaviour_logits': np.ndarray((20, 2), dtype=float32, min=-0.005, max=0.003, mean=-0.001),\n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m                         'dones': np.ndarray((20,), dtype=bool, min=0.0, max=1.0, mean=0.05),\n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m                         'eps_id': np.ndarray((20,), dtype=int64, min=1657935879.0, max=1657935879.0, mean=1657935879.0),\n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m                         'infos': np.ndarray((20,), dtype=object, head={'time': Timestamp('2009-03-16 00:00:00'), 'nlv': 99.8500499750125, 'nr_contracts': np.ndarray((3,), dtype=float64, min=-0.05, max=0.231, mean=0.06), 'bid_prices': np.ndarray((3,), dtype=float64, min=1.0, max=1911.643, mean=781.446), 'ask_prices': np.ndarray((3,), dtype=float64, min=1.0, max=1913.555, mean=782.227), 'cost_of_spread': np.ndarray((3,), dtype=float64, min=0.0, max=0.05, mean=0.017), 'cost_of_commissions': np.ndarray((3,), dtype=float64, min=0.0, max=0.05, mean=0.017), 'profit_on_idle_cash': 0.0, 'margin': 0, 'weights_target': np.ndarray((3,), dtype=float64, min=0.0, max=1.0, mean=0.333), 'weights_drift': np.ndarray((3,), dtype=float64, min=0.0, max=1.0, mean=0.333), 'weights_diff': np.ndarray((3,), dtype=float64, min=-1.0, max=1.0, mean=0.0), 'positions_value_diff': np.ndarray((3,), dtype=float64, min=-100.0, max=100.0, mean=0.0), 'acq_prices': np.ndarray((3,), dtype=float64, min=1.0, max=1912.599, mean=781.908), 'nr_contracts_diff': np.ndarray((3,), dtype=float64, min=-100.0, max=0.231, mean=-33.256), 'nlv_pre': 100.0}),\n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m                         'new_obs': np.ndarray((20, 5), dtype=float32, min=-4.948, max=6.657, mean=0.461),\n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m                         'obs': np.ndarray((20, 5), dtype=float32, min=-1.0, max=6.657, mean=0.508),\n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m                         'prev_actions': np.ndarray((20, 2), dtype=float32, min=0.0, max=0.998, mean=0.475),\n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m                         'prev_rewards': np.ndarray((20,), dtype=float32, min=0.0, max=0.028, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m                         'rewards': np.ndarray((20,), dtype=float32, min=0.0, max=0.028, mean=0.002),\n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m                         't': np.ndarray((20,), dtype=int64, min=0.0, max=19.0, mean=9.5),\n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m                         'unroll_id': np.ndarray((20,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m                         'value_targets': np.ndarray((20,), dtype=float64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m                         'vf_preds': np.ndarray((20,), dtype=float32, min=0.0, max=0.0, mean=0.0)},\n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m               'type': 'SampleBatch'}}\n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27824)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=27824)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27824)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=27824)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m 2019-07-12 14:36:02,906\tINFO rollout_worker.py:462 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m { 'data': { 'action_prob': np.ndarray((200,), dtype=float32, min=0.988, max=1.03, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m             'actions': np.ndarray((200, 2), dtype=float32, min=0.002, max=0.998, mean=0.5),\n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m             'advantages': np.ndarray((200,), dtype=float32, min=-0.025, max=0.032, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m             'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m             'behaviour_logits': np.ndarray((200, 2), dtype=float32, min=-0.005, max=0.006, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m             'dones': np.ndarray((200,), dtype=bool, min=0.0, max=1.0, mean=0.05),\n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m             'eps_id': np.ndarray((200,), dtype=int64, min=97709523.0, max=1899557939.0, mean=1114479137.0),\n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m             'infos': np.ndarray((200,), dtype=object, head={'time': Timestamp('2009-03-16 00:00:00'), 'nlv': 99.8500499750125, 'nr_contracts': np.ndarray((3,), dtype=float64, min=-0.05, max=0.231, mean=0.06), 'bid_prices': np.ndarray((3,), dtype=float64, min=1.0, max=1911.643, mean=781.446), 'ask_prices': np.ndarray((3,), dtype=float64, min=1.0, max=1913.555, mean=782.227), 'cost_of_spread': np.ndarray((3,), dtype=float64, min=0.0, max=0.05, mean=0.017), 'cost_of_commissions': np.ndarray((3,), dtype=float64, min=0.0, max=0.05, mean=0.017), 'profit_on_idle_cash': 0.0, 'margin': 0, 'weights_target': np.ndarray((3,), dtype=float64, min=0.0, max=1.0, mean=0.333), 'weights_drift': np.ndarray((3,), dtype=float64, min=0.0, max=1.0, mean=0.333), 'weights_diff': np.ndarray((3,), dtype=float64, min=-1.0, max=1.0, mean=0.0), 'positions_value_diff': np.ndarray((3,), dtype=float64, min=-100.0, max=100.0, mean=0.0), 'acq_prices': np.ndarray((3,), dtype=float64, min=1.0, max=1912.599, mean=781.908), 'nr_contracts_diff': np.ndarray((3,), dtype=float64, min=-100.0, max=0.231, mean=-33.256), 'nlv_pre': 100.0}),\n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m             'new_obs': np.ndarray((200, 5), dtype=float32, min=-6.399, max=6.657, mean=0.295),\n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m             'obs': np.ndarray((200, 5), dtype=float32, min=-6.399, max=6.657, mean=0.304),\n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m             'prev_actions': np.ndarray((200, 2), dtype=float32, min=0.0, max=0.998, mean=0.475),\n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m             'prev_rewards': np.ndarray((200,), dtype=float32, min=-0.025, max=0.032, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m             'rewards': np.ndarray((200,), dtype=float32, min=-0.025, max=0.032, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m             't': np.ndarray((200,), dtype=int64, min=0.0, max=19.0, mean=9.5),\n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m             'unroll_id': np.ndarray((200,), dtype=int64, min=0.0, max=9.0, mean=4.5),\n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m             'value_targets': np.ndarray((200,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m             'vf_preds': np.ndarray((200,), dtype=float32, min=0.0, max=0.0, mean=0.0)},\n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m   'type': 'SampleBatch'}\n",
      "\u001b[2m\u001b[36m(pid=27804)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:36:09,145\tINFO multi_gpu_impl.py:146 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m { 'inputs': [ np.ndarray((4000, 2), dtype=float32, min=0.0, max=1.0, mean=0.475),\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m               np.ndarray((4000,), dtype=float32, min=-0.049, max=0.032, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m               np.ndarray((4000, 5), dtype=float32, min=-14.063, max=8.218, mean=0.247),\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m               np.ndarray((4000, 2), dtype=float32, min=0.0, max=1.0, mean=0.5),\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m               np.ndarray((4000,), dtype=float32, min=-10.383, max=6.56, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m               np.ndarray((4000, 2), dtype=float32, min=-0.006, max=0.007, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m               np.ndarray((4000,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m               np.ndarray((4000,), dtype=float32, min=0.0, max=0.0, mean=0.0)],\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m   'placeholders': [ <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m                     <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m                     <tf.Tensor 'default_policy/observation:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m                     <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m                     <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m                     <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m                     <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m                     <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>],\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m   'state_inputs': []}\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:36:09,146\tINFO multi_gpu_impl.py:191 -- Divided 4000 rollout sequences, each of length 1, among 1 devices.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:36:10,900\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:36:12,118\tINFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:36:22,149\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:36:33,069\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:36:42,817\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:36:53,381\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:37:03,451\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:37:12,378\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:37:22,020\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:37:32,189\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:37:41,384\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:37:51,827\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:38:01,753\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:38:11,471\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:38:21,000\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:38:31,041\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:38:40,998\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:38:50,036\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:38:59,879\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:39:10,311\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:39:20,191\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:39:33,059\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:39:44,060\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:39:54,219\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:40:05,010\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:40:16,788\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:40:27,460\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:40:37,878\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:40:47,616\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:40:58,038\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:41:07,870\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:41:18,368\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:41:28,742\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:41:38,990\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:41:48,848\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:41:58,935\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:42:09,389\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:42:20,183\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:42:30,889\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:42:42,401\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:42:53,897\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:43:05,486\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:43:17,046\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:43:27,713\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:43:39,010\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:43:49,961\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:44:00,455\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:44:11,185\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:44:22,127\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:44:32,668\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:44:43,058\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:44:53,323\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:45:03,112\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:45:13,701\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:45:25,279\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:45:35,545\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:45:45,531\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:45:55,816\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:46:06,341\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:46:16,603\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:46:27,193\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:46:37,172\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:46:47,435\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:46:58,121\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:47:07,885\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:47:17,927\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:47:28,349\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:47:38,339\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:47:48,715\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:47:59,294\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:48:09,472\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:48:19,810\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:48:30,630\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:48:40,564\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:48:50,918\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:49:01,318\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:49:11,493\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:49:22,255\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:49:32,553\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:49:42,636\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:49:53,248\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:50:03,642\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:50:13,624\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:50:23,837\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:50:34,818\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:50:44,829\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:50:55,029\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:51:06,173\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:51:16,195\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:51:26,738\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:51:37,479\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:51:47,492\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:51:58,044\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:52:08,533\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:52:18,714\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:52:29,004\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:52:39,510\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:52:49,505\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:52:59,437\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:53:09,977\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:53:19,875\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:53:30,613\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:53:40,730\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:53:50,243\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:54:00,624\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:54:11,037\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:54:20,764\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:54:30,832\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:54:41,405\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:54:51,082\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:55:01,239\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:55:12,150\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:55:21,801\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:55:32,195\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:55:42,864\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:55:52,280\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:56:02,398\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:56:13,058\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:56:23,197\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:56:33,421\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:56:43,990\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:56:53,985\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:57:04,094\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:57:15,281\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:57:25,986\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:57:39,297\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:57:51,443\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:58:03,265\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:58:14,740\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:58:24,666\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:58:34,458\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:58:44,903\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:58:53,871\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:59:03,413\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:59:13,153\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:59:22,780\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:59:32,241\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:59:41,991\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 14:59:52,396\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 15:00:02,082\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 15:00:12,159\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 15:00:23,439\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 15:00:33,909\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 15:00:43,989\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 15:00:54,455\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 15:01:04,148\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 15:01:14,018\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 15:01:24,328\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 15:01:34,017\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 15:01:44,016\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 15:01:54,528\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 15:02:03,830\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 15:02:13,979\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 15:02:24,774\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 15:02:34,398\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 15:02:44,912\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 15:02:56,036\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 15:03:05,714\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 15:03:15,756\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 15:03:27,155\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 15:03:37,427\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 15:03:48,072\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 15:04:00,081\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 15:04:11,447\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 15:04:22,268\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 15:04:33,257\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 15:04:43,106\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 15:04:53,322\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 15:05:03,671\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 15:05:13,718\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 15:05:23,752\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 15:05:34,547\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 15:05:44,980\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 15:05:55,529\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 15:06:07,564\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 15:06:18,329\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 15:06:28,836\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 15:06:39,151\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 15:06:49,458\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 15:06:59,973\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 15:07:09,394\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 15:07:19,551\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 15:07:30,884\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 15:07:41,201\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 15:07:52,650\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 15:08:05,153\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 15:08:16,623\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 15:08:28,884\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=125848)\u001b[0m 2019-07-12 15:08:40,929\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-12 15:08:44,227\tINFO ray_trial_executor.py:187 -- Destroying actor for trial PPO_GAIAPredictorsContinuousV8_0_clip_param=0.8,entropy_coeff=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2019-07-12 15:08:44,290\tINFO tune.py:65 -- Did not find checkpoint file in logs/tran_cost_x10/clip_0.8-tc-WalkForward-750k2015.\n",
      "2019-07-12 15:08:44,292\tINFO tune.py:233 -- Starting a new experiment.\n",
      "2019-07-12 15:08:44,337\tWARNING signature.py:108 -- The function with_updates has a **kwargs argument, which is currently not supported.\n",
      "2019-07-12 15:08:44,451\tWARNING util.py:64 -- The `start_trial` operation took 0.12708306312561035 seconds to complete, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_______________________________________2015____________________________________________\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:08:46,970\tWARNING ppo.py:151 -- FYI: By default, the value function will not share layers with the policy model ('vf_share_layers': False).\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:08:48,301\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 0 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:08:48.302875: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:08:56,635\tINFO dynamic_tf_policy.py:313 -- Initializing loss function with dummy input:\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m { 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m   'actions': <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m   'behaviour_logits': <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m   'prev_actions': <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:08:58,272\tINFO rollout_worker.py:719 -- Built policy map: {'default_policy': <ray.rllib.policy.tf_policy_template.PPOTFPolicy object at 0x7f793b710208>}\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:08:58,272\tINFO rollout_worker.py:720 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7f793b7b0da0>}\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:08:58,272\tINFO rollout_worker.py:333 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7f793b7b0be0>}\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:08:58,458\tINFO multi_gpu_optimizer.py:79 -- LocalMultiGPUOptimizer devices ['/cpu:0']\n",
      "\u001b[2m\u001b[36m(pid=27880)\u001b[0m 2019-07-12 15:09:02,706\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 2 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=27880)\u001b[0m 2019-07-12 15:09:02.741530: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=27869)\u001b[0m 2019-07-12 15:09:03,494\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 4 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m 2019-07-12 15:09:03,510\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 1 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=27869)\u001b[0m 2019-07-12 15:09:03.564848: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m 2019-07-12 15:09:03.562251: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=27942)\u001b[0m 2019-07-12 15:09:03,863\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 6 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=27942)\u001b[0m 2019-07-12 15:09:03.890570: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=27867)\u001b[0m 2019-07-12 15:09:04,445\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 3 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=27867)\u001b[0m 2019-07-12 15:09:04.501101: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=27914)\u001b[0m 2019-07-12 15:09:04,601\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 5 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=27914)\u001b[0m 2019-07-12 15:09:04.634051: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=27880)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=27880)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27880)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=27880)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27867)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=27867)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27867)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=27867)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m 2019-07-12 15:09:21,846\tINFO dynamic_tf_policy.py:313 -- Initializing loss function with dummy input:\n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m { 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m   'actions': <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m   'behaviour_logits': <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,\n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m   'prev_actions': <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}\n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27869)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=27869)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27869)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=27869)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27942)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=27942)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27942)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=27942)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27914)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=27914)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27914)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=27914)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m 2019-07-12 15:09:24,796\tINFO rollout_worker.py:428 -- Generating sample batch of size 200\n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m 2019-07-12 15:09:24,821\tINFO sampler.py:308 -- Raw obs from env: { 0: { 'agent0': np.ndarray((5,), dtype=float64, min=-1.0, max=3.35, mean=0.788)}}\n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m 2019-07-12 15:09:24,821\tINFO sampler.py:309 -- Info return from env: {0: {'agent0': None}}\n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m 2019-07-12 15:09:24,821\tINFO sampler.py:407 -- Preprocessed obs: np.ndarray((5,), dtype=float64, min=-1.0, max=3.35, mean=0.788)\n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m 2019-07-12 15:09:24,822\tINFO sampler.py:411 -- Filtered obs: np.ndarray((5,), dtype=float64, min=-1.0, max=3.35, mean=0.788)\n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m 2019-07-12 15:09:24,824\tINFO sampler.py:525 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m                                   'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m                                   'obs': np.ndarray((5,), dtype=float64, min=-1.0, max=3.35, mean=0.788),\n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m 2019-07-12 15:09:24,825\tINFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m 2019-07-12 15:09:24,929\tINFO sampler.py:552 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m { 'default_policy': ( np.ndarray((1, 2), dtype=float32, min=0.463, max=0.537, mean=0.5),\n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m                       [],\n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m                       { 'action_prob': np.ndarray((1,), dtype=float32, min=1.004, max=1.004, mean=1.004),\n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m                         'behaviour_logits': np.ndarray((1, 2), dtype=float32, min=0.001, max=0.01, mean=0.005),\n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=0.0, max=0.0, mean=0.0)})}\n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m 2019-07-12 15:09:25,021\tINFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m { 'agent0': { 'data': { 'action_prob': np.ndarray((20,), dtype=float32, min=0.993, max=1.006, mean=1.001),\n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m                         'actions': np.ndarray((20, 2), dtype=float32, min=0.051, max=0.949, mean=0.5),\n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m                         'advantages': np.ndarray((20,), dtype=float32, min=-0.017, max=0.011, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m                         'agent_index': np.ndarray((20,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m                         'behaviour_logits': np.ndarray((20, 2), dtype=float32, min=-0.001, max=0.01, mean=0.003),\n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m                         'dones': np.ndarray((20,), dtype=bool, min=0.0, max=1.0, mean=0.05),\n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m                         'eps_id': np.ndarray((20,), dtype=int64, min=1376323374.0, max=1376323374.0, mean=1376323374.0),\n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m                         'infos': np.ndarray((20,), dtype=object, head={'time': Timestamp('2014-01-10 00:00:00'), 'nlv': 99.8500499750125, 'nr_contracts': np.ndarray((3,), dtype=float64, min=-0.05, max=0.105, mean=0.021), 'bid_prices': np.ndarray((3,), dtype=float64, min=1.0, max=5301.844, mean=1937.724), 'ask_prices': np.ndarray((3,), dtype=float64, min=1.0, max=5307.148, mean=1939.662), 'cost_of_spread': np.ndarray((3,), dtype=float64, min=0.0, max=0.027, mean=0.017), 'cost_of_commissions': np.ndarray((3,), dtype=float64, min=0.0, max=0.027, mean=0.017), 'profit_on_idle_cash': 0.0, 'margin': 0, 'weights_target': np.ndarray((3,), dtype=float32, min=0.0, max=0.537, mean=0.333), 'weights_drift': np.ndarray((3,), dtype=float64, min=0.0, max=1.0, mean=0.333), 'weights_diff': np.ndarray((3,), dtype=float64, min=-1.0, max=0.537, mean=0.0), 'positions_value_diff': np.ndarray((3,), dtype=float64, min=-100.0, max=53.683, mean=0.0), 'acq_prices': np.ndarray((3,), dtype=float64, min=1.0, max=5307.148, mean=1939.662), 'nr_contracts_diff': np.ndarray((3,), dtype=float64, min=-100.0, max=0.105, mean=-33.295), 'nlv_pre': 100.0}),\n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m                         'new_obs': np.ndarray((20, 5), dtype=float32, min=-1.0, max=1.647, mean=0.246),\n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m                         'obs': np.ndarray((20, 5), dtype=float32, min=-1.0, max=3.35, mean=0.279),\n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m                         'prev_actions': np.ndarray((20, 2), dtype=float32, min=0.0, max=0.949, mean=0.475),\n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m                         'prev_rewards': np.ndarray((20,), dtype=float32, min=-0.017, max=0.011, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m                         'rewards': np.ndarray((20,), dtype=float32, min=-0.017, max=0.011, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m                         't': np.ndarray((20,), dtype=int64, min=0.0, max=19.0, mean=9.5),\n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m                         'unroll_id': np.ndarray((20,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m                         'value_targets': np.ndarray((20,), dtype=float64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m                         'vf_preds': np.ndarray((20,), dtype=float32, min=0.0, max=0.0, mean=0.0)},\n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m               'type': 'SampleBatch'}}\n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m 2019-07-12 15:09:26,797\tINFO rollout_worker.py:462 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m { 'data': { 'action_prob': np.ndarray((200,), dtype=float32, min=0.96, max=1.018, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m             'actions': np.ndarray((200, 2), dtype=float32, min=0.0, max=1.0, mean=0.5),\n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m             'advantages': np.ndarray((200,), dtype=float32, min=-0.019, max=0.013, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m             'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m             'behaviour_logits': np.ndarray((200, 2), dtype=float32, min=-0.008, max=0.01, mean=0.003),\n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m             'dones': np.ndarray((200,), dtype=bool, min=0.0, max=1.0, mean=0.05),\n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m             'eps_id': np.ndarray((200,), dtype=int64, min=193777411.0, max=1813667296.0, mean=1119423621.4),\n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m             'infos': np.ndarray((200,), dtype=object, head={'time': Timestamp('2014-01-10 00:00:00'), 'nlv': 99.8500499750125, 'nr_contracts': np.ndarray((3,), dtype=float64, min=-0.05, max=0.105, mean=0.021), 'bid_prices': np.ndarray((3,), dtype=float64, min=1.0, max=5301.844, mean=1937.724), 'ask_prices': np.ndarray((3,), dtype=float64, min=1.0, max=5307.148, mean=1939.662), 'cost_of_spread': np.ndarray((3,), dtype=float64, min=0.0, max=0.027, mean=0.017), 'cost_of_commissions': np.ndarray((3,), dtype=float64, min=0.0, max=0.027, mean=0.017), 'profit_on_idle_cash': 0.0, 'margin': 0, 'weights_target': np.ndarray((3,), dtype=float32, min=0.0, max=0.537, mean=0.333), 'weights_drift': np.ndarray((3,), dtype=float64, min=0.0, max=1.0, mean=0.333), 'weights_diff': np.ndarray((3,), dtype=float64, min=-1.0, max=0.537, mean=0.0), 'positions_value_diff': np.ndarray((3,), dtype=float64, min=-100.0, max=53.683, mean=0.0), 'acq_prices': np.ndarray((3,), dtype=float64, min=1.0, max=5307.148, mean=1939.662), 'nr_contracts_diff': np.ndarray((3,), dtype=float64, min=-100.0, max=0.105, mean=-33.295), 'nlv_pre': 100.0}),\n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m             'new_obs': np.ndarray((200, 5), dtype=float32, min=-6.399, max=3.371, mean=0.258),\n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m             'obs': np.ndarray((200, 5), dtype=float32, min=-3.184, max=4.202, mean=0.259),\n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m             'prev_actions': np.ndarray((200, 2), dtype=float32, min=0.0, max=1.0, mean=0.475),\n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m             'prev_rewards': np.ndarray((200,), dtype=float32, min=-0.019, max=0.013, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m             'rewards': np.ndarray((200,), dtype=float32, min=-0.019, max=0.013, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m             't': np.ndarray((200,), dtype=int64, min=0.0, max=19.0, mean=9.5),\n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m             'unroll_id': np.ndarray((200,), dtype=int64, min=0.0, max=9.0, mean=4.5),\n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m             'value_targets': np.ndarray((200,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m             'vf_preds': np.ndarray((200,), dtype=float32, min=0.0, max=0.0, mean=0.0)},\n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m   'type': 'SampleBatch'}\n",
      "\u001b[2m\u001b[36m(pid=27882)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:09:30,359\tINFO multi_gpu_impl.py:146 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m { 'inputs': [ np.ndarray((4000, 2), dtype=float32, min=0.0, max=1.0, mean=0.475),\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m               np.ndarray((4000,), dtype=float32, min=-0.064, max=0.042, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m               np.ndarray((4000, 5), dtype=float32, min=-13.352, max=10.882, mean=0.263),\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m               np.ndarray((4000, 2), dtype=float32, min=0.0, max=1.0, mean=0.5),\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m               np.ndarray((4000,), dtype=float32, min=-13.42, max=8.77, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m               np.ndarray((4000, 2), dtype=float32, min=-0.009, max=0.01, mean=0.002),\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m               np.ndarray((4000,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m               np.ndarray((4000,), dtype=float32, min=0.0, max=0.0, mean=0.0)],\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m   'placeholders': [ <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m                     <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m                     <tf.Tensor 'default_policy/observation:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m                     <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m                     <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m                     <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m                     <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m                     <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>],\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m   'state_inputs': []}\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:09:30,359\tINFO multi_gpu_impl.py:191 -- Divided 4000 rollout sequences, each of length 1, among 1 devices.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:09:32,303\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:09:33,556\tINFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:09:43,405\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:09:53,173\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:10:03,182\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:10:15,074\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:10:25,103\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:10:35,396\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:10:45,530\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:10:55,307\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:11:04,919\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:11:15,010\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:11:24,642\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:11:34,605\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:11:45,001\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:11:54,595\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:12:04,596\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:12:14,960\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:12:24,446\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:12:34,711\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:12:45,202\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:12:54,364\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:13:04,353\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:13:14,916\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:13:23,777\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:13:33,512\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:13:44,182\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:13:53,163\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:14:02,868\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:14:12,688\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:14:22,011\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:14:31,532\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:14:41,090\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:14:51,429\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:15:00,555\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:15:10,188\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:15:20,680\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:15:29,917\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:15:39,906\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:15:50,768\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:15:59,946\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:16:09,486\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:16:19,636\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:16:28,650\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:16:38,712\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:16:49,454\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:16:58,756\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:17:09,037\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:17:18,933\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:17:29,121\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:17:39,217\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:17:49,237\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:17:59,004\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:18:09,142\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:18:19,035\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:18:29,299\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:18:39,052\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:18:49,264\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:18:59,501\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:19:09,014\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:19:19,441\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:19:29,837\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:19:39,317\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:19:49,591\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:20:00,313\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:20:09,689\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:20:19,444\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:20:30,406\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:20:39,752\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:20:49,529\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:21:00,516\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:21:09,886\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:21:19,941\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:21:30,775\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:21:40,257\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:21:50,124\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:22:00,782\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:22:10,251\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:22:20,212\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:22:30,314\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:22:40,349\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:22:50,266\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:23:00,644\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:23:10,059\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:23:19,882\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:23:29,881\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:23:40,025\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:23:49,985\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:23:59,829\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:24:10,236\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:24:19,717\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:24:29,854\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:24:40,058\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:24:49,516\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:24:59,701\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:25:10,264\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:25:19,834\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:25:30,358\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:25:41,171\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:25:50,516\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:26:00,145\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:26:11,355\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:26:20,585\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:26:30,228\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:26:40,962\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:26:49,788\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:26:59,987\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:27:10,283\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:27:19,853\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:27:30,103\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:27:40,317\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:27:50,416\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:28:00,235\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:28:10,600\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:28:20,337\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:28:30,342\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:28:40,317\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:28:50,278\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:29:00,377\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:29:10,572\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:29:20,665\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:29:30,629\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:29:40,693\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:29:51,083\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:30:00,604\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:30:10,645\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:30:21,198\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:30:30,793\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:30:40,939\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:30:51,596\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:31:00,959\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:31:11,249\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:31:21,901\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:31:31,521\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:31:41,317\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:31:52,384\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:32:02,021\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:32:11,831\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:32:22,835\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:32:32,830\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:32:43,717\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:32:54,000\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:33:03,647\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:33:13,811\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:33:26,365\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:33:39,275\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:33:52,131\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:34:04,195\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:34:15,634\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:34:29,012\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:34:40,048\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:34:50,551\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:35:02,579\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:35:13,050\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:35:23,817\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:35:35,904\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:35:46,942\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:35:58,825\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:36:12,137\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:36:22,889\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:36:34,861\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:36:45,350\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:36:56,454\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:37:06,969\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:37:17,025\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:37:28,291\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:37:38,813\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:37:48,698\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:37:58,712\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:38:08,824\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:38:18,648\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:38:28,495\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:38:39,356\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:38:48,891\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:38:58,796\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:39:09,203\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:39:19,007\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:39:28,917\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:39:40,108\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:39:49,896\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:39:59,802\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:40:10,412\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:40:20,180\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:40:29,864\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:40:40,365\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:40:49,781\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:41:00,293\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:41:11,269\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27946)\u001b[0m 2019-07-12 15:41:20,662\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-12 15:41:23,734\tINFO ray_trial_executor.py:187 -- Destroying actor for trial PPO_GAIAPredictorsContinuousV8_0_clip_param=0.8,entropy_coeff=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2019-07-12 15:41:23,869\tINFO tune.py:65 -- Did not find checkpoint file in logs/tran_cost_x10/clip_0.8-tc-WalkForward-750k2016.\n",
      "2019-07-12 15:41:23,876\tINFO tune.py:233 -- Starting a new experiment.\n",
      "2019-07-12 15:41:23,892\tWARNING signature.py:108 -- The function with_updates has a **kwargs argument, which is currently not supported.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_______________________________________2016____________________________________________\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:41:25,993\tWARNING ppo.py:151 -- FYI: By default, the value function will not share layers with the policy model ('vf_share_layers': False).\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:41:27,093\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 0 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:41:27.094217: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:41:34,430\tINFO dynamic_tf_policy.py:313 -- Initializing loss function with dummy input:\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m { 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m   'actions': <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m   'behaviour_logits': <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m   'prev_actions': <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:41:35,992\tINFO rollout_worker.py:719 -- Built policy map: {'default_policy': <ray.rllib.policy.tf_policy_template.PPOTFPolicy object at 0x7f0d43cd4198>}\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:41:35,992\tINFO rollout_worker.py:720 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7f0d43d75d30>}\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:41:35,993\tINFO rollout_worker.py:333 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7f0d43d75b70>}\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:41:36,282\tINFO multi_gpu_optimizer.py:79 -- LocalMultiGPUOptimizer devices ['/cpu:0']\n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m 2019-07-12 15:41:50,498\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 1 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m 2019-07-12 15:41:50.539960: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=61323)\u001b[0m 2019-07-12 15:41:51,070\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 2 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=61323)\u001b[0m 2019-07-12 15:41:51.093404: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=61348)\u001b[0m 2019-07-12 15:41:51,459\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 5 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=61348)\u001b[0m 2019-07-12 15:41:51.486077: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=61325)\u001b[0m 2019-07-12 15:41:51,584\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 4 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=61325)\u001b[0m 2019-07-12 15:41:51.614346: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=61322)\u001b[0m 2019-07-12 15:41:51,683\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 3 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=61322)\u001b[0m 2019-07-12 15:41:51.718982: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=61347)\u001b[0m 2019-07-12 15:41:51,958\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 6 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=61347)\u001b[0m 2019-07-12 15:41:52.031756: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m 2019-07-12 15:42:06,325\tINFO dynamic_tf_policy.py:313 -- Initializing loss function with dummy input:\n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m { 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m   'actions': <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m   'behaviour_logits': <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,\n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m   'prev_actions': <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}\n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61322)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=61322)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61322)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=61322)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61348)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=61348)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61348)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=61348)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61323)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=61323)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61323)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=61323)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61347)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=61347)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61347)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=61347)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=61325)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=61325)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61325)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=61325)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m 2019-07-12 15:42:09,665\tINFO rollout_worker.py:428 -- Generating sample batch of size 200\n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m 2019-07-12 15:42:09,690\tINFO sampler.py:308 -- Raw obs from env: { 0: { 'agent0': np.ndarray((5,), dtype=float64, min=-1.0, max=4.971, mean=1.266)}}\n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m 2019-07-12 15:42:09,691\tINFO sampler.py:309 -- Info return from env: {0: {'agent0': None}}\n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m 2019-07-12 15:42:09,691\tINFO sampler.py:407 -- Preprocessed obs: np.ndarray((5,), dtype=float64, min=-1.0, max=4.971, mean=1.266)\n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m 2019-07-12 15:42:09,692\tINFO sampler.py:411 -- Filtered obs: np.ndarray((5,), dtype=float64, min=-1.0, max=4.971, mean=1.266)\n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m 2019-07-12 15:42:09,693\tINFO sampler.py:525 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m                                   'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m                                   'obs': np.ndarray((5,), dtype=float64, min=-1.0, max=4.971, mean=1.266),\n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m 2019-07-12 15:42:09,693\tINFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m 2019-07-12 15:42:09,821\tINFO sampler.py:552 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m { 'default_policy': ( np.ndarray((1, 2), dtype=float32, min=0.176, max=0.824, mean=0.5),\n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m                       [],\n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m                       { 'action_prob': np.ndarray((1,), dtype=float32, min=0.999, max=0.999, mean=0.999),\n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m                         'behaviour_logits': np.ndarray((1, 2), dtype=float32, min=0.001, max=0.002, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=0.0, max=0.0, mean=0.0)})}\n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m 2019-07-12 15:42:09,892\tINFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m { 'agent0': { 'data': { 'action_prob': np.ndarray((20,), dtype=float32, min=0.998, max=1.005, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m                         'actions': np.ndarray((20, 2), dtype=float32, min=0.052, max=0.948, mean=0.5),\n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m                         'advantages': np.ndarray((20,), dtype=float32, min=-0.007, max=0.008, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m                         'agent_index': np.ndarray((20,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m                         'behaviour_logits': np.ndarray((20, 2), dtype=float32, min=-0.002, max=0.003, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m                         'dones': np.ndarray((20,), dtype=bool, min=0.0, max=1.0, mean=0.05),\n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m                         'eps_id': np.ndarray((20,), dtype=int64, min=1892894219.0, max=1892894219.0, mean=1892894219.0),\n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m                         'infos': np.ndarray((20,), dtype=object, head={'time': Timestamp('2006-08-15 00:00:00'), 'nlv': 99.85004996384036, 'nr_contracts': np.ndarray((3,), dtype=float64, min=-0.05, max=0.053, mean=0.01), 'bid_prices': np.ndarray((3,), dtype=float64, min=1.0, max=3083.216, mean=1138.144), 'ask_prices': np.ndarray((3,), dtype=float64, min=1.0, max=3086.3, mean=1139.282), 'cost_of_spread': np.ndarray((3,), dtype=float64, min=0.0, max=0.041, mean=0.017), 'cost_of_commissions': np.ndarray((3,), dtype=float64, min=0.0, max=0.041, mean=0.017), 'profit_on_idle_cash': 0.0, 'margin': 0, 'weights_target': np.ndarray((3,), dtype=float32, min=0.0, max=0.824, mean=0.333), 'weights_drift': np.ndarray((3,), dtype=float64, min=0.0, max=1.0, mean=0.333), 'weights_diff': np.ndarray((3,), dtype=float64, min=-1.0, max=0.824, mean=0.0), 'positions_value_diff': np.ndarray((3,), dtype=float64, min=-100.0, max=82.419, mean=0.0), 'acq_prices': np.ndarray((3,), dtype=float64, min=1.0, max=3086.3, mean=1139.282), 'nr_contracts_diff': np.ndarray((3,), dtype=float64, min=-100.0, max=0.053, mean=-33.307), 'nlv_pre': 100.0}),\n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m                         'new_obs': np.ndarray((20, 5), dtype=float32, min=-1.136, max=3.124, mean=0.331),\n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m                         'obs': np.ndarray((20, 5), dtype=float32, min=-1.136, max=4.971, mean=0.396),\n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m                         'prev_actions': np.ndarray((20, 2), dtype=float32, min=0.0, max=0.947, mean=0.475),\n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m                         'prev_rewards': np.ndarray((20,), dtype=float32, min=-0.007, max=0.008, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m                         'rewards': np.ndarray((20,), dtype=float32, min=-0.007, max=0.008, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m                         't': np.ndarray((20,), dtype=int64, min=0.0, max=19.0, mean=9.5),\n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m                         'unroll_id': np.ndarray((20,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m                         'value_targets': np.ndarray((20,), dtype=float64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m                         'vf_preds': np.ndarray((20,), dtype=float32, min=0.0, max=0.0, mean=0.0)},\n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m               'type': 'SampleBatch'}}\n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m 2019-07-12 15:42:11,112\tINFO rollout_worker.py:462 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m { 'data': { 'action_prob': np.ndarray((200,), dtype=float32, min=0.98, max=1.025, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m             'actions': np.ndarray((200, 2), dtype=float32, min=0.001, max=0.999, mean=0.5),\n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m             'advantages': np.ndarray((200,), dtype=float32, min=-0.042, max=0.021, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m             'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m             'behaviour_logits': np.ndarray((200, 2), dtype=float32, min=-0.007, max=0.005, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m             'dones': np.ndarray((200,), dtype=bool, min=0.0, max=1.0, mean=0.05),\n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m             'eps_id': np.ndarray((200,), dtype=int64, min=259947617.0, max=1892894219.0, mean=1222653435.4),\n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m             'infos': np.ndarray((200,), dtype=object, head={'time': Timestamp('2006-08-15 00:00:00'), 'nlv': 99.85004996384036, 'nr_contracts': np.ndarray((3,), dtype=float64, min=-0.05, max=0.053, mean=0.01), 'bid_prices': np.ndarray((3,), dtype=float64, min=1.0, max=3083.216, mean=1138.144), 'ask_prices': np.ndarray((3,), dtype=float64, min=1.0, max=3086.3, mean=1139.282), 'cost_of_spread': np.ndarray((3,), dtype=float64, min=0.0, max=0.041, mean=0.017), 'cost_of_commissions': np.ndarray((3,), dtype=float64, min=0.0, max=0.041, mean=0.017), 'profit_on_idle_cash': 0.0, 'margin': 0, 'weights_target': np.ndarray((3,), dtype=float32, min=0.0, max=0.824, mean=0.333), 'weights_drift': np.ndarray((3,), dtype=float64, min=0.0, max=1.0, mean=0.333), 'weights_diff': np.ndarray((3,), dtype=float64, min=-1.0, max=0.824, mean=0.0), 'positions_value_diff': np.ndarray((3,), dtype=float64, min=-100.0, max=82.419, mean=0.0), 'acq_prices': np.ndarray((3,), dtype=float64, min=1.0, max=3086.3, mean=1139.282), 'nr_contracts_diff': np.ndarray((3,), dtype=float64, min=-100.0, max=0.053, mean=-33.307), 'nlv_pre': 100.0}),\n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m             'new_obs': np.ndarray((200, 5), dtype=float32, min=-4.442, max=3.124, mean=0.232),\n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m             'obs': np.ndarray((200, 5), dtype=float32, min=-4.442, max=4.971, mean=0.243),\n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m             'prev_actions': np.ndarray((200, 2), dtype=float32, min=0.0, max=0.999, mean=0.475),\n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m             'prev_rewards': np.ndarray((200,), dtype=float32, min=-0.042, max=0.021, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m             'rewards': np.ndarray((200,), dtype=float32, min=-0.042, max=0.021, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m             't': np.ndarray((200,), dtype=int64, min=0.0, max=19.0, mean=9.5),\n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m             'unroll_id': np.ndarray((200,), dtype=int64, min=0.0, max=9.0, mean=4.5),\n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m             'value_targets': np.ndarray((200,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m             'vf_preds': np.ndarray((200,), dtype=float32, min=0.0, max=0.0, mean=0.0)},\n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m   'type': 'SampleBatch'}\n",
      "\u001b[2m\u001b[36m(pid=61324)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:42:16,467\tINFO multi_gpu_impl.py:146 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m { 'inputs': [ np.ndarray((4000, 2), dtype=float32, min=0.0, max=1.0, mean=0.475),\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m               np.ndarray((4000,), dtype=float32, min=-0.061, max=0.025, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m               np.ndarray((4000, 5), dtype=float32, min=-15.841, max=15.356, mean=0.254),\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m               np.ndarray((4000, 2), dtype=float32, min=0.0, max=1.0, mean=0.5),\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m               np.ndarray((4000,), dtype=float32, min=-12.915, max=5.212, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m               np.ndarray((4000, 2), dtype=float32, min=-0.007, max=0.007, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m               np.ndarray((4000,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m               np.ndarray((4000,), dtype=float32, min=0.0, max=0.0, mean=0.0)],\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m   'placeholders': [ <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m                     <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m                     <tf.Tensor 'default_policy/observation:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m                     <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m                     <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m                     <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m                     <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m                     <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>],\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m   'state_inputs': []}\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:42:16,468\tINFO multi_gpu_impl.py:191 -- Divided 4000 rollout sequences, each of length 1, among 1 devices.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:42:18,741\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:42:20,107\tINFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:42:29,016\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:42:39,178\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:42:48,678\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:42:58,515\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:43:08,415\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:43:19,520\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:43:29,457\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:43:39,507\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:43:50,184\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:43:59,969\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:44:10,539\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:44:21,144\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:44:31,010\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:44:41,047\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:44:51,991\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:45:01,615\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:45:11,609\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:45:22,175\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:45:31,647\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:45:41,702\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:45:52,666\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:46:02,286\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:46:12,049\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:46:22,827\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:46:32,291\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:46:42,085\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:46:52,775\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:47:02,253\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:47:11,967\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:47:22,850\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:47:32,108\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:47:42,135\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:47:52,477\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:48:01,617\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:48:11,563\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:48:21,875\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:48:30,882\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:48:40,832\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:48:50,942\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:49:01,257\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:49:11,152\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:49:21,554\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:49:31,618\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:49:41,561\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:49:51,634\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:50:01,800\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:50:11,472\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:50:21,629\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:50:33,569\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:50:43,938\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:50:54,177\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:51:04,814\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:51:14,799\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:51:24,666\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:51:35,969\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:51:47,550\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:51:57,702\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:52:06,934\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:52:16,676\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:52:26,119\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:52:35,814\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:52:45,657\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:52:55,253\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:53:05,245\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:53:14,524\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:53:24,525\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:53:35,020\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:53:44,181\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:53:54,500\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:54:04,607\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:54:13,992\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:54:23,609\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:54:33,886\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:54:43,099\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:54:52,896\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:55:03,215\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:55:12,318\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:55:21,790\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:55:31,713\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:55:41,815\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:55:51,307\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:56:00,932\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:56:11,313\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:56:20,458\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:56:30,192\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:56:40,563\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:56:49,554\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:56:59,121\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:57:09,362\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:57:18,695\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:57:28,417\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:57:38,317\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:57:47,546\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:57:57,055\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:58:06,508\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:58:16,526\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:58:26,109\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:58:35,778\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:58:46,040\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:58:55,506\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:59:05,244\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:59:15,848\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:59:25,470\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:59:35,308\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:59:45,414\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 15:59:54,972\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 16:00:05,115\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 16:00:15,175\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 16:00:24,138\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 16:00:34,191\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 16:00:44,689\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 16:00:53,887\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 16:01:03,724\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 16:01:13,525\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 16:01:23,331\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 16:01:32,892\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 16:01:42,733\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 16:01:53,243\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 16:02:02,883\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 16:02:12,877\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 16:02:23,339\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 16:02:32,984\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 16:02:42,724\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 16:02:53,168\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 16:03:02,618\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 16:03:12,383\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 16:03:23,341\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 16:03:32,740\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 16:03:42,597\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 16:03:53,464\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 16:04:03,885\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 16:04:14,847\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 16:04:26,485\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 16:04:37,367\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 16:04:48,252\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 16:04:59,726\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 16:05:10,850\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 16:05:21,582\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 16:05:32,808\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 16:05:43,593\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 16:05:55,021\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 16:06:05,631\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 16:06:16,141\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 16:06:27,597\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 16:06:37,444\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 16:06:48,054\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 16:06:59,491\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 16:07:09,431\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 16:07:19,723\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 16:07:30,769\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 16:07:40,741\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 16:07:51,154\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 16:08:02,176\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 16:08:12,124\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 16:08:22,627\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 16:08:33,741\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 16:08:43,352\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 16:08:53,834\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 16:09:05,078\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 16:09:14,638\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 16:09:25,140\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 16:09:36,334\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 16:09:46,075\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 16:09:56,935\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 16:10:07,618\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 16:10:17,503\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 16:10:27,975\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 16:10:39,189\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 16:10:48,893\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 16:10:59,043\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 16:11:10,227\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 16:11:19,851\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 16:11:29,899\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 16:11:40,884\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 16:11:50,531\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 16:12:00,726\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 16:12:11,789\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 16:12:21,503\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 16:12:31,585\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 16:12:42,392\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 16:12:51,963\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 16:13:01,745\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 16:13:12,558\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 16:13:22,107\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 16:13:32,429\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 16:13:43,418\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=27868)\u001b[0m 2019-07-12 16:13:53,163\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-12 16:13:56,314\tINFO ray_trial_executor.py:187 -- Destroying actor for trial PPO_GAIAPredictorsContinuousV8_0_clip_param=0.8,entropy_coeff=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2019-07-12 16:13:56,381\tINFO tune.py:65 -- Did not find checkpoint file in logs/tran_cost_x10/clip_0.8-tc-WalkForward-750k2017.\n",
      "2019-07-12 16:13:56,382\tINFO tune.py:233 -- Starting a new experiment.\n",
      "2019-07-12 16:13:56,400\tWARNING signature.py:108 -- The function with_updates has a **kwargs argument, which is currently not supported.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_______________________________________2017____________________________________________\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:13:58,783\tWARNING ppo.py:151 -- FYI: By default, the value function will not share layers with the policy model ('vf_share_layers': False).\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:13:59,979\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 0 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:13:59.980205: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:14:07,920\tINFO dynamic_tf_policy.py:313 -- Initializing loss function with dummy input:\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m { 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m   'actions': <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m   'behaviour_logits': <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m   'prev_actions': <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:14:09,642\tINFO rollout_worker.py:719 -- Built policy map: {'default_policy': <ray.rllib.policy.tf_policy_template.PPOTFPolicy object at 0x7fe11513e208>}\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:14:09,642\tINFO rollout_worker.py:720 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7fe1151dfda0>}\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:14:09,642\tINFO rollout_worker.py:333 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7fe1151dfbe0>}\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:14:09,875\tINFO multi_gpu_optimizer.py:79 -- LocalMultiGPUOptimizer devices ['/cpu:0']\n",
      "\u001b[2m\u001b[36m(pid=61378)\u001b[0m 2019-07-12 16:14:14,165\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 3 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=61378)\u001b[0m 2019-07-12 16:14:14.204092: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m 2019-07-12 16:14:14,178\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 1 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=61457)\u001b[0m 2019-07-12 16:14:14,201\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 2 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m 2019-07-12 16:14:14.209034: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=61457)\u001b[0m 2019-07-12 16:14:14.229046: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=61407)\u001b[0m 2019-07-12 16:14:14,404\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 6 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=61377)\u001b[0m 2019-07-12 16:14:14,495\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 4 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=61407)\u001b[0m 2019-07-12 16:14:14.452986: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=61377)\u001b[0m 2019-07-12 16:14:14.526101: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=61381)\u001b[0m 2019-07-12 16:14:15,055\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 5 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=61381)\u001b[0m 2019-07-12 16:14:15.081463: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m 2019-07-12 16:14:29,278\tINFO dynamic_tf_policy.py:313 -- Initializing loss function with dummy input:\n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m { 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m   'actions': <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m   'behaviour_logits': <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,\n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m   'prev_actions': <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}\n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61457)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=61457)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61457)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=61457)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61378)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=61378)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61378)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=61378)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61377)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=61377)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61377)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=61377)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61407)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=61407)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61407)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=61407)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=61381)\u001b[0m /home/Nicholas/.venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning:\n",
      "\u001b[2m\u001b[36m(pid=61381)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61381)\u001b[0m Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=61381)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m 2019-07-12 16:14:32,071\tINFO rollout_worker.py:428 -- Generating sample batch of size 200\n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m 2019-07-12 16:14:32,153\tINFO sampler.py:308 -- Raw obs from env: { 0: { 'agent0': np.ndarray((5,), dtype=float64, min=0.0, max=1.0, mean=0.2)}}\n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m 2019-07-12 16:14:32,153\tINFO sampler.py:309 -- Info return from env: {0: {'agent0': None}}\n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m 2019-07-12 16:14:32,154\tINFO sampler.py:407 -- Preprocessed obs: np.ndarray((5,), dtype=float64, min=0.0, max=1.0, mean=0.2)\n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m 2019-07-12 16:14:32,154\tINFO sampler.py:411 -- Filtered obs: np.ndarray((5,), dtype=float64, min=0.0, max=1.0, mean=0.2)\n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m 2019-07-12 16:14:32,155\tINFO sampler.py:525 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m                                   'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m                                   'obs': np.ndarray((5,), dtype=float64, min=0.0, max=1.0, mean=0.2),\n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m 2019-07-12 16:14:32,156\tINFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m 2019-07-12 16:14:32,264\tINFO sampler.py:552 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m { 'default_policy': ( np.ndarray((1, 2), dtype=float32, min=0.451, max=0.549, mean=0.5),\n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m                       [],\n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m                       { 'action_prob': np.ndarray((1,), dtype=float32, min=0.997, max=0.997, mean=0.997),\n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m                         'behaviour_logits': np.ndarray((1, 2), dtype=float32, min=-0.01, max=0.007, mean=-0.002),\n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=0.0, max=0.0, mean=0.0)})}\n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m 2019-07-12 16:14:32,481\tINFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m { 'agent0': { 'data': { 'action_prob': np.ndarray((20,), dtype=float32, min=0.989, max=1.018, mean=0.998),\n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m                         'actions': np.ndarray((20, 2), dtype=float32, min=0.028, max=0.972, mean=0.5),\n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m                         'advantages': np.ndarray((20,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m                         'agent_index': np.ndarray((20,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m                         'behaviour_logits': np.ndarray((20, 2), dtype=float32, min=-0.01, max=0.007, mean=-0.003),\n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m                         'dones': np.ndarray((20,), dtype=bool, min=0.0, max=1.0, mean=0.05),\n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m                         'eps_id': np.ndarray((20,), dtype=int64, min=93254248.0, max=93254248.0, mean=93254248.0),\n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m                         'infos': np.ndarray((20,), dtype=object, head={'time': Timestamp('2011-08-11 00:00:00'), 'nlv': 99.8500499750125, 'nr_contracts': np.ndarray((3,), dtype=float64, min=-0.05, max=0.201, mean=0.05), 'bid_prices': np.ndarray((3,), dtype=float64, min=1.0, max=3181.947, mean=1226.581), 'ask_prices': np.ndarray((3,), dtype=float64, min=1.0, max=3185.131, mean=1227.808), 'cost_of_spread': np.ndarray((3,), dtype=float64, min=0.0, max=0.05, mean=0.017), 'cost_of_commissions': np.ndarray((3,), dtype=float64, min=0.0, max=0.05, mean=0.017), 'profit_on_idle_cash': 0.0, 'margin': 0, 'weights_target': np.ndarray((3,), dtype=float64, min=0.0, max=1.0, mean=0.333), 'weights_drift': np.ndarray((3,), dtype=float64, min=0.0, max=1.0, mean=0.333), 'weights_diff': np.ndarray((3,), dtype=float64, min=-1.0, max=1.0, mean=0.0), 'positions_value_diff': np.ndarray((3,), dtype=float64, min=-100.0, max=100.0, mean=0.0), 'acq_prices': np.ndarray((3,), dtype=float64, min=1.0, max=3183.539, mean=1227.277), 'nr_contracts_diff': np.ndarray((3,), dtype=float64, min=-100.0, max=0.201, mean=-33.266), 'nlv_pre': 100.0}),\n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m                         'new_obs': np.ndarray((20, 5), dtype=float32, min=0.0, max=1.0, mean=0.4),\n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m                         'obs': np.ndarray((20, 5), dtype=float32, min=0.0, max=1.0, mean=0.39),\n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m                         'prev_actions': np.ndarray((20, 2), dtype=float32, min=0.0, max=0.972, mean=0.475),\n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m                         'prev_rewards': np.ndarray((20,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m                         'rewards': np.ndarray((20,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m                         't': np.ndarray((20,), dtype=int64, min=0.0, max=19.0, mean=9.5),\n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m                         'unroll_id': np.ndarray((20,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m                         'value_targets': np.ndarray((20,), dtype=float64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m                         'vf_preds': np.ndarray((20,), dtype=float32, min=0.0, max=0.0, mean=0.0)},\n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m               'type': 'SampleBatch'}}\n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m 2019-07-12 16:14:33,835\tINFO rollout_worker.py:462 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m { 'data': { 'action_prob': np.ndarray((200,), dtype=float32, min=0.963, max=1.045, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m             'actions': np.ndarray((200, 2), dtype=float32, min=0.005, max=0.995, mean=0.5),\n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m             'advantages': np.ndarray((200,), dtype=float32, min=-0.011, max=0.013, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m             'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m             'behaviour_logits': np.ndarray((200, 2), dtype=float32, min=-0.011, max=0.013, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m             'dones': np.ndarray((200,), dtype=bool, min=0.0, max=1.0, mean=0.05),\n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m             'eps_id': np.ndarray((200,), dtype=int64, min=72714008.0, max=1802838395.0, mean=1012122784.8),\n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m             'infos': np.ndarray((200,), dtype=object, head={'time': Timestamp('2011-08-11 00:00:00'), 'nlv': 99.8500499750125, 'nr_contracts': np.ndarray((3,), dtype=float64, min=-0.05, max=0.201, mean=0.05), 'bid_prices': np.ndarray((3,), dtype=float64, min=1.0, max=3181.947, mean=1226.581), 'ask_prices': np.ndarray((3,), dtype=float64, min=1.0, max=3185.131, mean=1227.808), 'cost_of_spread': np.ndarray((3,), dtype=float64, min=0.0, max=0.05, mean=0.017), 'cost_of_commissions': np.ndarray((3,), dtype=float64, min=0.0, max=0.05, mean=0.017), 'profit_on_idle_cash': 0.0, 'margin': 0, 'weights_target': np.ndarray((3,), dtype=float64, min=0.0, max=1.0, mean=0.333), 'weights_drift': np.ndarray((3,), dtype=float64, min=0.0, max=1.0, mean=0.333), 'weights_diff': np.ndarray((3,), dtype=float64, min=-1.0, max=1.0, mean=0.0), 'positions_value_diff': np.ndarray((3,), dtype=float64, min=-100.0, max=100.0, mean=0.0), 'acq_prices': np.ndarray((3,), dtype=float64, min=1.0, max=3183.539, mean=1227.277), 'nr_contracts_diff': np.ndarray((3,), dtype=float64, min=-100.0, max=0.201, mean=-33.266), 'nlv_pre': 100.0}),\n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m             'new_obs': np.ndarray((200, 5), dtype=float32, min=-3.823, max=6.349, mean=0.243),\n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m             'obs': np.ndarray((200, 5), dtype=float32, min=-3.823, max=10.5, mean=0.269),\n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m             'prev_actions': np.ndarray((200, 2), dtype=float32, min=0.0, max=0.995, mean=0.475),\n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m             'prev_rewards': np.ndarray((200,), dtype=float32, min=-0.01, max=0.013, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m             'rewards': np.ndarray((200,), dtype=float32, min=-0.011, max=0.013, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m             't': np.ndarray((200,), dtype=int64, min=0.0, max=19.0, mean=9.5),\n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m             'unroll_id': np.ndarray((200,), dtype=int64, min=0.0, max=9.0, mean=4.5),\n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m             'value_targets': np.ndarray((200,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m             'vf_preds': np.ndarray((200,), dtype=float32, min=0.0, max=0.0, mean=0.0)},\n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m   'type': 'SampleBatch'}\n",
      "\u001b[2m\u001b[36m(pid=61437)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:14:38,744\tINFO multi_gpu_impl.py:146 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m { 'inputs': [ np.ndarray((4000, 2), dtype=float32, min=0.0, max=1.0, mean=0.475),\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m               np.ndarray((4000,), dtype=float32, min=-0.041, max=0.038, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m               np.ndarray((4000, 5), dtype=float32, min=-14.063, max=10.831, mean=0.234),\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m               np.ndarray((4000, 2), dtype=float32, min=0.0, max=1.0, mean=0.5),\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m               np.ndarray((4000,), dtype=float32, min=-8.453, max=7.603, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m               np.ndarray((4000, 2), dtype=float32, min=-0.013, max=0.014, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m               np.ndarray((4000,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m               np.ndarray((4000,), dtype=float32, min=0.0, max=0.0, mean=0.0)],\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m   'placeholders': [ <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m                     <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m                     <tf.Tensor 'default_policy/observation:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m                     <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m                     <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m                     <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m                     <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m                     <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>],\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m   'state_inputs': []}\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:14:38,744\tINFO multi_gpu_impl.py:191 -- Divided 4000 rollout sequences, each of length 1, among 1 devices.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:14:40,942\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:14:42,333\tINFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:14:51,917\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:15:01,803\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:15:11,787\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:15:23,330\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:15:33,348\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:15:43,291\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:15:53,752\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:16:03,575\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:16:13,332\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:16:23,459\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:16:33,485\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:16:43,173\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:16:53,222\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:17:02,813\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:17:12,572\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:17:23,134\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:17:32,418\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:17:42,430\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:17:52,662\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:18:01,768\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:18:11,556\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:18:22,007\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:18:31,050\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:18:41,030\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:18:51,104\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:19:00,022\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:19:09,671\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:19:19,628\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:19:29,043\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:19:38,514\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:19:48,270\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:19:58,324\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:20:07,548\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:20:17,107\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:20:27,572\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:20:36,980\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:20:47,066\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:20:56,945\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:21:06,213\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:21:15,833\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:21:26,010\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:21:34,925\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:21:44,399\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:21:53,938\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:22:03,480\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:22:12,704\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:22:22,269\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:22:32,738\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:22:41,636\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:22:50,960\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:23:01,074\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:23:09,842\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:23:19,279\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:23:28,936\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:23:38,080\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:23:47,569\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:23:57,092\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:24:06,837\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:24:16,017\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:24:25,354\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:24:35,784\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:24:44,355\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:24:53,698\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:25:03,903\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:25:12,580\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:25:21,819\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:25:31,599\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:25:40,974\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:25:50,210\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:25:59,359\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:26:09,748\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:26:19,028\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:26:28,582\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:26:38,677\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:26:47,386\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:26:56,728\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:27:06,254\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:27:15,130\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:27:24,468\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:27:33,789\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:27:43,625\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:27:52,520\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:28:01,854\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:28:11,759\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:28:20,701\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:28:30,051\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:28:39,759\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:28:48,565\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:28:57,801\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:29:06,970\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:29:16,854\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:29:26,008\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:29:35,502\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:29:45,633\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:29:54,698\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:30:04,028\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:30:13,834\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:30:22,817\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:30:32,234\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:30:41,837\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:30:51,037\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:31:00,121\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:31:09,522\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:31:19,552\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:31:29,016\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:31:38,591\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:31:48,938\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:31:57,804\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:32:07,053\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:32:16,732\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:32:25,702\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:32:35,278\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:32:44,753\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:32:54,108\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:33:03,458\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:33:12,659\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:33:23,122\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:33:32,435\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:33:41,766\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:33:51,946\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:34:00,834\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:34:10,047\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:34:19,751\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:34:28,493\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:34:38,088\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:34:47,474\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:34:57,264\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:35:06,896\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:35:16,633\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:35:27,344\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:35:37,016\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:35:46,574\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:35:56,968\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:36:06,269\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:36:15,934\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:36:25,723\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:36:35,642\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:36:45,018\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:36:55,003\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:37:03,681\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:37:12,916\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:37:22,417\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:37:32,311\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:37:41,447\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:37:51,111\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:38:01,345\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:38:10,074\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:38:19,480\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:38:29,216\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:38:38,023\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:38:47,755\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:38:57,087\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:39:06,485\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:39:15,342\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:39:24,839\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:39:35,033\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:39:43,904\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:39:53,401\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:40:03,444\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:40:12,216\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:40:21,807\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:40:31,650\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:40:40,410\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:40:49,645\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:40:59,179\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:41:08,985\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:41:17,888\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:41:27,707\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:41:37,923\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:41:46,888\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:41:56,485\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:42:05,958\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:42:15,128\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:42:24,508\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:42:34,509\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:42:43,852\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:42:53,316\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:43:02,846\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:43:13,212\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:43:22,505\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:43:32,455\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:43:43,001\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:43:52,134\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:44:01,716\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:44:12,330\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:44:21,347\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=61454)\u001b[0m 2019-07-12 16:44:30,677\tWARNING ppo.py:129 -- The magnitude of your environment rewards are more than infx the scale of `vf_clip_param`. This means that it will take more than inf iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-12 16:44:33,359\tINFO ray_trial_executor.py:187 -- Destroying actor for trial PPO_GAIAPredictorsContinuousV8_0_clip_param=0.8,entropy_coeff=1e-05,kl_coeff=0.2,kl_target=0.01,lr=1e-05,num_sgd_iter=8,train_batch_size=4000. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for year in range(2007, 2018):\n",
    "    print('_______________________________________{}____________________________________________'.format(year))\n",
    "    \n",
    "    \n",
    "    config['env_config'] = {\n",
    "        'folds': {\n",
    "            'training-set': [datetime.min, datetime(year, 12, 31)],\n",
    "            'test-set': [datetime(year + 1, 1, 1), datetime(year + 1, 12, 31)],\n",
    "        }, \n",
    "        'cost_of_commissions': 0.00050,   #0.00005 default\n",
    "        'cost_of_spread': 0.0010, #0.0001\n",
    "    }\n",
    "    experiment = tune.Experiment(\n",
    "        name='clip_0.8-tc-WalkForward-750k{}'.format(year),\n",
    "        run=rllib.agents.ppo.PPOTrainer,\n",
    "        stop={\"timesteps_total\": 750000},\n",
    "        config=deepcopy(config),\n",
    "        num_samples=1,\n",
    "        local_dir='logs/tran_cost_x10',\n",
    "        #checkpoint_freq=int(1e4 / config['train_batch_size']),  # checkpoint every 100k iters\n",
    "        checkpoint_at_end=True,\n",
    "        max_failures=0,\n",
    "        loggers=[CustomLogger],\n",
    "    )\n",
    "    trials = tune.run_experiments(\n",
    "        experiments=experiment,\n",
    "        search_alg=tune.suggest.BasicVariantGenerator(),\n",
    "        scheduler=tune.schedulers.FIFOScheduler(),\n",
    "        verbose=0,\n",
    "        reuse_actors=False,\n",
    "        resume=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for year in range(2007, 2018):\n",
    "#     print('_______________________________________{}____________________________________________'.format(year))\n",
    "    \n",
    "    \n",
    "#     config['env_config'] = {\n",
    "#         'folds': {\n",
    "#             'training-set': [datetime.min, datetime(year, 12, 31)],\n",
    "#             'test-set': [datetime(year + 1, 1, 1), datetime(year + 1, 12, 31)],\n",
    "#         }, \n",
    "#         'cost_of_commissions': 0.00025,   #0.00005 default\n",
    "#         'cost_of_spread': 0.0050, #0.0001\n",
    "#     }\n",
    "#     experiment = tune.Experiment(\n",
    "#         name='clip_0.8-tc-WalkForward-750k{}'.format(year),\n",
    "#         run=rllib.agents.ppo.PPOTrainer,\n",
    "#         stop={\"timesteps_total\": 750000},\n",
    "#         config=deepcopy(config),\n",
    "#         num_samples=1,\n",
    "#         local_dir='logs/tran_cost_x50',\n",
    "#         #checkpoint_freq=int(1e4 / config['train_batch_size']),  # checkpoint every 100k iters\n",
    "#         checkpoint_at_end=True,\n",
    "#         max_failures=0,\n",
    "#         loggers=[CustomLogger],\n",
    "#     )\n",
    "#     trials = tune.run_experiments(\n",
    "#         experiments=experiment,\n",
    "#         search_alg=tune.suggest.BasicVariantGenerator(),\n",
    "#         scheduler=tune.schedulers.FIFOScheduler(),\n",
    "#         verbose=0,\n",
    "#         reuse_actors=False,\n",
    "#         resume=False,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ray import cloudpickle\n",
    "from ray.utils import binary_to_hex, hex_to_binary\n",
    "\n",
    "\n",
    "def cloudpickleloads(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        try:\n",
    "            return cloudpickle.loads(hex_to_binary(obj[\"value\"]))\n",
    "        except:\n",
    "            for key, value in obj.items():\n",
    "                if isinstance(value, dict):\n",
    "                    if sorted(value) == ['_type', 'value']:\n",
    "                        obj[key] = cloudpickle.loads(hex_to_binary(value[\"value\"]))\n",
    "                    else:\n",
    "                        obj[key] = cloudpickleloads(value)\n",
    "                elif isinstance(value, list):\n",
    "                    for i, item in enumerate(value):\n",
    "                        obj[key][i] = cloudpickleloads(item)\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# no transaction costs with clip 0.8 \n",
    "paths = {2007: '/home/Nicholas/trading-gym/notebooks/registry/gaia/v8/logs/PPOclip_0.8-WalkForward-750k2007/experiment_state-2019-07-02_12-08-42.json',\n",
    "        2008: '/home/Nicholas/trading-gym/notebooks/registry/gaia/v8/logs/PPOclip_0.8-WalkForward-750k2008/experiment_state-2019-07-02_12-39-37.json',\n",
    "        2009: '/home/Nicholas/trading-gym/notebooks/registry/gaia/v8/logs/PPOclip_0.8-WalkForward-750k2009/experiment_state-2019-07-02_13-11-44.json',\n",
    "        2010: '/home/Nicholas/trading-gym/notebooks/registry/gaia/v8/logs/PPOclip_0.8-WalkForward-750k2010/experiment_state-2019-07-02_13-44-01.json',\n",
    "        2011: '/home/Nicholas/trading-gym/notebooks/registry/gaia/v8/logs/PPOclip_0.8-WalkForward-750k2011/experiment_state-2019-07-02_14-15-31.json',\n",
    "        2012: '/home/Nicholas/trading-gym/notebooks/registry/gaia/v8/logs/PPOclip_0.8-WalkForward-750k2012/experiment_state-2019-07-02_14-37-48.json',\n",
    "        2013: '/home/Nicholas/trading-gym/notebooks/registry/gaia/v8/logs/PPOclip_0.8-WalkForward-750k2013/experiment_state-2019-07-02_15-03-59.json',\n",
    "        2014: '/home/Nicholas/trading-gym/notebooks/registry/gaia/v8/logs/PPOclip_0.8-WalkForward-750k2014/experiment_state-2019-07-02_15-25-46.json',\n",
    "        2015: '/home/Nicholas/trading-gym/notebooks/registry/gaia/v8/logs/PPOclip_0.8-WalkForward-750k2015/experiment_state-2019-07-02_15-48-10.json',\n",
    "        2016: '/home/Nicholas/trading-gym/notebooks/registry/gaia/v8/logs/PPOclip_0.8-WalkForward-750k2016/experiment_state-2019-07-02_16-11-44.json',\n",
    "        2017: '/home/Nicholas/trading-gym/notebooks/registry/gaia/v8/logs/PPOclip_0.8-WalkForward-750k2017/experiment_state-2019-07-02_16-42-02.json'\n",
    "        }\n",
    "\n",
    "# No transaction costs with clip 0.8 as well\n",
    "# paths = {2007: '/home/Nicholas/trading-gym/notebooks/registry/gaia/v8/logs/clip_1.0-tc-WalkForward-750k2007/experiment_state-2019-07-02_17-16-37.json',\n",
    "#         2008: '/home/Nicholas/trading-gym/notebooks/registry/gaia/v8/logs/clip_1.0-tc-WalkForward-750k2008/experiment_state-2019-07-02_17-49-56.json',\n",
    "#         2009: '/home/Nicholas/trading-gym/notebooks/registry/gaia/v8/logs/clip_1.0-tc-WalkForward-750k2009/experiment_state-2019-07-02_18-24-50.json',\n",
    "#         2010: '/home/Nicholas/trading-gym/notebooks/registry/gaia/v8/logs/clip_1.0-tc-WalkForward-750k2010/experiment_state-2019-07-02_18-59-46.json',\n",
    "#         2011: '/home/Nicholas/trading-gym/notebooks/registry/gaia/v8/logs/clip_1.0-tc-WalkForward-750k2011/experiment_state-2019-07-02_19-34-34.json',\n",
    "#         2012: '/home/Nicholas/trading-gym/notebooks/registry/gaia/v8/logs/clip_1.0-tc-WalkForward-750k2012/experiment_state-2019-07-02_20-08-38.json',\n",
    "#         2013: '/home/Nicholas/trading-gym/notebooks/registry/gaia/v8/logs/clip_1.0-tc-WalkForward-750k2013/experiment_state-2019-07-02_20-32-51.json',\n",
    "#         2014: '/home/Nicholas/trading-gym/notebooks/registry/gaia/v8/logs/clip_1.0-tc-WalkForward-750k2014/experiment_state-2019-07-02_20-57-24.json',\n",
    "#         2015: '/home/Nicholas/trading-gym/notebooks/registry/gaia/v8/logs/clip_1.0-tc-WalkForward-750k2015/experiment_state-2019-07-02_21-21-59.json',\n",
    "#         2016: '/home/Nicholas/trading-gym/notebooks/registry/gaia/v8/logs/clip_1.0-tc-WalkForward-750k2016/experiment_state-2019-07-02_21-46-11.json',\n",
    "#         2017: '/home/Nicholas/trading-gym/notebooks/registry/gaia/v8/logs/clip_1.0-tc-WalkForward-750k2017/experiment_state-2019-07-02_22-10-54.json'\n",
    "#         }\n",
    "\n",
    "# no transaction costs with clip 0.9\n",
    "# paths = {2007: '/home/Nicholas/trading-gym/notebooks/registry/gaia/v8/logs/clip_0.9-tc-WalkForward-750k2007/experiment_state-2019-07-03_00-01-00.json',\n",
    "#         2008: '/home/Nicholas/trading-gym/notebooks/registry/gaia/v8/logs/clip_0.9-tc-WalkForward-750k2008/experiment_state-2019-07-03_00-48-41.json',\n",
    "#         2009: '/home/Nicholas/trading-gym/notebooks/registry/gaia/v8/logs/clip_0.9-tc-WalkForward-750k2009/experiment_state-2019-07-03_01-26-09.json',\n",
    "#         2010: '/home/Nicholas/trading-gym/notebooks/registry/gaia/v8/logs/clip_0.9-tc-WalkForward-750k2010/experiment_state-2019-07-03_01-51-35.json',\n",
    "#         2011: '/home/Nicholas/trading-gym/notebooks/registry/gaia/v8/logs/clip_0.9-tc-WalkForward-750k2011/experiment_state-2019-07-03_02-17-30.json',\n",
    "#         2012: '/home/Nicholas/trading-gym/notebooks/registry/gaia/v8/logs/clip_0.9-tc-WalkForward-750k2012/experiment_state-2019-07-03_02-41-49.json',\n",
    "#         2013: '/home/Nicholas/trading-gym/notebooks/registry/gaia/v8/logs/clip_0.9-tc-WalkForward-750k2013/experiment_state-2019-07-03_03-07-25.json',\n",
    "#         2014: '/home/Nicholas/trading-gym/notebooks/registry/gaia/v8/logs/clip_0.9-tc-WalkForward-750k2014/experiment_state-2019-07-03_03-32-32.json',\n",
    "#         2015: '/home/Nicholas/trading-gym/notebooks/registry/gaia/v8/logs/clip_0.9-tc-WalkForward-750k2015/experiment_state-2019-07-03_03-58-08.json',\n",
    "#         2016: '/home/Nicholas/trading-gym/notebooks/registry/gaia/v8/logs/clip_0.9-tc-WalkForward-750k2016/experiment_state-2019-07-03_04-23-49.json',\n",
    "#         2017: '/home/Nicholas/trading-gym/notebooks/registry/gaia/v8/logs/clip_0.9-tc-WalkForward-750k2017/experiment_state-2019-07-03_04-49-35.json'\n",
    "#         }\n",
    "\n",
    "\n",
    "\n",
    "#  stilll Transaction cost, clip of 0.8  -- to add to results \n",
    "# paths = {2007: '/home/Nicholas/trading-gym/notebooks/registry/gaia/v8/logs/clip_0.8-tc-WalkForward-750k2007/experiment_state-2019-07-03_10-43-30.json',\n",
    "#         2008: '/home/Nicholas/trading-gym/notebooks/registry/gaia/v8/logs/clip_0.8-tc-WalkForward-750k2008/experiment_state-2019-07-03_11-26-04.json',\n",
    "#         2009: '/home/Nicholas/trading-gym/notebooks/registry/gaia/v8/logs/clip_0.8-tc-WalkForward-750k2009/experiment_state-2019-07-03_12-04-55.json',\n",
    "#         2010: '/home/Nicholas/trading-gym/notebooks/registry/gaia/v8/logs/clip_0.8-tc-WalkForward-750k2010/experiment_state-2019-07-03_12-40-43.json',\n",
    "#         2011: '/home/Nicholas/trading-gym/notebooks/registry/gaia/v8/logs/clip_0.8-tc-WalkForward-750k2011/experiment_state-2019-07-03_13-21-05.json',\n",
    "#         2012: '/home/Nicholas/trading-gym/notebooks/registry/gaia/v8/logs/clip_0.8-tc-WalkForward-750k2012/experiment_state-2019-07-03_13-58-51.json',\n",
    "#         2013: '/home/Nicholas/trading-gym/notebooks/registry/gaia/v8/logs/clip_0.8-tc-WalkForward-750k2013/experiment_state-2019-07-03_14-36-10.json',\n",
    "#         2014: '/home/Nicholas/trading-gym/notebooks/registry/gaia/v8/logs/clip_0.8-tc-WalkForward-750k2014/experiment_state-2019-07-03_15-22-57.json',\n",
    "#         2015: '/home/Nicholas/trading-gym/notebooks/registry/gaia/v8/logs/clip_0.8-tc-WalkForward-750k2015/experiment_state-2019-07-03_16-10-41.json',\n",
    "#         2016: '/home/Nicholas/trading-gym/notebooks/registry/gaia/v8/logs/clip_0.8-tc-WalkForward-750k2016/experiment_state-2019-07-03_16-57-02.json',\n",
    "#         2017: '/home/Nicholas/trading-gym/notebooks/registry/gaia/v8/logs/clip_0.8-tc-WalkForward-750k2017/experiment_state-2019-07-03_17-44-47.json'\n",
    "#         }\n",
    "\n",
    "# Actually with transaction cost now (at the default level) and 0.8 clip param\n",
    "# paths = {2007: '/home/Nicholas/trading-gym/notebooks/registry/gaia/v8/logs/tran_cost/clip_0.8-tc-WalkForward-750k2007/experiment_state-2019-07-04_09-43-03.json',\n",
    "#         2008: '/home/Nicholas/trading-gym/notebooks/registry/gaia/v8/logs/tran_cost/clip_0.8-tc-WalkForward-750k2008/experiment_state-2019-07-04_10-15-25.json',\n",
    "#         2009: '/home/Nicholas/trading-gym/notebooks/registry/gaia/v8/logs/tran_cost/clip_0.8-tc-WalkForward-750k2009/experiment_state-2019-07-04_10-51-51.json',\n",
    "#         2010: '/home/Nicholas/trading-gym/notebooks/registry/gaia/v8/logs/tran_cost/clip_0.8-tc-WalkForward-750k2010/experiment_state-2019-07-04_11-28-12.json',\n",
    "#         2011: '/home/Nicholas/trading-gym/notebooks/registry/gaia/v8/logs/tran_cost/clip_0.8-tc-WalkForward-750k2011/experiment_state-2019-07-04_11-57-09.json',\n",
    "#         2012: '/home/Nicholas/trading-gym/notebooks/registry/gaia/v8/logs/tran_cost/clip_0.8-tc-WalkForward-750k2012/experiment_state-2019-07-04_12-23-50.json',\n",
    "#         2013: '/home/Nicholas/trading-gym/notebooks/registry/gaia/v8/logs/tran_cost/clip_0.8-tc-WalkForward-750k2013/experiment_state-2019-07-04_12-50-33.json',\n",
    "#         2014: '/home/Nicholas/trading-gym/notebooks/registry/gaia/v8/logs/tran_cost/clip_0.8-tc-WalkForward-750k2014/experiment_state-2019-07-04_13-16-57.json',\n",
    "#         2015: '/home/Nicholas/trading-gym/notebooks/registry/gaia/v8/logs/tran_cost/clip_0.8-tc-WalkForward-750k2015/experiment_state-2019-07-04_13-44-00.json',\n",
    "#         2016: '/home/Nicholas/trading-gym/notebooks/registry/gaia/v8/logs/tran_cost/clip_0.8-tc-WalkForward-750k2016/experiment_state-2019-07-04_14-14-27.json',\n",
    "#         2017: '/home/Nicholas/trading-gym/notebooks/registry/gaia/v8/logs/tran_cost/clip_0.8-tc-WalkForward-750k2017/experiment_state-2019-07-04_14-49-49.json'\n",
    "#         }\n",
    "\n",
    "# With transaction cost now (at double the level) and 0.8 clip param\n",
    "# paths = {2007: '/home/Nicholas/trading-gym/notebooks/registry/gaia/v8/logs/tran_cost_double/clip_0.8-tc-WalkForward-750k2007/experiment_state-2019-07-08_09-08-56.json',\n",
    "#         2008: '/home/Nicholas/trading-gym/notebooks/registry/gaia/v8/logs/tran_cost_double/clip_0.8-tc-WalkForward-750k2008/experiment_state-2019-07-08_09-34-49.json',\n",
    "#         2009: '/home/Nicholas/trading-gym/notebooks/registry/gaia/v8/logs/tran_cost_double/clip_0.8-tc-WalkForward-750k2009/experiment_state-2019-07-08_10-02-15.json',\n",
    "#         2010: '/home/Nicholas/trading-gym/notebooks/registry/gaia/v8/logs/tran_cost_double/clip_0.8-tc-WalkForward-750k2010/experiment_state-2019-07-08_10-29-33.json',\n",
    "#         2011: '/home/Nicholas/trading-gym/notebooks/registry/gaia/v8/logs/tran_cost_double/clip_0.8-tc-WalkForward-750k2011/experiment_state-2019-07-08_10-58-06.json',\n",
    "#         2012: '/home/Nicholas/trading-gym/notebooks/registry/gaia/v8/logs/tran_cost_double/clip_0.8-tc-WalkForward-750k2012/experiment_state-2019-07-08_11-24-29.json',\n",
    "#         2013: '/home/Nicholas/trading-gym/notebooks/registry/gaia/v8/logs/tran_cost_double/clip_0.8-tc-WalkForward-750k2013/experiment_state-2019-07-08_11-50-59.json',\n",
    "#         2014: '/home/Nicholas/trading-gym/notebooks/registry/gaia/v8/logs/tran_cost_double/clip_0.8-tc-WalkForward-750k2014/experiment_state-2019-07-08_12-18-35.json',\n",
    "#         2015: '/home/Nicholas/trading-gym/notebooks/registry/gaia/v8/logs/tran_cost_double/clip_0.8-tc-WalkForward-750k2015/experiment_state-2019-07-08_12-47-25.json',\n",
    "#         2016: '/home/Nicholas/trading-gym/notebooks/registry/gaia/v8/logs/tran_cost_double/clip_0.8-tc-WalkForward-750k2016/experiment_state-2019-07-08_13-14-43.json',\n",
    "#         2017: '/home/Nicholas/trading-gym/notebooks/registry/gaia/v8/logs/tran_cost_double/clip_0.8-tc-WalkForward-750k2017/experiment_state-2019-07-08_13-41-22.json'\n",
    "#         }\n",
    "\n",
    "# With transaction cost now (at triple the level) and 0.8 clip param\n",
    "paths = {2007: '/home/Nicholas/trading-gym/notebooks/registry/gaia/v8/logs/tran_cost_triple/clip_0.8-tc-WalkForward-750k2007/experiment_state-2019-07-08_14-26-15.json',\n",
    "        2008: '/home/Nicholas/trading-gym/notebooks/registry/gaia/v8/logs/tran_cost_triple/clip_0.8-tc-WalkForward-750k2008/experiment_state-2019-07-08_14-52-38.json',\n",
    "        2009: '/home/Nicholas/trading-gym/notebooks/registry/gaia/v8/logs/tran_cost_triple/clip_0.8-tc-WalkForward-750k2009/experiment_state-2019-07-08_15-20-13.json',\n",
    "        2010: '/home/Nicholas/trading-gym/notebooks/registry/gaia/v8/logs/tran_cost_triple/clip_0.8-tc-WalkForward-750k2010/experiment_state-2019-07-08_15-47-57.json',\n",
    "        2011: '/home/Nicholas/trading-gym/notebooks/registry/gaia/v8/logs/tran_cost_triple/clip_0.8-tc-WalkForward-750k2011/experiment_state-2019-07-08_16-15-37.json',\n",
    "        2012: '/home/Nicholas/trading-gym/notebooks/registry/gaia/v8/logs/tran_cost_triple/clip_0.8-tc-WalkForward-750k2012/experiment_state-2019-07-08_16-44-41.json',\n",
    "        2013: '/home/Nicholas/trading-gym/notebooks/registry/gaia/v8/logs/tran_cost_triple/clip_0.8-tc-WalkForward-750k2013/experiment_state-2019-07-08_17-13-04.json',\n",
    "        2014: '/home/Nicholas/trading-gym/notebooks/registry/gaia/v8/logs/tran_cost_triple/clip_0.8-tc-WalkForward-750k2014/experiment_state-2019-07-08_17-39-13.json',\n",
    "        2015: '/home/Nicholas/trading-gym/notebooks/registry/gaia/v8/logs/tran_cost_triple/clip_0.8-tc-WalkForward-750k2015/experiment_state-2019-07-08_18-06-10.json',\n",
    "        2016: '/home/Nicholas/trading-gym/notebooks/registry/gaia/v8/logs/tran_cost_triple/clip_0.8-tc-WalkForward-750k2016/experiment_state-2019-07-08_18-31-47.json',\n",
    "        2017: '/home/Nicholas/trading-gym/notebooks/registry/gaia/v8/logs/tran_cost_triple/clip_0.8-tc-WalkForward-750k2017/experiment_state-2019-07-08_18-58-52.json'\n",
    "        }\n",
    "\n",
    "# To-do: put in both the quadruple and the x5 cost levels \n",
    "\n",
    "# To-do: put for x6 as well\n",
    "# Would be interesting to see how the the turnover changes as a function of this \n",
    "# Will make it easier for us to design something around the turnvoer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check to see what the config is to be sure**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': tune.function(<function calculate_tearsheet at 0x7f9ddbf89158>), 'on_postprocess_traj': None}, 'ignore_worker_failures': False, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'squash_to_range': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action_reward': False, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': 'MLP', 'custom_options': {}}, 'optimizer': {}, 'gamma': 0, 'horizon': None, 'soft_horizon': False, 'env_config': {'folds': {'training-set': [datetime.datetime(1, 1, 1, 0, 0), datetime.datetime(2007, 12, 31, 0, 0)], 'test-set': [datetime.datetime(2008, 1, 1, 0, 0), datetime.datetime(2008, 12, 31, 0, 0)]}, 'cost_of_commissions': 0.00015, 'cost_of_spread': 0.0003}, 'env': <class 'trading_gym.registry.gaia.v8.env.GAIAPredictorsContinuousV8'>, 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_config': {}, 'num_workers': 6, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 200, 'train_batch_size': 4000, 'batch_mode': 'complete_episodes', 'sample_async': False, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_evaluator_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'input': 'sampler', 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {}, 'policy_mapping_fn': None, 'policies_to_train': None}, 'use_gae': False, 'lambda': 0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 8, 'lr': 1e-05, 'lr_schedule': None, 'vf_share_layers': False, 'vf_loss_coeff': 0, 'entropy_coeff': 1e-05, 'clip_param': 0.8, 'vf_clip_param': 0, 'grad_clip': None, 'kl_target': 0.01, 'simple_optimizer': False, 'straggler_mitigation': False}\n"
     ]
    }
   ],
   "source": [
    "for year,path in paths.items():\n",
    "    with open(path) as f:\n",
    "        metadata = json.load(f)\n",
    "\n",
    "    runner_data = metadata['runner_data']\n",
    "    stats = metadata['stats']\n",
    "\n",
    "    checkpoint = metadata['checkpoints'][-1]\n",
    "    checkpoint = cloudpickleloads(checkpoint)\n",
    "    checkpoint_path = cloudpickle.loads(hex_to_binary(checkpoint['_checkpoint'])).value\n",
    "\n",
    "    config = checkpoint['config']\n",
    "    print(config)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'folds': {'training-set': [datetime.datetime(1, 1, 1, 0, 0), datetime.datetime(2007, 12, 31, 0, 0)], 'test-set': [datetime.datetime(2008, 1, 1, 0, 0), datetime.datetime(2008, 12, 31, 0, 0)]}, 'cost_of_commissions': 0.00015, 'cost_of_spread': 0.0003}\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Unknown config parameter `local_evaluator_tf_session_args` ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-c05aa6624682>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mpath_restore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'logdir'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mppo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPPOTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_restore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/trading-gym/.venv/lib/python3.6/site-packages/ray/rllib/agents/trainer_template.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, env, logger_creator)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger_creator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0mTrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger_creator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_creator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/trading-gym/.venv/lib/python3.6/site-packages/ray/rllib/agents/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, env, logger_creator)\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0mlogger_creator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefault_logger_creator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m         \u001b[0mTrainable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger_creator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/trading-gym/.venv/lib/python3.6/site-packages/ray/tune/trainable.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, logger_creator)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterations_since_restore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_restored\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_local_ip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mservices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_node_ip_address\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/trading-gym/.venv/lib/python3.6/site-packages/ray/rllib/agents/trainer.py\u001b[0m in \u001b[0;36m_setup\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    422\u001b[0m         merged_config = deep_update(merged_config, config,\n\u001b[1;32m    423\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_allow_unknown_configs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m                                     self._allow_unknown_subkeys)\n\u001b[0m\u001b[1;32m    425\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_user_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerged_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/trading-gym/.venv/lib/python3.6/site-packages/ray/tune/util.py\u001b[0m in \u001b[0;36mdeep_update\u001b[0;34m(original, new_dict, new_keys_allowed, whitelist)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moriginal\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnew_keys_allowed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unknown config parameter `{}` \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwhitelist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Unknown config parameter `local_evaluator_tf_session_args` "
     ]
    }
   ],
   "source": [
    "episodes = dict()\n",
    "agents = dict()\n",
    "for year, path in paths.items():\n",
    "    # RESTORE part (a)\n",
    "    with open(path) as f:\n",
    "        metadata = json.load(f)\n",
    "\n",
    "    runner_data = metadata['runner_data']\n",
    "    stats = metadata['stats']\n",
    "\n",
    "    checkpoint = metadata['checkpoints'][-1]\n",
    "    checkpoint = cloudpickleloads(checkpoint)\n",
    "    checkpoint_path = cloudpickle.loads(hex_to_binary(checkpoint['_checkpoint'])).value\n",
    "\n",
    "    config = checkpoint['config']\n",
    "#     Don't actually need to redefine the env_cls as it's always the same \n",
    "    env_cls = config['env']\n",
    "    env_config = config['env_config']\n",
    "    print(env_config)\n",
    "    \n",
    "#   Manually enter in the cost of commissions etc -- although is this right? \n",
    "#     env_config['cost_of_commissions'] = 0.00005  \n",
    "#     env_config['cost_of_spread'] = 0.0001  \n",
    "    \n",
    "    path_restore = os.path.join(checkpoint['logdir'], checkpoint_path)\n",
    "    \n",
    "    agent = rllib.agents.ppo.PPOTrainer(config, env_cls)\n",
    "    agent.restore(path_restore)\n",
    "\n",
    "    env = env_cls(env_config)\n",
    "\n",
    "    \n",
    "    episode = env.sample_episode(\n",
    "        fold='test-set',\n",
    "        policy=agent,\n",
    "        episode_length=None,\n",
    "        benchmark=env._load_benchmark().squeeze(),\n",
    "        risk_free=env._load_risk_free().squeeze(),\n",
    "        burn=1,\n",
    "    )\n",
    "    \n",
    "    renderer = env.render()\n",
    "    renderer.level.to_plotly()\n",
    "    renderer.cost_of_commissions.to_plotly()\n",
    "    renderer.cost_of_spread.to_plotly()\n",
    "    \n",
    "    episodes[year] = episode\n",
    "    agents[year] = agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "renderer = env.render()\n",
    "renderer.level.to_plotly()\n",
    "renderer.cost_of_commissions.to_plotly()\n",
    "renderer.cost_of_spread.to_plotly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "levels = list()\n",
    "mappings = pd.DataFrame()\n",
    "mapping_functions = dict()\n",
    "for year in paths:\n",
    "    episode = episodes[year]\n",
    "    agent = agents[year]\n",
    "\n",
    "    # Load.\n",
    "    actions = episode.actions_as_frame()\n",
    "    states = episode.states_as_frame()\n",
    "    \n",
    "    # Parse.\n",
    "    gaia_predictor = states[0].to_frame('GAIA Predictor')\n",
    "    \n",
    "#     The following line was here before\n",
    "#     target_weight_russell_1000 = actions[ETF('Russell 1000')]\n",
    "    target_weight_russell_1000 = actions[actions.columns[0]]\n",
    "    target_weight_russell_1000.name = 'Target weight: ' + str(target_weight_russell_1000.name)\n",
    "    mapping = gaia_predictor.join(target_weight_russell_1000)\n",
    "    mapping_function = mapping.set_index('GAIA Predictor')\n",
    "\n",
    "    levels.append(episode.renderer.level.to_frame().pct_change())\n",
    "    mappings = mappings.append(mapping)\n",
    "    mapping_functions[year] = mapping_function\n",
    "\n",
    "    # Visualize.\n",
    "    mapping.iplot(\n",
    "        title=\"Hisorical GAIA predictor for Russell 1000 vs agent's target weights\",\n",
    "        secondary_y='GAIA Predictor',\n",
    "        yTitle=target_weight_russell_1000.name,\n",
    "        secondary_y_title='GAIA Predictor',\n",
    "        legend={'orientation': 'h'},\n",
    "    )\n",
    "    mapping_function.iplot(\n",
    "        title='Policy: mapping from GAIA predictor (state) to target weight for Russell 1000 (action)',\n",
    "        xTitle='GAIA predictor for Russell 1000 (standardized)',\n",
    "        yTitle='Target weight for Russell 1000',\n",
    "        kind='scatter',\n",
    "        mode='markers',\n",
    "        size=4,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "daily_ret = pd.concat(levels).sort_index().fillna(0)\n",
    "cumulative_performance = (1 + daily_ret).cumprod() - 1\n",
    "cumulative_performance *= 100\n",
    "\n",
    "aric = cumulative_performance.columns[1]\n",
    "cumulative_performance['Strategy relative to Aric-Benchmark'] = cumulative_performance['Strategy'] - cumulative_performance[aric]\n",
    "\n",
    "\n",
    "# Visualizations.\n",
    "cumulative_performance.iplot(\n",
    "    legend={'orientation': 'h'},\n",
    "    yTitle='Total returns',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "levels = (1 + cumulative_performance / 100)\n",
    "annual_rets = (levels.resample('Y').last() / levels.resample('Y').first() - 1)\n",
    "\n",
    "    \n",
    "annual_rets['Strategy relative to Aric-Benchmark'] = annual_rets['Strategy'] - annual_rets[aric]\n",
    "annual_rets.index = annual_rets.index.year\n",
    "annual_rets *= 100\n",
    "annual_rets.iplot(kind='bar', legend={'orientation': 'h'}, yTitle='%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "levels.drop('Strategy relative to Aric-Benchmark', axis='columns').tearsheet(\n",
    "    benchmark=env._load_benchmark().loc['2008':].squeeze(),\n",
    "    risk_free=env._load_risk_free().loc['2008':].squeeze(),\n",
    "    weights=env.broker.track_record.to_frame('weights_target').iloc[1:]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "import cufflinks\n",
    "cufflinks.go_offline()\n",
    "init_notebook_mode(connected=False)\n",
    "\n",
    "\n",
    "traces = list()\n",
    "for year, series in mapping_functions.items():\n",
    "    trace = go.Scatter(\n",
    "        x = list(series.squeeze().index[:-1]),\n",
    "        y = list(series.squeeze().values[:-1]),\n",
    "        mode = 'markers',\n",
    "        name = year\n",
    "    )\n",
    "    traces.append(trace)\n",
    "    \n",
    "layout = go.Layout(\n",
    "    title='GAIA vs RL mapping functions',\n",
    "    xaxis=dict(\n",
    "        title='GAIA Mapping'\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='PPO Mapping'\n",
    "        )\n",
    "        \n",
    "    )\n",
    "fig = go.Figure(data=traces,layout=layout)\n",
    "iplot(fig,filename='scatter=mode')\n",
    "\n",
    "# iplot(traces, filename='scatter-mode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
